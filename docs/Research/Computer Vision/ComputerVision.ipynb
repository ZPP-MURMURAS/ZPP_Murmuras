{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 Microsoft OmniParser",
   "id": "1acc9043b7f37543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "https://github.com/microsoft/OmniParser \\\n",
    "https://microsoft.github.io/OmniParser/ \\\n",
    "https://huggingface.co/microsoft/OmniParser \\\n",
    "\\\n",
    "OmniParser was the first model that I tested. My impressions after playing with its demo were grand (you can find it here: https://huggingface.co/spaces/microsoft/OmniParser); after providing it with a picture of Lidl's promo magazine, it was able to extract text from it with very high precision. Additionally, because it wasn't parsing any XML trees, output was very clear; as a result I got a list of extracted strings and a list of their bounding boxes. Using clustering or even simple algorithms to bind together close elements could provide us with a very clean data that later BERT model could easily label. \\\n",
    "Problems arise with the deployment of the model. As of today (12.11.24) I couldn't find any official data regarding the number of params of the model. Additionally, its deployment on a local machine is non-trivial: it's huggingface page doesn't hold all necessary files, so to run the model, you have to clone the github repo AND THEN download files from the huggingface page. And when I finally managed to do that, I found that this model can't be run without NVIDIA GPU drivers (I'm using an AMD GPU). \\\n",
    "In conclusion, I think that OmniParser could prove very useful to us, but we won't be able to deploy it to the mobile."
   ],
   "id": "a45761401ae1152b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 MoondreamAI",
   "id": "6a49679e89a42750"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "https://huggingface.co/vikhyatk/moondream2 \\\n",
    "https://github.com/vikhyat/moondream \\\n",
    "https://moondream.ai/ \\\n"
   ],
   "id": "483cc66bd7fabddf"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "model_id = \"vikhyatk/moondream2\"\n",
    "revision = \"2024-08-26\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, trust_remote_code=True, revision=revision\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
    "\n",
    "image = Image.open('Screenshot from 2024-11-10 18-13-40.png')\n",
    "enc_image = model.encode_image(image)\n",
    "print(model.answer_question(enc_image, \"Describe this image.\", tokenizer))"
   ],
   "id": "f0f1147eb7149a1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Above you can see a demo representing the MoonDreamAI2. As you can see, instead of operating on a raw data, we can prompt this model for a description of an image. Model itself has around 1.8B params and is marketed as something that should be able to run on edge devices (such as mobile phones). Additionally, it's supposed to be easily configurable to be smaller (on its official page it is said that it can be easily distilled). Sadly, I didn't find any examples of that other than HF tutotial on distilling Vision models (can be found here: https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%203%20-%20Vision%20Transformers/KnowledgeDistillation.ipynb). Still, there is an option for batch requests, as well as a flash attention, which should speed-up the model (presented below). \\",
   "id": "f9a0494f4cfbf12b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_id = \"vikhyatk/moondream2\"\n",
    "revision = \"2024-08-26\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, trust_remote_code=True, revision=revision\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision,\n",
    "                                          attn_implementation=\"flash_attention_2\") # flash_attention\n",
    "\n",
    "image = Image.open('Screenshot from 2024-11-10 18-13-40.png')\n",
    "enc_image = model.encode_image(image)\n",
    "print(model.answer_question(enc_image, \"Describe this image.\", tokenizer))"
   ],
   "id": "9bbde844435d366b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If your laptop/IDE crashed during the execution of any of those two code snippetss, don't you worry; at least on my machine, one run of this model is enough for my laptop. Which contradicts with the statement that this model is supposed to be run on edge devices. \\\n",
    "Another issue is that this model is, at the end of the day, too dumb for our needs. You can play with it safely on its demo page, and you will probably quickly notice that while it responds very well to prompts like \"Describe me this\", modifying the prompt to something like \"Describe me this AS json\" will cause the model to generate random sequences of words from the picture.\n",
    "In conclusion, I'm not sure whether this would run on a mobile device (at least without distillation), and even if, it would be nice for tasks related to image description, but we need something much more specific."
   ],
   "id": "23ca5d4cbe71a67f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 InternVL family",
   "id": "fd0ff5b76441d1cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "https://huggingface.co/OpenGVLab/InternVL2-2B \\\n",
    "https://github.com/OpenGVLab/InternVL \\\n",
    "https://internvl.github.io/blog/2024-10-21-Mini-InternVL-2.0/ \\\n",
    "https://internvl.readthedocs.io/en/latest/internvl2.0/domain_adaptation.html"
   ],
   "id": "ac1654718fd74386"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:44:04.724472Z",
     "start_time": "2024-11-12T11:43:59.078922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer\n",
    "from PIL import Image\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "# Load model, processor, and tokenizer\n",
    "model_name = \"OpenGVLab/InternVL2-2B\"\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Load and prepare image\n",
    "image_url = \"https://example.com/sample-image.jpg\"  # Replace with your image\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "# Encode prompt and image\n",
    "prompt = \"Describe this image\"\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}  # Ensure using CPU\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs)\n",
    "    description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(description)\n"
   ],
   "id": "9fffc38a173ca5ee",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Load model, processor, and tokenizer\u001B[39;00m\n\u001B[1;32m      7\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpenGVLab/InternVL2-2B\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 8\u001B[0m processor \u001B[38;5;241m=\u001B[39m \u001B[43mAutoProcessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[1;32m     10\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name, trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/transformers/models/auto/processing_auto.py:299\u001B[0m, in \u001B[0;36mAutoProcessor.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m processor_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    297\u001B[0m     \u001B[38;5;66;03m# Otherwise, load config, if it can be loaded.\u001B[39;00m\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, PretrainedConfig):\n\u001B[0;32m--> 299\u001B[0m         config \u001B[38;5;241m=\u001B[39m \u001B[43mAutoConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    300\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    301\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;66;03m# And check if the config contains the processor class.\u001B[39;00m\n\u001B[1;32m    304\u001B[0m     processor_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocessor_class\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1020\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m   1018\u001B[0m has_remote_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1019\u001B[0m has_local_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m CONFIG_MAPPING\n\u001B[0;32m-> 1020\u001B[0m trust_remote_code \u001B[38;5;241m=\u001B[39m \u001B[43mresolve_trust_remote_code\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_local_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_remote_code\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_remote_code \u001B[38;5;129;01mand\u001B[39;00m trust_remote_code:\n\u001B[1;32m   1025\u001B[0m     class_ref \u001B[38;5;241m=\u001B[39m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:651\u001B[0m, in \u001B[0;36mresolve_trust_remote_code\u001B[0;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001B[0m\n\u001B[1;32m    649\u001B[0m signal\u001B[38;5;241m.\u001B[39malarm(TIME_OUT_REMOTE_CODE)\n\u001B[1;32m    650\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m trust_remote_code \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 651\u001B[0m     answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    652\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mThe repository for \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmodel_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m contains custom code which must be executed to correctly \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    653\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mload the model. You can inspect the repository content at https://hf.co/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmodel_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    654\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    655\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDo you wish to run the custom code? [y/N] \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    656\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    657\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m answer\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myes\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    658\u001B[0m         trust_remote_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[0;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[0;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[1;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[1;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Above you can see my attempt to run the InternVL2-2B model. After some time I managed to run it on CPU, not GPU, but my IDE crashes because of too many restarted processes on CPU. Which is s shame. This model has 2,2B params (there is a version with 900mln params), and when prompted with Lidl image in the online demo, it quickly returned a very accurate json with products and their prices. So it is both small and accurate, but if running it on a laptop CPU is impossible, then running it on a mobile phone is even more so.",
   "id": "d1c784fddce1d518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b0aac5b07e07506"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
