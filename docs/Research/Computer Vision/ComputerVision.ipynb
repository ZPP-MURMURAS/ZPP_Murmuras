{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 Microsoft OmniParser",
   "id": "1acc9043b7f37543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "https://github.com/microsoft/OmniParser \\\n",
    "https://microsoft.github.io/OmniParser/ \\\n",
    "https://huggingface.co/microsoft/OmniParser \\\n",
    "\\\n",
    "OmniParser was the first model that I tested. My impressions after playing with its demo were grand (you can find it here: https://huggingface.co/spaces/microsoft/OmniParser); after providing it with a picture of Lidl's promo magazine, it was able to extract text from it with very high precision. Additionally, because it wasn't parsing any XML trees, output was very clear; as a result I got a list of extracted strings and a list of their bounding boxes. Using clustering or even simple algorithms to bind together close elements could provide us with a very clean data that later BERT model could easily label. \\\n",
    "Problems arise with the deployment of the model. As of today (12.11.24) I couldn't find any official data regarding the number of params of the model. Additionally, its deployment on a local machine is non-trivial: it's huggingface page doesn't hold all necessary files, so to run the model, you have to clone the github repo AND THEN download files from the huggingface page. And when I finally managed to do that, I found that this model can't be run without NVIDIA GPU drivers (I'm using an AMD GPU). \\\n",
    "In conclusion, I think that OmniParser could prove very useful to us, but we won't be able to deploy it to the mobile."
   ],
   "id": "a45761401ae1152b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 MoondreamAI",
   "id": "6a49679e89a42750"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "https://huggingface.co/vikhyatk/moondream2 \\\n",
    "https://github.com/vikhyat/moondream \\\n",
    "https://moondream.ai/ \\\n"
   ],
   "id": "483cc66bd7fabddf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:48:58.178159Z",
     "start_time": "2024-11-12T14:48:41.455369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "model_id = \"vikhyatk/moondream2\"\n",
    "revision = \"2024-08-26\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, trust_remote_code=True, revision=revision\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
    "\n",
    "image = Image.open('Screenshot from 2024-11-10 18-13-40.png')\n",
    "enc_image = model.encode_image(image)\n",
    "print(model.answer_question(enc_image, \"Describe this image.\", tokenizer))"
   ],
   "id": "f0f1147eb7149a1a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Above you can see a demo representing the MoonDreamAI2. As you can see, instead of operating on a raw data, we can prompt this model for a description of an image. Model itself has around 1.8B params and is marketed as something that should be able to run on edge devices (such as mobile phones). Additionally, it's supposed to be easily configurable to be smaller (on its official page it is said that it can be easily distilled). Sadly, I didn't find any examples of that other than HF tutotial on distilling Vision models (can be found here: https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%203%20-%20Vision%20Transformers/KnowledgeDistillation.ipynb). Still, there is an option for batch requests, as well as a flash attention, which should speed-up the model (presented below). \\",
   "id": "f9a0494f4cfbf12b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_id = \"vikhyatk/moondream2\"\n",
    "revision = \"2024-08-26\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, trust_remote_code=True, revision=revision\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision,\n",
    "                                          attn_implementation=\"flash_attention_2\") # flash_attention\n",
    "\n",
    "image = Image.open('Screenshot from 2024-11-10 18-13-40.png')\n",
    "enc_image = model.encode_image(image)\n",
    "print(model.answer_question(enc_image, \"Describe this image.\", tokenizer))"
   ],
   "id": "9bbde844435d366b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If your laptop/IDE crashed during the execution of any of those two code snippetss, don't you worry; at least on my machine, one run of this model is enough for my laptop. Which contradicts with the statement that this model is supposed to be run on edge devices. \\\n",
    "Another issue is that this model is, at the end of the day, too dumb for our needs. You can play with it safely on its demo page, and you will probably quickly notice that while it responds very well to prompts like \"Describe me this\", modifying the prompt to something like \"Describe me this AS json\" will cause the model to generate random sequences of words from the picture.\n",
    "In conclusion, I'm not sure whether this would run on a mobile device (at least without distillation), and even if, it would be nice for tasks related to image description, but we need something much more specific."
   ],
   "id": "23ca5d4cbe71a67f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 InternVL family",
   "id": "fd0ff5b76441d1cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "https://huggingface.co/OpenGVLab/InternVL2-2B \\\n",
    "https://github.com/OpenGVLab/InternVL \\\n",
    "https://internvl.github.io/blog/2024-10-21-Mini-InternVL-2.0/ \\\n",
    "https://internvl.readthedocs.io/en/latest/internvl2.0/domain_adaptation.html"
   ],
   "id": "ac1654718fd74386"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:50:46.575951Z",
     "start_time": "2024-11-12T15:50:40.004572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "# If you want to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\n",
    "path = 'OpenGVLab/InternVL2-2B'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# pure-text conversation (Á∫ØÊñáÊú¨ÂØπËØù)\n",
    "question = 'Hello, who are you?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Can you tell me a story?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# single-image single-round conversation (ÂçïÂõæÂçïËΩÆÂØπËØù)\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# single-image multi-round conversation (ÂçïÂõæÂ§öËΩÆÂØπËØù)\n",
    "question = '<image>\\nPlease describe the image in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Please write a poem according to the image.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# multi-image multi-round conversation, combined images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÊãºÊé•ÂõæÂÉè)\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "question = '<image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# multi-image multi-round conversation, separate images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÁã¨Á´ãÂõæÂÉè)\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "\n",
    "question = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# batch inference, single image per sample (ÂçïÂõæÊâπÂ§ÑÁêÜ)\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "questions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\n",
    "responses = model.batch_chat(tokenizer, pixel_values,\n",
    "                             num_patches_list=num_patches_list,\n",
    "                             questions=questions,\n",
    "                             generation_config=generation_config)\n",
    "for question, response in zip(questions, responses):\n",
    "    print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# video multi-round conversation (ËßÜÈ¢ëÂ§öËΩÆÂØπËØù)\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([\n",
    "        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "        for idx in range(num_segments)\n",
    "    ])\n",
    "    return frame_indices\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list\n",
    "\n",
    "video_path = './examples/red-panda.mp4'\n",
    "pixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\n",
    "pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
    "video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "question = video_prefix + 'What is the red panda doing?'\n",
    "# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Describe this video in detail. Don\\'t repeat.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n"
   ],
   "id": "9fffc38a173ca5ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'images': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1207x1001 at 0x736DE8BEA7B0>} not recognized.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Inference\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 23\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m     description \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(outputs[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(description)\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/OpenGVLab/InternVL2-2B/783b80681b65cb0e4d255b11235fc57dbb0035d3/modeling_internvl_chat.py:321\u001B[0m, in \u001B[0;36mInternVLChatModel.generate\u001B[0;34m(self, pixel_values, input_ids, attention_mask, visual_features, generation_config, output_hidden_states, return_dict, **generate_kwargs)\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mno_grad()\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate\u001B[39m(\n\u001B[1;32m    310\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    318\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgenerate_kwargs,\n\u001B[1;32m    319\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mLongTensor:\n\u001B[0;32m--> 321\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_context_token_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pixel_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    323\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m visual_features \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Above you can see my attempt to run the InternVL2-2B model. Both my notebook in pycharm and Google Collab fail because of lacking NVIDIA drivers. Which is s shame. This model has 2.2B params (there is a version with 900mln params), and when prompted with Lidl image in the online demo, it quickly returned a very accurate json with products and their prices. So it is both small and accurate, but if running it on a laptop CPU is impossible, then running it on a mobile phone is even more so.",
   "id": "d1c784fddce1d518"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 AIDC-AI/Ovis1.6-Llama3.2-3B",
   "id": "818b73ce5fba7b27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:55:40.466322Z",
     "start_time": "2024-11-12T14:55:38.738696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load model on CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\"AIDC-AI/Ovis1.6-Llama3.2-3B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             multimodal_max_length=8192,\n",
    "                                             trust_remote_code=True).to(\"cpu\")\n",
    "text_tokenizer = model.get_text_tokenizer()\n",
    "visual_tokenizer = model.get_visual_tokenizer()\n",
    "\n",
    "# Enter image path and prompt\n",
    "image_path = input(\"Enter image path: \")\n",
    "image = Image.open(image_path)\n",
    "text = input(\"Enter prompt: \")\n",
    "query = f'<image>\\n{text}'\n",
    "\n",
    "# Format conversation\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(query, [image])\n",
    "attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=\"cpu\")\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=\"cpu\")\n",
    "pixel_values = [pixel_values.to(dtype=visual_tokenizer.dtype, device=\"cpu\")]\n",
    "\n",
    "# Generate output\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        top_p=None,\n",
    "        top_k=None,\n",
    "        temperature=None,\n",
    "        repetition_penalty=None,\n",
    "        eos_token_id=model.generation_config.eos_token_id,\n",
    "        pad_token_id=text_tokenizer.pad_token_id,\n",
    "        use_cache=True\n",
    "    )\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    output = text_tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(f'Output:\\n{output}')\n",
    "\n"
   ],
   "id": "b8c0758c457ef0fa",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Load model on CPU\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAIDC-AI/Ovis1.6-Llama3.2-3B\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbfloat16\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43mmultimodal_max_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8192\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m text_tokenizer \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_text_tokenizer()\n\u001B[1;32m     11\u001B[0m visual_tokenizer \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_visual_tokenizer()\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:559\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    557\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mregister(config\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m, model_class, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    558\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m add_generation_mixin_to_remote_model(model_class)\n\u001B[0;32m--> 559\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    560\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    561\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4097\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   4091\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_autoset_attn_implementation(\n\u001B[1;32m   4092\u001B[0m         config, use_flash_attention_2\u001B[38;5;241m=\u001B[39muse_flash_attention_2, torch_dtype\u001B[38;5;241m=\u001B[39mtorch_dtype, device_map\u001B[38;5;241m=\u001B[39mdevice_map\n\u001B[1;32m   4093\u001B[0m     )\n\u001B[1;32m   4095\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ContextManagers(init_contexts):\n\u001B[1;32m   4096\u001B[0m     \u001B[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001B[39;00m\n\u001B[0;32m-> 4097\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4099\u001B[0m \u001B[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001B[39;00m\n\u001B[1;32m   4100\u001B[0m config \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/AIDC-AI/Ovis1.6-Llama3.2-3B/06de50f3f49a8c13b1aaa31bf1ecca2bcdea0deb/modeling_ovis.py:280\u001B[0m, in \u001B[0;36mOvis.__init__\u001B[0;34m(self, config, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mllm_attn_implementation:\n\u001B[1;32m    279\u001B[0m     attn_kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattn_implementation\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mllm_attn_implementation\n\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_config\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mattn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mhidden_size \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mhidden_size, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhidden size mismatch\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mname_or_path)\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:440\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_config\u001B[0;34m(cls, config, **kwargs)\u001B[0m\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    439\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    442\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    443\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    444\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    445\u001B[0m )\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1527\u001B[0m, in \u001B[0;36mPreTrainedModel._from_config\u001B[0;34m(cls, config, **kwargs)\u001B[0m\n\u001B[1;32m   1525\u001B[0m config\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattn_implementation\u001B[39m\u001B[38;5;124m\"\u001B[39m, attn_implementation)\n\u001B[1;32m   1526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_attn_implementation_autoset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m-> 1527\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autoset_attn_implementation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1528\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_flash_attention_2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_flash_attention_2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_device_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1531\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1534\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_deepspeed_zero3_enabled():\n\u001B[1;32m   1535\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdeepspeed\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1617\u001B[0m, in \u001B[0;36mPreTrainedModel._autoset_attn_implementation\u001B[0;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001B[0m\n\u001B[1;32m   1614\u001B[0m     config\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflash_attention_2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflash_attention_2\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1617\u001B[0m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_and_enable_flash_attn_2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1618\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1619\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1620\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1621\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhard_check_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1622\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_device_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_device_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1623\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1624\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m requested_attn_implementation \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msdpa\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available():\n\u001B[1;32m   1625\u001B[0m     \u001B[38;5;66;03m# use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\u001B[39;00m\n\u001B[1;32m   1626\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_check_and_enable_sdpa(\n\u001B[1;32m   1627\u001B[0m         config,\n\u001B[1;32m   1628\u001B[0m         hard_check_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m requested_attn_implementation \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   1629\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/ZPP_Murmuras/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1747\u001B[0m, in \u001B[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001B[0;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001B[0m\n\u001B[1;32m   1744\u001B[0m install_message \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mutil\u001B[38;5;241m.\u001B[39mfind_spec(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflash_attn\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpreface\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m the package flash_attn seems to be not installed. \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minstall_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1749\u001B[0m flash_attention_version \u001B[38;5;241m=\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(importlib\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mversion(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflash_attn\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mversion\u001B[38;5;241m.\u001B[39mcuda:\n",
      "\u001B[0;31mImportError\u001B[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"AIDC-AI/Ovis1.6-Llama3.2-3B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             multimodal_max_length=8192,\n",
    "                                             trust_remote_code=True).cuda()\n",
    "text_tokenizer = model.get_text_tokenizer()\n",
    "visual_tokenizer = model.get_visual_tokenizer()\n",
    "\n",
    "# enter image path and prompt\n",
    "image_path = input(\"Enter image path: \")\n",
    "image = Image.open(image_path)\n",
    "text = input(\"Enter prompt: \")\n",
    "query = f'<image>\\n{text}'\n",
    "\n",
    "# format conversation\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(query, [image])\n",
    "attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "pixel_values = [pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)]\n",
    "\n",
    "# generate output\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        top_p=None,\n",
    "        top_k=None,\n",
    "        temperature=None,\n",
    "        repetition_penalty=None,\n",
    "        eos_token_id=model.generation_config.eos_token_id,\n",
    "        pad_token_id=text_tokenizer.pad_token_id,\n",
    "        use_cache=True\n",
    "    )\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    output = text_tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(f'Output:\\n{output}')\n"
   ],
   "id": "1176b8b57de09a21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Above you can see two versions of the code: 1. original 2. on cpu because I have no CUDA. Both of them failed sadly because of the flash-attn package. I tried to download it for AMD but HG stringly suggest running it in docker, and on collab it wasn't possible to install it. It seems that the only easy way is fo NVIDIA products, which I don't have. \\\n",
    "About the model: this is 4,1B param model.  Available demo haa a bug when asked for json of Lild's items (started well, then stopped generating anything). It's the biggest model here (not counting OmniParser). In this particular case I don't think we are missing anything important."
   ],
   "id": "6aa14ee134ebf24c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b2bb7103e0ea2b45"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
