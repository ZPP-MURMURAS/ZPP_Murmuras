%
% Niniejszy plik stanowi przykład formatowania pracy magisterskiej na
% Wydziale MIM UW.  Szkielet użytych poleceń można wykorzystywać do
% woli, np. formatujac wlasna prace.
%
% Zawartosc merytoryczna stanowi oryginalnosiagniecie
% naukowosciowe Marcina Wolinskiego.  Wszelkie prawa zastrzeżone.
%
% Copyright (c) 2001 by Marcin Woliński <M.Wolinski@gust.org.pl>
% Poprawki spowodowane zmianami przepisów - Marcin Szczuka, 1.10.2004
% Poprawki spowodowane zmianami przepisow i ujednolicenie 
% - Seweryn Karłowicz, 05.05.2006
% Dodanie wielu autorów i tłumaczenia na angielski - Kuba Pochrybniak, 29.11.2016

% dodaj opcję [licencjacka] dla pracy licencjackiej
% dodaj opcję [en] dla wersji angielskiej (mogą być obie: [licencjacka,en])
\documentclass[licencjacka,en]{pracamgr}
\usepackage{hyperref}  % Enables clickable links
\usepackage{xcolor}    % Allows hyperlink color customization
\usepackage{graphicx}
\usepackage{listings}
\usepackage{minted}
\usepackage[toc]{appendix}
\usepackage{algorithm}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{fancyvrb}
\usepackage{rotating}
\usepackage{geometry}
\usepackage{caption}
\usepackage{booktabs}


% Define JSON formatting style for listings
\lstdefinelanguage{json}{
    basicstyle=\ttfamily\small,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    showstringspaces=false,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    stringstyle=\color{red}
}

% Set hyperlink colors
\hypersetup{
    colorlinks=false,
    urlcolor=blue
}

% Dane magistranta:
\autori{Szymon Kozłowski}{448304}
\autorii{Gustaw Blachowski}{448194}
\autoriii{Kamil Dybek}{448224}
\autoriv{Natalia Junkiert}{448267}

\title{Using machine learning models for processing the data presented to the user by mobile devices.}

\tytulang{Using machine learning models for processing data presented to user by mobile devices.}
\titlepl{Wykorzystanie modeli uczenia maszynowego do przetwarzania danych zaprezentowanych użytkownikowi przez urządzenie mobilne.}

%kierunek: 
% - matematyka, informacyka, ...
% - Mathematics, Computer Science, ...
\kierunek{Computer Science}

% informatyka - nie okreslamy zakresu (opcja zakomentowana)
% matematyka - zakres moze pozostac nieokreslony,
% a jesli ma byc okreslony dla pracy mgr,
% to przyjmuje jedna z wartosci:
% {metod matematycznych w finansach}
% {metod matematycznych w ubezpieczeniach}
% {matematyki stosowanej}
% {nauczania matematyki}
% Dla pracy licencjackiej mamy natomiast
% mozliwosc wpisania takiej wartosci zakresu:
% {Jednoczesnych Studiow Ekonomiczno--Matematycznych}

% \zakres{Tu wpisac, jesli trzeba, jedna z opcji podanych wyzej}

% Praca wykonana pod kierunkiem:
% (podać tytuł/stopień imię i nazwisko opiekuna
% Instytut
% ew. Wydział ew. Uczelnia (jeżeli nie MIM UW))
\opiekun{Jacek Sroka PhD\\
  Institute of Informatics\\
  }

% miesiąc i~rok:
\date{\today}

%Podać dziedzinę wg klasyfikacji Socrates-Erasmus:
\dziedzina{ 
%11.0 Matematyka, Informatyka:\\ 
%11.1 Matematyka\\ 
%11.2 Statystyka\\ 
%11.3 Informatyka\\ 
11.4 Artificial Intelligence\\ 
%11.5 Nauki aktuarialne\\
%11.9 Inne nauki matematyczne i informatyczne
}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{
  I.2.7: Natural Language Processing\\
  H.3.3: Information Search and Retrieval}

% Słowa kluczowe:
\keywords{LLM, NLP, BERT, Android, Edge-device, Fine-Tuning}

% Tu jest dobre miejsce na Twoje własne makra i~środowiska:
\newtheorem{defi}{Definicja}[section]

% koniec definicji
\let\cleardoublepage\clearpage
\begin{document}

\maketitle

%tu idzie streszczenie na strone poczatkowa
\begin{abstract}
In the era of rapidly evolving digital applications, traditional scraping techniques face increasing challenges in maintaining reliable data collection pipelines. Commissioned by Murmuras, a company specializing in commercial and scientific data analysis, in this project we present a novel approach to processing phone screen content, such as displayed social media posts and website advertisements. Our solution leverages Large Language Models (LLMs) running locally on the user's device to handle diverse data formats while ensuring that sensitive information remains protected. The primary application explored in this study is the extraction of discount coupons, demonstrating the feasibility of our method in identifying and structuring valuable content from varying digital sources. Furthermore, the system is designed to be easily adaptable to other use cases, such as analyzing users' political views. Additionally, we explore the usage of non-LLM models for the defined task. The results highlight the potential of LLM-driven content analysis as an alternative to conventional scraping techniques.
\raggedright
\end{abstract}

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms

\chapter{Introduction} \label{chap:intro}

\section{Project background and motivation}
With the rapid advancement of information technology, the Internet has become one of the most crucial facets for many businesses to perform marketing activities \cite{design_of_coupons}. One of the key marketing tools in business-to-consumer (B2C) e-commerce is the digital coupon \cite{targeted_reminders}. Compared to paper coupons, digital coupons are characterized by their wide reach, rapid distribution, and low spread costs. Furthermore, a key advantage of digital coupons is their ability to facilitate targeted marketing by offering personalized discounts to different customers, thereby increasing sales \cite{design_of_coupons}. 

Recent statistics underscore the significance of mobile devices in the domain of coupon distribution. For example, studies have shown that over 90\% of digital coupon users access their vouchers via smartphones \cite{emarketer_coupon_stats}, and similar figures are reported by other industry sources \cite{coupon_stats_2}. This high rate of mobile usage creates a pressing need for coupon analysis tools that are optimized for mobile platforms, ensuring that consumers receive timely and personalized offers regardless of their location or device.

Large Language Models (LLMs) have become a fundamental technique in contemporary machine learning, replacing previously utilized recurrent neural network (RNN) architectures in the field of natural language processing (NLP) \cite{li2024}. Subsequent research has demonstrated their applicability to structured input data \cite{sui2024}, such as screen views and coupons. Additionally, there have been efforts to integrate these models into web scraping pipelines \cite{scapegraph_repo}. 

In light of these trends, the company Murmuras has tasked us with developing a solution based on a machine learning model that can be deployed as a mobile application. This model will process input representing the user's onscreen view and extract digital coupons along with their relevant data. This solution must be capable of running locally on the device, ensuring efficient processing without relying on external servers. By leveraging advanced machine learning techniques, the app will handle the diverse formats and layouts of digital coupons, thus facilitating the collection of data regarding coupons.

\section{The definition of a coupon} 
A coupon is a physical piece of paper or digital voucher that can be redeemed for a financial discount when purchasing a product \cite{coupon_definition}. A coupon is characterized by a name, expiration date, and a discount type, e.g. '20\% off', 'buy 1 get 1 free', etc., however, not every coupon contains each of these features. Furthermore, coupons may contain numerous other features such as images and eligibility requirements. Henceforth, the term 'coupon' will refer exclusively to a digital coupon. The term 'conventional coupon' will refer to the traditional physical coupon. Examples of digital coupons encountered in mobile applications are presented in \ref{fig:example_coupons}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{bachelor_images/coupon1.jpg}
        \caption{Example coupon from fast-food restaurants chain mobile application}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{bachelor_images/coupon2.jpg}
        \caption{Example coupon from grocery store mobile application}
    \end{minipage}
    \caption{Example digital coupons}
    \label{fig:example_coupons}
\end{figure}


\subsection{Our data model of digital coupon}
\label{sec:coupon_model}
In the following research we model a digital coupon as a collection of named fields:
\begin{enumerate}
    \item \textit{product\_name}: the name of the product,
    \item \textit{valid\_until}: the text representing the date of coupon expiration,
    \item \textit{discount\_text}: the text representing the discount offered to the user,
    \item \textit{activated}: either true or false, indicates whether the coupon has been activated,
\end{enumerate}
We allow for special \textit{null} value in the above fields in case no data is available. 

An example of a digital coupon represented in JSON format is shown in listing \ref{lst:coupon_example}:

\begin{lstlisting}[language=json, caption={Example of a digital coupon in JSON format}, label={lst:coupon_example}]
{
    "product_name": "Shampoo X",
    "valid_unitl": "2025-06-30",
    "discount_text": "20% OFF",
    "activated": true
}
\end{lstlisting}

\section{The Significance of the Digital Coupon}
The digital coupon is one of the most important tool in contemporary marketing strategies \cite{targeted_reminders}, therefore analyzing their lifecycle is essential to maximize their benefits. To facilitate such analyses, researchers collect various statistical metrics, including the fraction of redeemed coupons among all distributed coupons referred henceforth as redemption rate \cite{danaher2015} and customer engagement \cite{jayadharshini2023}, while also assessing their impact on sales performance \cite{jayadharshini2023}.  Additionally, studying competitors' digital coupon strategies enables businesses to identify market trends, adjust their promotional tactics, and maintain a competitive edge in the evolving digital marketplace. 

\subsubsection{Redemption Rate}  
The measurement of coupon redemption rates is primarily based on either survey data \cite{nayal2021} or controlled experimental studies \cite{danaher2015}. However, the company Murmuras \cite{murmuras} has introduced an alternative approach that enables the direct collection of coupon-related data from users' devices. This method utilizes a screen content scraping tool installed on the devices. Additionally, the tool has the ability to record user's actions.  Having access to all the user's interactions and visual changes in the layout, it is possible to detect the coupon redemption. This allows for large-scale data acquisition while reducing the costs associated with traditional survey-based methods.

\subsubsection{Customer Engagement and Impact on Sales}  

Customer engagement metrics, such as conversion rates and the effect of e-coupon issuance on sales, can potentially be measured using statistical analysis tools operating on the seller's website~\cite{seo2023}. The conversion rate is typically derived by tracking visitor activity, while the impact on sales is estimated by correlating the updated conversion rate with the frequency of coupon issuance.  

Although this approach provides valuable insights, it relies on direct collaboration with the coupon issuer and is constrained to a single webpage. Consequently, it is not applicable to our study, as we aim to analyze arbitrary mobile applications with diverse coupon designs.

\section{Problem Statement}

The objective of this work is to extract coupons visible to the user from the content displayed on a mobile device screen. The extracted coupons should be represented as a JSON list, with each entry conforming to the format specified in Section~\ref{sec:coupon_model}.  

The screen content is provided in the form of a \texttt{.CSV} file, which encodes an XML tree structure representing the underlying screen layout. Each row in this file corresponds to a single view element within the screen hierarchy~\cite{android_view}. The dataset includes at least the following attributes:  

\begin{enumerate}
    \item \textbf{view\_depth}: The depth of the view within the XML tree hierarchy.
    \item \textbf{text}: The textual content displayed to the user within the view.
    \item \textbf{id}: A unique identifier for a screen sample. Each sample consists of a set of views observed either simultaneously or in directly consecutive snapshots.
    \item \textbf{time}: The timestamp indicating when the view was recorded.
    \item \textbf{view\_id}: The unique identifier assigned to the view by the Android API.
\end{enumerate}

An example of the dataset to illustrate described format is provided in Table~\ref{tab:dataset_example}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{view\_depth} & \textbf{text} & \textbf{id} & \textbf{time} & \textbf{view\_id} \\
        \hline
        2 & "50\% OFF" & 101 & 12:30:15 & \texttt{com.example.app:id/discount\_label} \\
        3 & "Buy 1 Get 1 Free" & 101 & 12:30:15 & \texttt{com.example.app:id/promo\_banner} \\
        2 & "Limited Offer" & 102 & 12:31:05 & \texttt{com.example.app:id/offer\_text} \\
        \hline
    \end{tabular}
    \caption{Example of dataset format representing screen content.}
    \label{tab:dataset_example}
\end{table}

Additional requirement is that the screen content processing will be performed exclusively on the end device to mitigate potential privacy concerns.

\section{Project goals}
The system will leverage machine learning to identify and structure relevant information while ensuring compatibility with mobile devices for enhanced accessibility and data privacy. The key goals of the project are as follows:

\begin{enumerate}
    \item A tool to process the data extracted from the device into a format suitable for use by the model.
    \item A machine learning tool for extracting the data that is of interest to us, such as the coupon name, expiration dates, prices, etc. The model should be capable of handling various coupon formats and layouts with high accuracy.
    \item An optional tool for post-processing the output data from the tool mentioned in the previous point into a common format.
    \item An application that runs the above three tools on a mobile device. (Optional)
    \item A key requirement is that the machine learning model must be deployable on the mobile device itself to guarantee data privacy.
\end{enumerate} 

\section{Potential applications of the project}
\subsection{Assessing coupon effectiveness}
The access to the content of mobile device screen allows us to list all the coupons seen by the user. Additionally, as we will retrieve information about coupon activation status, there will be possibility to track coupon redemptions by comparing the coupons models \textit{active} field.

Given that, our solution will aid businesses in analyzing consumer behavior and optimizing their marketing strategies. By facilitating the collection of data on coupon characteristics and their redemption rates, businesses will be able to assess the effectiveness of their coupon campaigns—determining whether they achieve the desired results. Additionally, large-scale analysis of coupon data can reveal valuable insights into purchasing patterns, preferred discount types, and the most appealing products or services.

\subsection{Market analysis and competitor monitoring}
Machine learning is proven to be a useful tool in the field of market competitors analysis but it requires significant amounts of data\cite{competitor_tariffs}.
The aforementioned gathering of data about displayed coupons can also be utilized in further monitoring of competitors' coupon strategies, their effectiveness, and whether they provide better discounts. Using machine learning to identify and analyze competitors' strategies is more cost-effective compared to exhaustive web scraping or mystery shopping \cite{competitor_tariffs}. This will enable businesses to make better informed decisions about their own marketing campaigns and provide a comprehensive understanding of the competitive landscape.


\chapter{Machine learning and the dangers associated with it}  \label{chap:ml}
% \subsection{Benchmark}
% Benchmarking is the process of running a set of, among others, computer programs against a set of tests to assess their relative performance or precision % \cite{benchmark}.

Over the past several years, artificial intelligence (AI) has been widely discussed in the media. Amid the promises of a utopian future, with self-driving cars and intelligent virtual assistants that dominate the headlines, concerns about AI are also growing. Many fear a future in which human labour has been made obsolete by automation and AI \cite{francuz_1}. Privacy concerns are also mounting, as AI models are often trained on vast datasets that may include sensitive information such as healthcare records, biometric data for facial recognition, and financial details — sometimes collected without consent \cite{ibm_privacy}. 

\section{Understanding the difference artificial intelligence, machine learning and deep learning}
Artificial intelligence (AI), machine learning (ML) and deep learning (DL) are terms often mistakenly used interchangeably to refer to the development of systems capable of performing tasks typically requiring human intelligence such as decision making and speech recognition \cite{ibm_ai}. AI is the umbrella term encompassing among others, machine learning and deep learning, as well as other approaches \cite{francuz_2}. Machine learning is a subset of AI, in which systems are able to learn and adapt without explicit rules \cite{ibm_ai}. Deep learning is a type of machine learning utilizing neural networks. This hierarchy is depicted in the image below.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{bachelor_images/nvidia_ai_hierarchy.png}
    \caption{The hierarchy of artificial intelligence, machine learning and deep learning \cite{nvidiaimage}}
    \label{fig:hierarchy-ai-ml-dl}
\end{figure}

\subsection{Artificial Intelligence}
Artificial intelligence can be succinctly described as "the effort to automate intellectual tasks normally performed by humans." The field of AI encompasses various approaches, including machine learning and deep learning, which rely on data and statistical models to identify patterns and make decisions. However, the field also includes symbolic AI, which operates differently by relying solely on predefined rules rather than data-driven models \cite{francuz_2}. An example of symbolic AI is expert systems like MYCIN, which used around 500 of IF-THEN rules to diagnose bacterial infections and recommend treatments with accuracy on par with human specialists and better than general practitioners \cite{mycin}.

\subsection{Machine Learning}
On the other hand, machine learning is a branch of artificial intelligence in which the machine is trained rather than explicitly programmed, by making inferences from the input data it is presented with \cite{francuz_3}. Two major types of machine learning are supervised and unsupervised learning. 

Supervised learning refers to an approach that relies on labeled datasets to train or "supervise" the model. The model is provided with input data along with the correct output, allowing it to learn by example. The model analyzes the relationship between the input features and the ground truth, gradually improving its ability to make accurate predictions. For example, to train a model to extract relevant coupon information such as the product name, new price, discount type, etc, the model is provided with coupon text and the corresponding labels such as for this example: 

\begin{center}
    \begin{minipage}{0.9\textwidth} % Set width to 0.9\textwidth
        \fbox{
            \parbox{\textwidth}{\raggedright\sloppy\texttt{UltraComfort Ergonomic Chair - Was \$199.99, Now \$149.99 (25\% OFF) - Limited-time offer, free shipping available, use code SAVE25NOW at checkout, offer expires April 10th, 2025.}}
        }
    \end{minipage}\\[5mm] % Adds vertical space between the boxes
    \begin{minipage}{0.9\textwidth} % Set width to 0.9\textwidth
        \fbox{
            \parbox{\textwidth}{%
                \raggedright\sloppy\texttt{%
                \{\\
                'product\_name': 'UltraComfort Ergonomic Chair',\\
                'discount': '25\%',\\
                'old\_price': '\$199.99',\\
                'new\_price': '\$149.99',\\
                'other\_discounts': [],\\
                'validity': 'April 10th, 2025'\\
                \}
                }}
        }
    \end{minipage}
\end{center}
% idk jak te warningi usunąć :((

Supervised learning is commonly used for tasks such as classification, where data is sorted into categories, and regression, where numerical values are predicted based on patterns in the data. This method is widely applied in real-world scenarios like email spam detection, image recognition, and sales forecasting. 

In contrast, unsupervised learning involves the model analyzing, clustering unlabeled data and identifying patterns without guidance from the programmer or datasets, hence it is called unsupervised learning. Unsupervised learning can be used to identify groups of products often purchased together \cite{supervised_ibm}.

In this project, we primarily employ supervised learning to develop our solution. This choice is driven by the nature of the problem, which involves identifying coupons from a screen view presented as an XML tree and extracting relevant data from them. Since this task is essentially a classification problem — where the goal is to categorize elements rather than uncover hidden patterns or group data points — supervised learning is the most suitable approach. By leveraging labeled data, the model can effectively learn to recognize and extract the desired information with accuracy and efficiency.

\subsection{Deep Learning}
Deep learning is a subset of machine learning wherein multilayered neural networks, called deep neural networks, are utilized to learn increasingly meaningful representations with each successive layer as seen in ~\ref{fig:nn_simple}; each representation is increasingly different from the original and more useful to determining the result. Traditionally, while machine learning models focus on learning one or two layers of representations of the data \cite{francuz_8}, deep learning employs at least three layers, and typically hundreds or thousands of layers to train the models \cite{ibm_dl}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{bachelor_images/nn_simple.png}
    \caption{Data representations learned by a digit-classification model \cite{francuz_8}}
    \label{fig:nn_simple}
\end{figure}

The transformation implemented by a layer is defined (parametrized) by its \textit{weights}, which are numerical parameters. Learning involves adjusting these weights to ensure the network accurately maps inputs to their corresponding targets. A deep neural network can have millions of parameters, leading to complex interdependencies, since changing one parameter affects the others. To guide this process, a \textit{loss function} measures how far the network's predictions deviate from the expected results, providing a score that reflects its performance. Deep learning relies on using the loss score as feedback to adjust the network's weights, guided by the optimizer using the backpropagation algorithm. Initially, the weights are typically random, resulting in poor predictions and a high loss score. With each example, the optimizer tweaks the weights to reduce the loss. Repeating this process across many examples gradually minimizes the loss, producing a trained network that closely matches its target outputs. This process is exemplified in ~\ref{fig:nn_function} \cite{francuz_9}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{bachelor_images/nn_function.png}
    \caption{A visualization of how a Deep Learning model works \cite{francuz_9}}
    \label{fig:nn_function}
\end{figure}

The advancement of deep learning contributed to the development of generative AI such as ChatGPT as well as Natural Language Processing (NLP) which enables machines understand and generate text and speech. This is useful for translations and extracting meaning from large quantities of data \cite{ibm_dl}.

\subsection{Transformers}
Transformers are deep learning models introduced in the 2017 paper "Attention Is All You Need" \cite{attention} by Vaswani et al., which have significantly impacted natural language processing and other sequential data tasks. Unlike traditional recurrent neural networks (RNNs), transformers utilize self-attention mechanisms to process input data in parallel, enhancing efficiency and scalability. This architecture has become foundational in models like BERT, and GPT \cite{medium_t}.

\subsubsection{Model architecture}
The proposed architecture has a encoder-decoder structure wherein, given a sequence of input symbols $ (x_1, … , x_n) $, the encoder computes a sequence of continuous vector representations $ z = (z_1, … , z_n) $, where each encodes contextual information about the corresponding within the entire input sequence. The decoder takes $ z $ and outputs a sequence $ (y_1, … , y_n) $. At every step, the model utilizes previously-generated symbols as additional input when creating the next output. This is shown in figure ~\ref{fig:transformers_fig}. 

The encoder consists of multiple layers, each typically comprising of two parts: a multi-head self-attention mechanism and a simple fully connected feed-forward network. Residual connection and layer normalization is utilized to improve model performance. 

Similarly, the decoder has a similar structure. In addition to the two parts in the encoder, each layer performs attention over the encoder’s output. To ensure the model does not look ahead in the sequence, the self-attention in the decoder is modified to prevent the model from attending to future positions, so each prediction only depends on earlier information.

Please refer to the aforementioned paper for more details.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{bachelor_images/transformer_arch.png}
    \caption{Transformer Architecture \cite{attention}}
    \label{fig:transformers_fig}
\end{figure}

% \subsubsection{Attention and multi-head attention}
% The attention mechanism is the core innovation of transformers. It allows the model to weigh the importance of each word (or token) in a sequence with respect to every other word. This is done by computing a set of attention scores, which decide how much attention one word should pay to others in the sequence.
% Multi-head attention is a key component of transformer architectures, enabling models to focus on different parts of an input sequence simultaneously. It works by projecting the input data into multiple subspaces, each corresponding to a separate attention head. These heads independently process the data, capturing various relationships and features. The outputs are then concatenated and transformed to produce the final result \cite{attention}.
% This approach allows the model to attend to multiple aspects of the input, such as different positions or semantic relationships, enhancing its ability to understand complex patterns. For example, in natural language processing tasks, one attention head might focus on syntactic structures while another captures semantic nuances. 
% By distributing the attention mechanism across multiple heads, transformers can efficiently process information in parallel, leading to improved performance in tasks like machine translation, text summarization, and language understanding \cite{medium_medium_t}. 
% The relationship between attention and multi-head attention is depicted in figure ~\ref{fig:attention_fig}.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{attention_fig.png}
%     \caption{Attention and multi-head attention \cite{attention}}
%     \label{fig:attention_fig}
% \end{figure}


\subsection{Quantization}
Quantization is a technique employed in many fields including machine learning, to reduce the precision of numerical representations within models, typically converting high-precision formats like 32-bit floating point (FP32) to lower-precision formats such as 8-bit integers (INT8). Using integer operations instead of floating-point reduces the computational and memory requirements during inference, thereby making it more efficient and faster, which is an essential benefit for real-time applications. Furthermore, quantization enables deployment on resource-constrained hardware such as smartphones and tablets, while also reducing power consumption due to lighter computational loads. Though this may slightly reduce model accuracy, the trade-off often proves worthwhile in practical applications \cite{ibm_quantization}.
The two most common quantization techniques are float32 -> float16 and float32 -> int8 \cite{quant_hf}.
\subsubsection{float32 -> float16}
Performing quantization from float32 to float16 is relatively straightforward as both data types are represented in the same manner. Float32 has the following format: 
\begin{enumerate}
	\item The most significant bit (MBS) represents the sign of the number, ie. whether it is negative or positive.
	\item The next 8 bits represent the exponent.
	\item The remaining 23 bits, are the mantissa, ie. the decimal.
\end{enumerate}

When converting from fromt32 to float16, we just remove the last 13 bits of the mantissa, creating a rounding error, and “shrinks” the exponent to fit into 5 bits. This may create a float overflow error if the float32 number is greater than $ 6.55e^4$ \cite{quant_explained}. 


\subsubsection{float32 -> int8}
This type of quantization is more complicated as an int8 can only represent 256 values which is significantly less compared to the $ 2^24 $  values represented by a float32. The idea of this quantization is to find the best way to map float32 values into the int8 space, using the affine quantization scheme: $ x = S * (x_q - Z) $. 
\begin{enumerate}
	\item $ x $ is the float32 value to be quantized.
	\item  $ x_q $ is the quantized int8 value associated with $ x $. It can be computed as follows $ x_q = round(x/S + Z) $.	
	\item $ S \in float32$ is the scale.
	\item $ Z $ is the zero-point, which is the int8 value that corresponds to 0 in the float32 range \cite{quant_hf}.
\end{enumerate}


\subsection{Finetuning}
Fine-tuning is a specialized form of transfer learning that involves adapting a pre-trained model to perform a specific task, such as identifying coupons or extracting relevant information from them, rather than identifying all kinds of objects. Instead of training a model from scratch, fine-tuning starts with a model already trained on a large dataset and further trains it using a smaller, task-specific dataset. Attempting to train a large model from scratch on a small dataset can lead to overfitting, where the model performs well on training data but generalizes poorly to unseen data. This approach is particularly beneficial for deep learning models, such as large language models (LLMs) in natural language processing or convolutional neural networks (CNNs) in computer vision, as it reduces the computational resources and labeled data required \cite{ibm_finetuning}.
Fine-tuning consists of the following steps:
\begin{enumerate}
	\item Training a source model on a large, general-purpose dataset. This enables the model to learn broadly useful feature representations.
	\item Construct a new target model by copying the architecture and parameters of the source model, excluding its final output layer. The retained parameters are presumed to encode transferable knowledge, while the output layer—being specific to the source dataset —is discarded.
	\item Introduce a new output layer tailored to the target task, ensuring it matches the number of classes in the target dataset. Initialize this layer’s parameters randomly.
	\item Train the target model on the new, task-specific dataset (e.g., a dataset of coupons. The new output layer is trained from scratch, while the remaining layers are fine-tuned using the pre-trained weights as a starting point \cite{finetune_cool_image}.
\end{enumerate}


% \subsection{}
% \subsection{Assessing the model - benchmarking}
% Benchmarking is the process of running a set of, among others, computer programs against a set of tests to assess their relative performance or precision \cite{benchmark}. Benchmarks are employed to evaluate and quantify the efficiency, accuracy, speed, and other metrics of a machine learning model or pipeline, thereby enabling the selection of the best overall solution to our problem.


% \subsection{Natural Language Processing (NLP)}

% \subsubsection{Relevant NLP models}

\section{Should We Be Afraid of AI? - Assessing the Risks and Ethical Implications of Artificial Intelligence}

% nie wydaje mi się że pisanie o adversarial attacks czy accuracy concerns w tym miejscu jest potrzebne. adversarial attacks bo nie robimy czegos co moze byc tak wykorzystane. accuracy bo bedziemy o tym pisać w kolejnych sekcjach i nie ma co od razu kota z worka wyciągać

As artificial intelligence becomes increasingly complex and integrated into our daily lives, the voices raising concerns about its dangers grow louder. Some express concern about the risks of excessive surveillance and privacy erosion, envisioning a future where AI systems are deeply intertwined with our surroundings, essentially hearing what we hear and seeing what we see \cite{not_sroka_vid} \cite{ai_scare2} \cite{ai_scare3}. Others highlight the environmental challenges tied to large-scale AI deployment, such as rising energy and water consumption, along with the need for rare materials in microchip production \cite{ai_env_concerns}. This section will focus on assessing the risks of artificial intelligence and how our solution will take them into account.

\subsection{Privacy Erosion}
AI systems heavily rely on vast amounts of user data to deploy machine learning techniques that identify subtle patterns and behaviors that may not be immediately evident, thus enabling personalized recommendations \cite{data_guard}. Social media platforms like TikTok exemplify this, as their algorithms suggest content based on users' previous interactions \cite{it_convergence} \cite{builtin}. While this personalization enhances user engagement, it also risks influencing people's opinions and shaping their worldview by trapping them in ideological echo chambers\cite{echo_chambers}.

Consequently, ‘information privacy’ is one of the primary concerns surrounding the use of AI. ‘Information privacy’ refers to the protection of personal data that is being collected, processed and stored by AI systems \cite{transcend}. Training machine learning models typically requires immense datasets, involving terabytes or even petabytes of information. Therefore, these training sets likely include sensitive user information such as healthcare records and biometric data \cite{ibm_vast_data}. Beyond explicit data collection, AI systems can also infer highly personal attributes, such as political beliefs, sexual orientation, or health conditions, from seemingly unrelated data — a phenomenon known as 'predictive harm' \cite{transcend}. This information can be utilized to subject individuals to targeted advertising, unwanted profiling, and even identity theft, often without the user’s consent or awareness \cite{data_guard}, thus posing a serious privacy risk.

Moreover, AI systems can pose ‘autonomy harms’, wherein the insights derived from data are used to influence individuals' decisions or behavior without their knowledge or proper consent. The common mindset of having "nothing to hide" overlooks the broader implications of these practices, which can undermine personal freedom and informed decision-making. A significant example of this problem is the Facebook-Cambridge Analytica scandal, in which a seemingly benign personality quiz was utilized to harvest over 87 million Facebook users’ data. Based on this data, detailed psychological profiles were constructed and leveraged to target individuals with personalized political ads during the 2016 US Presidential Election. This case underscores how AI can extract deeply personal insights from mundane user interactions, demonstrating the potential for misuse when data privacy protections are inadequate \cite{transcend}.

To address privacy concerns associated with AI systems, we are implementing locally-deployable models that process data directly on users' devices. This ensures that raw, unprocessed information never leaves the device or gets transmitted to external servers. By keeping data local, we significantly reduce the risk of unauthorized access, data breaches, or misuse. This approach empowers users with greater control over their information while still benefiting from the capabilities of AI in a secure and privacy-conscious manner. 
% ngl to jest troche bullshit ale trzeba jakoś uzasadnić że my nie jesteśmy tymi złymi

\subsection{Environmental Concerns}
AI has a notable carbon footprint due to its increasing energy consumption, particularly during model training and usage \cite{forbes_dl_env}. One study predicts that by 2027, AI-related energy consumption could reach 85–134 TWh \cite{this_study}, representing nearly 0.5\% of today's global electricity usage. This estimate is based on the energy consumption of the Nvidia A100 servers - the hardware estimated to be used by 95 percent of the A.I. market, and their projected sales in the upcoming years \cite{nyt_el}. Training LLMs typically requires significantly more energy in comparison to making a single prediction on the trained model \cite{sci_dir_comp}, for instance BERT required "the energy of a round-trip transcontinental flight" to train, while GPT-3 emitted 552 metric tons of carbon dioxide which is "the equivalent of 123 gasoline-powered passenger vehicles driven for one year" \cite{sci_am_co2}. 

Furthermore, while a quarter of the world lacks access to clean water and sanitation, data centers consume significant quantities of water during their construction and during operation to cool electrical components \cite{first}. AI server cooling requires up to 9 liters of water per kWh of energy used. Given the amount of energy required to train and operate a model, this will exacerbate the lack of water, with UN estimates stating that by 2030 half of the world’s population will be facing severe water stress \cite{water_scarcity}. It is estimated that globally, infrastructure related to AI will soon require six times more water than Denmark \cite{first}. 

In our approach, we opted to fine-tune existing language models rather than train new ones from scratch, a decision driven by both computational efficiency and environmental considerations. As highlighted by Wang et al. (2023), pre-training a model like BERT can require the equivalent energy of anywhere from 400 to 45,000 fine-tuning runs, depending on the dataset size. Their analysis shows that the number of training tokens is a reliable heuristic for estimating energy use during fine-tuning, and that sequence length significantly impacts energy intensity during this phase. These findings emphasize the substantial energy and carbon costs associated with pre-training, underscoring the environmental benefits of leveraging pre-trained models for downstream tasks \cite{finetuning_env_good}.


\chapter{Overview of Existing Solutions}  \label{chap:solutions}
To the best of our knowledge, at the time of this project's commissioning, no publicly available solutions directly addressed this problem. The most comparable approaches involve existing multimodal models. While widely used models such as ChatGPT and Gemini provide general data extraction capabilities~\cite{brinkmann2023}, they are unsuitable for our task due to their substantial computational requirements. A key limitation of these models is their large size—for example, GPT-3 consists of 175 billion parameters\cite{chatgpt_params}—rendering them impractical for deployment on mobile devices~\cite{LinguaLinked}.

Alternatively, computer vision models can be used to extract text and bounding boxes from screen images. Microsoft’s OmniParser~\cite{omniparser_intro}, for instance, has demonstrated strong performance on the ScreenSpot dataset~\cite{omniparser_intro, cheng2024}. However, the challenge of organizing extracted text into structured coupon data renders this approach unsuitable for our study. Furthermore, our experiments with running OmniParser locally on example images indicate that it relies on CUDA technology, making it impractical for deployment on mobile devices.

\section{Murmuras' existing solution} 
Murmuras' current approach relies on a set of fixed scrapping programms tailored to specific layouts from limited set of applications, making it inflexible and expansive to generalize across diverse coupon formats. This lack of adaptability limits its usefulness in real-world scenarios where coupon structures vary widely. Since our goal is to develop a solution that is easily adaptable for processing diverse mobile content, this method is not well-suited for our needs.

In contrast, Murmuras' most-recent proof of concept involves wrapping the \texttt{CSV} data with a prompt that instructs the model and sending it to GPT-4o-mini. This approach leverages an LLM to interpret the data to extract relevant coupon details. However, the reliance on an external server means the solution does not run locally on the mobile device, leading to potential privacy concerns, latency issues, and a dependence on internet connectivity. 

\section{Scapegraph AI}
ScrapeGraphAI~\cite{scapegraph_repo} is an open-source library that streamlines data extraction from websites and local documents by utilizing LLMs and graph logic to construct scraping pipelines. The library supports integration with various LLMs, including local models through the use of Ollama \cite{ollama_repo} \cite{scapegraph_usage}.

However, Scrapepraph AI provides only Python and Node.js SDKs \cite{scapegraph_sdks}, which could prove to be an issue with regard to mobile deployment, because neither Python nor Node.js is natively supported on iOS or Android \cite{android_dev_site} \cite{ios_dev_site}.

Moreover, due to mobile devices typically having limited processing power and memory compared to desktop computers or servers \cite{mobile_resources}, we cannot solely rely on the size of the model in order to improve performance. We believe that through fine-tuning LLMs, we are able to develop tools that are far more viable for edge device usage.

\chapter{Description of datasets} \label{chap:datasets}
\section{Raw datasets} 
We received datasets corresponding to six different mobile applications: Edeka, Rossmann, DM, Rewe, Lidl, and Penny. Each dataset was provided as a CSV file containing XML representations of the data captured from the device interface during user interactions with the respective app. The selection of these applications, as well as the data collection and processing procedures, were carried out by Murmuras.

For each application, two key files were provided: the \textit{content\_generic} CSV file, which contains screen-captured data in XML format (see ~\ref{tab:csv_raw_cg}), and the \textit{coupons} CSV file, which serves as the ground truth (see ~\ref{tab:csv_raw_coupons}). The latter includes the coupons that our solution is expected to identify, along with the specific information that should be extracted from them. 

We have streamlined the tables by removing the columns that were deemed unnecessary for our project, as their inclusion would have added excessive complexity without providing relevant value.

\begin{table}[h]
\centering
\begin{tabular}{lllll}
\toprule
id & time & i & view\_depth & text  \\
\midrule
12240295656 & 2024-03-07T16:55:32.405 & 1 & 3 & Coupon Text \\
\bottomrule
\end{tabular}

\vspace{1em}

\begin{tabular}{lllllll}
\toprule
description & seen\_timestamp & is\_visible & x\_1 & y\_1 & x\_2 & y\_2 \\
\midrule
  & 1709826932091 & true & 0 & 81 & 1080 & 2188 \\
\bottomrule
\end{tabular}

\caption{Content generic file format (split view for readability)}
\label{tab:csv_raw_cg}
\end{table}

\begin{sidewaystable}
\centering

\begin{tabular}{lllll}
\toprule
id & time & coupon\_i & visible \\
\midrule
12240296756 & 2024-03-07T16:55:51.831 & 2 & 1 \\
\bottomrule
\end{tabular}

\vspace{1em}

\begin{tabular}{lll}
\toprule
discount\_details & validity\_text & activation\_text \\
\midrule
Vor Aktivierung bitte Hinweise lesen. & Gültig bis 31.03.2024 & \\
\bottomrule
\end{tabular}

\vspace{1em}

\begin{tabular}{l}
\toprule
content\_full \\
\midrule
"[Düfte ,  Vor Aktivierung bitte Hinweise lesen. ,  15\% ,  Gültig bis 31.03.2024]"  \\
\bottomrule
\end{tabular}

\vspace{1em}

\begin{tabular}{llllllll}
\toprule
content\_proper & product\_text & extra & discount\_text & product & limitation & validity\_date \\
\midrule
 "[Düfte]" & Düfte & 15\% & Düfte &  & 2024-03-31 \\
\bottomrule
\end{tabular}

\caption{Coupon file format (split for readability)}
\label{tab:csv_raw_coupons}
\end{sidewaystable}


\section{Dataset Formats}
We preprocess the raw data to ensure it is structured in a manner that can be effectively utilized by the models.

\subsection{BERT Datasets}
The BERT models accept two dataset formats:

\begin{enumerate}
    \item \textbf{Plain format:} This format consists of raw, concatenated texts extracted from the `text` field in the \texttt{content\_generic} CSV file.
    \item \textbf{XML Tree format:} This format encodes the data from the \texttt{content\_generic} CSV file into an XML tree structure, which is then represented in JSON format, as shown in ~\ref{fig:json_example_xml_tree}.
\end{enumerate}



\begin{figure}[h]
\centering
\begin{tcolorbox}[sharp corners, boxrule=0.5mm, colframe=black, colback=white, coltitle=black, width=0.9\textwidth] 
\begin{BVerbatim}
{
   "text": "text field content",
   "children": {
       "child1_view_id": ...,
       "child2_view_id": ...,
       ...
   }
}
\end{BVerbatim}
\end{tcolorbox}
\caption{Sample JSON Format for BERT Dataset}
\label{fig:json_example_xml_tree}
\end{figure}

\subsection{Llama Datasets}\label{llamaDsDesc}
The Llama models take in the following dataset formats:

\begin{enumerate}
    \item \textbf{one\_input\_multiple\_outputs\_wrequest}: the dataset includes a request to the Llama model.
    \item \textbf{one\_input\_multiple\_outputs\_wthrequest}: the dataset does not include a request.
\end{enumerate}

The input format for the Llama models is as in ~\ref{fig:llama_ds_w} and ~\ref{fig:llama_ds_wth}.

\begin{figure}[h]
\centering
\begin{tcolorbox}[sharp corners, boxrule=0.5mm, colframe=black, colback=white, coltitle=black, width=0.9\textwidth] 
\begin{BVerbatim}
{
   {prompt text}
   +
   ## Input: {coupon text extracted for one timestamp}
   +
   ## Response:{response text}
}
\end{BVerbatim}
\end{tcolorbox}
\caption{Llama input format with a prompt}
\label{fig:llama_ds_w}
\end{figure}

\begin{figure}[h]
\centering
\begin{tcolorbox}[sharp corners, boxrule=0.5mm, colframe=black, colback=white, coltitle=black, width=0.9\textwidth] 
\begin{BVerbatim}
{
   ## Input: {coupon text extracted for one timestamp}
   +
   ## Response:{response text}
}
\end{BVerbatim}
\end{tcolorbox}
\caption{Llama input format without a prompt}
\label{fig:llama_ds_wth}
\end{figure}

\subsection{Dataset statistics}
This section provides an overview of the dataset statistics along with a brief analysis.

% może z tego zrobić appendix? 
\begin{table}[h!]
\centering
\begin{tabular}{|l|l|r|r|}
\hline
\textbf{Store} & \textbf{Split} & \textbf{Bytes} & \textbf{Examples} \\
\hline
Edeka     & Train & 632,413  & 298 \\
          & Test  & 208,987  & 75 \\
DM        & Train & 6,863,888 & 2,317 \\
          & Test  & 1,734,466 & 580 \\
Lidl      & Train & 8,160,797 & 3,180 \\
          & Test  & 2,194,200 & 796 \\
Penny     & Train & 51,263   & 79 \\
          & Test  & 11,724   & 20 \\
Rewe      & Train & 4,210,545 & 1,724 \\
          & Test  & 1,051,741 & 432 \\
Rossmann  & Train & 3,806,354 & 1,476 \\
          & Test  & 942,792  & 370 \\
\hline
\end{tabular}
\caption{Dataset sizes and example counts per application and split for the coupon selection stage.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|r|r|}
\hline
\textbf{Store} & \textbf{Split} & \textbf{Examples} & \textbf{Bytes} \\
\hline
Penny     & Train & 76  & 77,006 \\
          & Test  & 20  & 15,834 \\
Lidl      & Train & 3,183 & 10,186,116 \\
          & Test  & 796  & 2,094,306 \\
Rewe      & Train & 1,728 & 5,811,132 \\
          & Test  & 432  & 2,223,958 \\
Edeka     & Train & 295  & 762,506 \\
          & Test  & 74   & 118,532 \\
DM        & Train & 2,279 & 6,720,006 \\
          & Test  & 570  & 2,739,686 \\
Rossmann  & Train & 1,476 & 3,883,376 \\
          & Test  & 370  & 1,220,512 \\
\hline
\end{tabular}
\caption{Dataset size and example counts per application and split for the Llama models}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|r|r|}
\hline
\textbf{Store} & \textbf{Split} & \textbf{Examples} & \textbf{Bytes} \\
\hline
Edeka     & Train & 52  & 7,683 \\
          & Test  & 14  & 1,345 \\
DM        & Train & 748 & 106,284 \\
          & Test  & 187 & 22,945 \\
Lidl      & Train & 1,773 & 223,428 \\
          & Test  & 444  & 55,066 \\
Penny     & Train & 38  & 3,635 \\
          & Test  & 10  & 823 \\
Rewe      & Train & 668 & 126,634 \\
          & Test  & 167 & 31,203 \\
Rossmann  & Train & 366 & 57,919 \\
          & Test  & 92  & 14,303 \\
\hline
\end{tabular}
\caption{Dataset size and example counts per application and split for the BERT models}
\end{table}

\begin{table}[h!]
\raggedright
\setlength{\tabcolsep}{4pt} 
\small % or \footnotesize or \scriptsize
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\textbf{Company} & \textbf{Discount Text} & \textbf{Product Name} & \textbf{Valid Until} & \textbf{Activation Text} & \textbf{Missing (\%)} \\
\hline
DM        & 1,497 & 1,614 & 1,530 & 2,095 & 50.15\% \\
Edeka     & 58    & 17    & 338   & 336   & 92.68\% \\
Lidl      & 235   & 104   & 242   & 258   & 3.01\% \\
Penny     & 0     & 0     & 0     & 36    & 53.73\% \\
Rewe      & 45    & 63    & 20    & 3     & 0.81\% \\
Rossmann  & 0     & 0     & 7     & 18    & 0.68\% \\
\hline
\end{tabular}
\caption{Missing feature counts and percentage of coupons missing at least one feature per company}
\label{tab:missing_coupons}
\end{table}

As seen in ~\ref{tab:missing_coupons}, 92.68\% of Edeka coupons and ~50\% of DM and Penny coupons, are missing at least one feature. This missing data in coupon features presents clear challenges for building reliable extraction models as it makes it difficult for models to learn consistent patterns across sources.

A major consequence is lower recall. When key fields such as discount text, product names, or validity dates are missing during training, the model lacks enough examples to learn how to identify them reliably. This often results in false negatives, especially for sources with frequent data gaps, while performance tends to improve on more complete datasets.

Missing data also complicates evaluation. When ground truth labels are incomplete, it becomes unclear whether a model's "mistake" is truly incorrect or simply unannotated. This can skew metrics like recall and F1-score, making model performance harder to assess accurately.

\chapter{Technologies} \label{chap:technologies}

\section{General}

The main platforms used during the development process were Hugging Face\cite{hugging-face} and GitHub\cite{github}. Hugging Face served as a hub for storing and managing both trained machine learning models and datasets. During development, we also made extensive use of several Python libraries provided by Hugging Face, including \textit{datasets}\cite{lhoest2021datasetscommunitylibrarynatural}, \textit{transformers}\cite{wolf-etal-2020-transformers}, and \textit{evaluate}\cite{evaluate}. GitHub, in turn, offered version control for our code and documentation, and supported DevOps tasks such as issue tracking and automated unit testing.

Python\cite{python} was the primary programming language used, due to its broad support for machine learning workflows. Additionally, some auxiliary experiments related to Android deployment were conducted using Kotlin\cite{kotlin}\cite{service_demo_app_repo}.

\section{LLaMA Proposal}

Our first solution was based on the LLaMA architecture and utilized a custom fine-tuned version of LLaMA-3.2-1b provided by Meta\cite{meta-llama}. Fine-tuning was performed using the Unsloth\cite{unsloth} tool, which enabled efficient resource usage. Specifically, Unsloth applied the LoRA (Low-Rank Adaptation) fine-tuning technique\cite{hu2021loralowrankadaptationlarge}, reducing the number of trainable parameters to approximately 10 million. It also facilitated easy model conversion to the GGUF format.

The fine-tuning process was carried out on the Modal platform\cite{modal}, which allowed us to define execution environments directly in Python and execute code on cloud infrastructure equipped with H100 GPUs. Training progress and logs were monitored using Weights \& Biases (Wandb)\cite{wandb}.

Model inference was conducted using the Llama.cpp\cite{llama-cpp} tool, utilizing the model converted to the GGUF format.

\section{BERT Proposal}

The second approach—a two-stage BERT pipeline—was also trained on the Modal platform. Logging and experiment tracking were similarly managed using Wandb.

\section{Auxiliary Experiments}

Additional experiments conducted as part of this project involved frameworks such as SpaCy\cite{spacy}\cite{spacy-exp} and Scrapegraph AI\cite{scapegraph_repo}\cite{scrapegraph-exp}, as well as mobile deployment tools including ONNX\cite{onnx}\cite{onnx-exp}, liteRT\cite{lite-rt}\cite{lite-rt-exp}, and Executorch\cite{executorch}\cite{executorch-exp}. These experiments were developed using Android Studio\cite{android-studio}\cite{service_demo_app_repo} and Jupyter notebooks\cite{jupyter}, running in an IPython\cite{ipython} environment.


\chapter{BERT 2-stage architecture overview} \label{chap:bert}
Our proposed solution addresses the problem by framing it as a Named Entity Recognition (NER) task and employing a pipeline of two BERT models. Introduced by Google in 2018~\cite{BERT_intro}, BERT has become a standard in NLP - as of 2025-04-04, it ranks as the fourth most downloaded model on HuggingFace~\cite{BERT_hf}. The base model contains 110 million parameters, and its output format is well-suited for token classification — this combination was the primary reason for choosing it. However, we observed that the model struggled to accurately predict coupon attributes solely within the target regions when processing complex input data (see~\ref{list:sel-input}).

To mitigate this, we designed a two-stage architecture inspired by region proposal networks in computer vision~\cite{Region_proposal}. The first stage identifies coupon regions, while the second extracts their attributes. This hierarchical approach significantly reduces false positives outside coupon boundaries.
\begin{center}
   \begin{listing}
        \begin{minted}[frame=single,
                       framesep=3mm,
                       linenos=true,
                       xleftmargin=21pt,
                       tabsize=4]{js}
        {     
            "texts": ['Wasch-', '&', 'Reinigungsmittel', 'Vor',
            'Aktivierung', 'bitte', 'Hinweise', 'lesen.',
            'Deos,', 'Duschen,', 'Badezusätze', '&', 'Bodylotions',
            'Vor', 'Aktivierung', 'bitte', 'Hinweise', 'lesen.']
        }
        \end{minted}
        \caption{Pipeline input} 
        \label{list:sel-input}
    \end{listing}
\end{center}  

\section{BERT family of models}
The widespread adoption of BERT~\cite{BERT_hf} has led to numerous architectural variants. Beyond the larger BERT-Large (around 300M parameters) and multilingual adaptations, efforts have focused on enhancing performance (e.g., RoBERTa~\cite{RoBERTa}, which outperforms vanilla BERT through extended pretraining) and improving efficiency (e.g., ALBERT~\cite{ALBERT}, which reduces parameters to  around 12M~\cite{ALBERT_hf}).
Additionally, during our research, ModernBERT~\cite{ModernBERTPaper} was released, offering advancements such as extended context length (8,192 tokens), faster inference, and superior performance on benchmark tasks. On top of that, it was still comparable with the base model, having around 150M parameters~\cite{ModernBERThf}.
After evaluating these variants, we selected the BERT-base multilingual model (179M parameters~\cite{BERT_multiling}). This decision was driven by ALBERT’s significant performance drop~\cite{BERT_comp} while both RoBERTa and ModernBert lacked multilingual support~\cite{FBAIHF}, making BERT-base the optimal balance of capability and versatility for our task.

\section{2-stage architecture}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{bachelor_images/zpp.png}
    \caption{Pipeline graph}
    \label{fig:zpp}
\end{figure}
Figure \ref{fig:zpp} represents the general structure of the system. It consists of:
\begin{enumerate}
    \item \textbf{Input Data} (Listing~\ref{list:sel-input}):
    \begin{itemize}
        \item Each frame captured by the user's phone is stored as an XML tree
        \item XML data is flattened into a CSV file where entries from the same frame share a timestamp value
        \item All text data from a frame is concatenated (grouped by timestamp) and fed into the model
    \end{itemize}
    
    \item \textbf{Selection Pass}:
    \begin{itemize}
        \item First multilingual BERT model performs Named Entity Recognition (NER)
        \item Tags tokens as either \texttt{COUPON} or \texttt{UNKNOWN} in IOB2 format~\cite{iob2}
    \end{itemize}
    
    \item \textbf{Selection Pass Output}:
    \begin{itemize}
        \item Labeled tokens are reassembled into contiguous strings (grouping adjacent tokens with the same label)
        \item Only text segments labeled \texttt{COUPON} are forwarded to the next stage
    \end{itemize}
    
    \item \textbf{Extraction Pass}:
    \begin{itemize}
        \item Second multilingual BERT model processes the filtered input
        \item Performs fine-grained NER to classify tokens into four fields:
        \begin{itemize}
            \item \texttt{Activation}
            \item \texttt{Discount}
            \item \texttt{Product}
            \item \texttt{Validity text}
        \end{itemize}
        There may be cases where multiple entities are classified under the same type, such as "ACTIVATION-TEXT". To handle this, we implemented two approaches. The first approach selects only the first entity of the given type, while the second approach concatenates all entities of that type. Both methods were evaluated in benchmark tests.
    \end{itemize}
    
    \item \textbf{Pipeline Output}:
    \begin{itemize}
        \item Tokens are concatenated into strings based on their field labels
        \item Results are packaged into JSON objects
        \item Final structure matches the format shown in Listing~\ref{list:pip-out}
    \end{itemize}
\end{enumerate}
\begin{center}
   \begin{listing}
        \begin{minted}[frame=single,
                       framesep=3mm,
                       linenos=true,
                       xleftmargin=21pt,
                       tabsize=4]{js}
        [
            {     
                "ACTIVATION-TEXT" : string,
                "DISCOUNT-TEXT" : string,
                "PRODUCT-NAME" : string,
                "VALIDITY-TEXT" : string,
            }
        ],
        ...
        \end{minted}
        \caption{Pipeline output} 
        \label{list:pip-out}
    \end{listing}
\end{center}   

\section{Fine Tuning}
We implemented independent training for both BERT models in our pipeline. This approach provides several key advantages:

\begin{itemize}
    \item \textbf{Isolated Error Penalization}: Each model is only penalized for its own mistakes during training. Joint training could distort the selection model's loss function due to errors propagating from the extraction pass.
    
    \item \textbf{Data Integrity}: The extraction model trains on accurate intermediate representations, preventing potential bias from the selection model's imperfect predictions during joint training.
    
    \item \textbf{Practical Benefits}:
    \begin{itemize}
        \item Simplified implementation compared to end-to-end training
        \item Greater control over each model's learning process
        \item Easier hyperparameter tuning for individual components
    \end{itemize}
\end{itemize}

This modular training strategy ensures both models reach their optimal performance without compromising each other's learning objectives.
\subsection{Fine-tuning methodology}\label{FTMethodologyBert}
Both selection and extraction models were trained on a dataset comprising data from four retail applications: DM, Lidl, Rewe, and Rossmann. The dataset was split 80\%--20\% for training and testing respectively, following common practice~\cite{pp}, with evaluation performed after each epoch. All fine-tuning experiments were conducted under the assumption that final model performance would be benchmarked on separate datasets from Edeka and Penny applications to maintain independence between training and evaluation data.

\begin{enumerate}
    \item \textbf{General Fine-tuning}:
    \begin{itemize}
        \item Objective: Establish baseline performance metrics
        \item Approach: Models trained on combined data from all four applications
    \end{itemize}
    
    \item \textbf{Per-application Fine-tuning}:
    \begin{itemize}
        \item Objective: Evaluate cross-application generalization capabilities
        \item Approach: Individual models fine-tuned separately on each application's data, then tested on other applications to measure dataset similarity
    \end{itemize}
    
    \item \textbf{Incremental Separate Fine-tuning}:
    \begin{itemize}
        \item Objective: Test knowledge extensibility through sequential learning
        \item Approach: Four progressive models for each pass:
        \begin{enumerate}
            \item Base model fine-tuned only on DM data
            \item Subsequent fine-tuning on Lidl data
            \item Additional fine-tuning on Rewe data
            \item Final fine-tuning on Rossmann data
        \end{enumerate}
    \end{itemize}
    
    \item \textbf{Incremental Combined Fine-tuning}:
    \begin{itemize}
        \item Objective: Evaluate cumulative learning effects
        \item Approach: Similar to incremental separate fine-tuning but with concatenated datasets:
        \begin{enumerate}
            \item Initial fine-tuning on DM data only
            \item Subsequent fine-tuning on combined DM + Lidl data
            \item Additional fine-tuning on DM + Lidl + Rewe data
            \item Final fine-tuning on all four applications' data
        \end{enumerate}
    \end{itemize}
\end{enumerate}

\subsection{Selection pass specifics}
Beyond the basic fine-tuning approach, we conducted two additional experiments with the selection pass model:

\begin{enumerate}
    \item \textbf{XML Tree Preservation}:
    \begin{itemize}
        \item \textit{Hypothesis}: Maintaining structural information from the original XML tree could improve coupon detection
        \item \textit{Implementation}: Created datasets where screen content strings were wrapped in JSON structures simulating the XML hierarchy
        \item \textit{Results}:
        \begin{itemize}
            \item Increased training cost due to additional tokens (e.g., structural delimiters)
            \item Ultimately underperformed compared to plain text approach
            \item Abandoned after general fine-tuning trials (see Appendix~\ref{AppB} for details)
        \end{itemize}
    \end{itemize}
    
    \item \textbf{Token Distribution Balancing}:
    \begin{itemize}
        \item \textit{Motivation}: Early datasets contained significant class imbalance with excessive non-coupon data. Additionally, JSON format introduced even more UNKNOWN tokens.
        \item \textit{Implementation}: Applied techniques to rebalance token distribution
        \item \textit{Findings}:
        \begin{itemize}
            \item Later data batches naturally resolved the imbalance issue
            \item Final performance remained unsatisfactory (see Appendix~\ref{AppA})
            \item Approach was discontinued
        \end{itemize}
    \end{itemize}
\end{enumerate}

Both experimental approaches were ultimately abandoned in favor of the simpler, more effective plain text processing method.

\section{Selection pass fine-tuning results}
General fine-tuning experiment demonstrated that a simple data representation without any class balancing yielded the best results. As a consequence, we decided to abandon both the JSON format and the Curriculum Learning approach. Furthermore, subsequent experiments were conducted using 10 epochs instead of 30, as higher epoch counts led to increasing loss values for the base model. In the plots (\ref{fig:s_elg}, \ref{fig:s_epg}, \ref{fig:s_erg}), models labeled as "curr" were trained using the Curriculum Learning algorithm.

For per-application fine-tuning, models generally performed poorly achieving recall statistic on data they had not encountered during training. An exception was the Lidl dataset, where the model achieved over 60\% recall (\ref{fig:s_ers}).

In both incremental fine-tuning scenarios, although the model initially trained on DM data struggled with classification, further fine-tuning significantly improved its recall. This suggests that it is possible to extend the model's functionality with minimal effort through simple fine-tuning (\ref{fig:s_eras}, \ref{fig:s_erag}).

\section{Extraction pass fine-tuning results}
In the general fine-tuning setting, the extraction pass was learned smoothly. A potential concern, however, is the simultaneous increase in loss, recall, and precision, which may indicate slight overfitting (\ref{fig:e_elg}, \ref{fig:e_epg}, \ref{fig:e_erg}).

The single-application experiment produced results similar to those observed in the selection pass. Once again, the Lidl dataset appeared to be the most similar to the others (\ref{fig:e_ers}).

For both types of incremental experiments, the results closely mirrored those of the selection pass. However, in this case, fine-tuning on the Lidl dataset led to even greater performance improvements, making additional fine-tuning less impactful. This indicates that with the right dataset, a relatively small amount of data may be sufficient to expand the model’s capabilities (\ref{fig:e_eras}, \ref{fig:e_erag}).

\chapter{Fine-tuned Llama model} \label{chap:llama}
Apart from the aforementioned BERT-based proposal, we have prepared a fine-tuned version of Llama\cite{meta-llama} model to measure its performance in the studied task as a scrapping tool. The following sections focus on our setup for this proposal and the achieved results.

\section{The choice of the model}
Our primary model selection criterion was the model size - we wanted it to be deployable on as many smartphones as possible. That is why we have rejected models with more than 1 billion parameters, like \emph{Gemma 2 2B}\cite{gemma2} or \emph{Qwen2 1.5B}\cite{yang2024qwen2technicalreport}. We have also decided not to use models with acceptable size, but insufficient performance, like the 360M variant of \emph{SmolLM2}\cite{smollm2}. \emph{GPT-Neo}\cite{gpt-neo} was disqualified due to the lack of support from the Unsloth tool. Finally, we have chosen the 1B variant of the already mentioned \emph{llama3.2}.

\section{Task setup}
We assumed that the model is expected to take the content of the text fields displayed to the user on the screen and return a list of coupons in JSON format described in section~\ref{sec:coupon_model}. Additionally, we have explored the model's performance when the prompt was enhanced with the task description. However, no significant gain was achieved by us this way. We will refer to the llama fine-tuned with the task description as \emph{llama-w} and to the llama fine-tuned without it as \emph{llama-wth}. The full description of the prompt format is given in\ref{llamaDsDesc}.
\section{Fine-Tuning}
The fine-tuning was performed on the Modal platform with the help of the Unsloth tool. We used the cross-entropy loss\cite{mao2023crossentropylossfunctionstheoretical} and the AdamW optimizer adapted for 8-bit cached values\cite{hf-bnb}. Finally, we have applied weight decay and used a linear learning rate scheduler. The split between training and evaluation samples in the datasets was 80:20.

\section{Results of the fine-tuning}
In this part, we will analyze only cross-entropy loss evaluated during the training here as the models' performance on our benchmark is described in section~\ref{benchmarkResults}. The detailed plots for fine-tuning results are available in Appendix~\ref{AppH}.
\subsection{Baseline fine-tuning}
The baseline fine-tuning was performed on the datasets from four apps: REWE, ROSSMANN, LIDL and DM. During the training, the test loss was computed on the datasets from PENNY and EDEKA applications.

During the training of llama-wth, we observe the constant decrease of the training loss. It reaches the value below 0.2 after 9 epochs (see plot~\ref{fig:llama-wth-loss}). The evaluation loss drops to the value around 0.7 after initial epochs and then grows slowly to reach a value around 0.8 after 10 epochs. The test loss reached a value close to 1.3 after the first epoch and continued to grow over the rest of the epochs. This suggests that the model is overfitting to the training data.
The llama-w training plot (see~\ref{fig:llama-w-loss}) shows us that the test loss is at approximately 1.25 after the first epoch, the evaluation loss is close to 0.6 in its minimum and the train loss value after 10 epochs is around 0.1. This is a significant improvement compared to llama-wth, however, the overfitting still occurs.

\subsection{Application-wise fine-tuning}
We have also tested the model's ability to generalize when the fine-tuning was performed on the dataset consisting of only one application. In this case, we have measured the training and evaluation loss on the data from the selected application, while the test loss was computed on the EDEKA and PENNY applications, as in the baseline experiment. Each experiment came in a variant with the task description inserted into the prompt and without it.

When training on the prompt without the description, the test loss was significantly higher than in case of training on 4 applications simultaneously (see plot~\ref{fig:llama-wth-appwise}). After the first epoch it reached a value around 2.95 in case of the dataset from ROSSMANN, between 2.8 and 2.9 in case of DM and REWE, and a value slightly lower than 1.8 in case of LIDL. It continued to grow in the following epochs.
However, when the training set contained the task description, the test loss for ROSSMANN after the first epoch was on the level comparable to experiments from the previous subsection and it dropped further to a value around 1.15 after several epochs. After that, it started to grow. In case of LIDL, REWE and DM, the test loss reached its minimum after the first epoch with value 1.15 (see plot~\ref{fig:llama-w-appwise}).

\subsection{Experiments}
Additionally, we have performed experiments similar to the ones in Section~\ref{FTMethodologyBert}. We have performed the incremental combined fine-tuning where every $K$ epochs we introduced a new application to the dataset. Additionally, we have conducted an incremental separate fine-tuning where the dataset was replaced each $K$ epochs. We will refer to the newly introduced portion of the data as \emph{increment}. The results are described in the following subsubsections.
\subsubsection{Incremental combined fine-tuning}
We have tested different sizes of increments: 15, 50 and 100 (see plots~\ref{fig:llama-inc-tot-15-eval}~\ref{fig:llama-inc-tot-50-eval}~\ref{fig:llama-inc-tot-100-eval}). The evaluation loss was computed on the same applications as in the train set. When the task description is missing, we see that in each case the evaluation loss stabilizes around 0.9. However, it is worth noticing that when increment size is equal to 15, the evaluation loss drops to a value around 0.5 and then increases while in case of the greater values it starts at higher level and is decreasing as the training progresses. The test loss seems to fluctuate between 1.2 and 1.3 in the case of an increment size equal to 15 (see~\ref{fig:llama-inc-15-test}), behave in a more stable way around 1.2 for size 50 (see~\ref{fig:llama-inc-50-test}) and stabilize around 1.2 when increment size is 100 (see~\ref{fig:llama-inc-100-test}).

When training with the task description the test loss fluctuates around 1.15, even for an increment of size 15, which suggests that the amount of the data required to fine-tune existing model to the new application might be small (see plots~\ref{fig:llama-inc-tot-15-test-w}~\ref{fig:llama-inc-tot-50-test-w}~\ref{fig:llama-inc-tot-100-test-w}).

\subsubsection{Fine-tuning with dataset replacements}
In this experiment, the evaluation loss was computed on the data from all apps used in the previous increments.
When training without the task description, the evaluation loss plots look similar regardless of increment size (see plots~\ref{fig:llama-inc-15-eval}~\ref{fig:llama-inc-50-eval}~\ref{fig:llama-inc-100-eval}). The evaluation loss is at first around 0.5, then it jumps to around 0.85, finally, after the last dataset substitution, it ends around 0.9. These visualizations allow us to suggest the hypothesis that the model \emph{forgets} the overfitted dataset-specific knowledge as it no longer sees the samples from this dataset. The test loss jumps to the value above 1.4 after the first dataset swap and then drops to the value around 1.15 (see plots~\ref{fig:llama-inc-15-test}~\ref{fig:llama-inc-50-test}~\ref{fig:llama-inc-100-test}). This suggests that this methodology might be considered an interesting alternative for standard fine-tuning.

Addition of the task description further decreases the test loss, to values below 1.1 in later epochs for an increment size of 10 (see~\ref{fig:llama-inc-10-test-w}), and even reached 1.05 in the case of larger increment sizes (see~\ref{fig:llama-inc-50-test-w} and~\ref{fig:llama-inc-100-test-w}).

\chapter{Benchmark and Results} \label{chap:benchmark}

The objective of the benchmark is to enable a fair comparison of the pipelines with respect to the quality of their outputs. To achieve this, the benchmark compares the coupons extracted by each pipeline against the ground truth data provided by Murmuras and counts the number of correctly extracted coupons. This is accomplished by computing the similarities between the generated and expected coupons and applying a simple greedy matching algorithm.

\section{Coupon Similarity}

The coupon similarity metric employed in this work is based on comparing text fields using a string distance measure (for further details, see Appendix~\ref{AppE}). The resulting similarity score ranges from 0, indicating completely different texts, to 1, indicating identical texts. An alternative approach was considered, in which a large language model (LLM) would be used to assess the similarity between the expected and generated outputs~\cite{llm-as-a-judge}. The motivation for this consideration is that the generative pipeline, based on a model such as Llama, could produce a semantically correct answer but in an incorrect format, resulting in a low string similarity score despite the answer being essentially correct. However, we were unable to identify any documented instance where the LLM-as-a-Judge technique was successfully applied to a task sufficiently similar to ours, and providing a thorough validation of this approach falls outside the scope of this thesis.

\section{Benchmarking Algorithm}

For each entry in the benchmarking dataset, the similarities between all expected and generated coupons are computed. The matching process then proceeds iteratively: at each step, the coupon pair with the highest similarity score is selected and marked as matched. The matched coupons are removed from their respective sets, and the next highest-scoring pair is selected from the remaining coupons. This process is repeated until the highest remaining similarity score falls below a predefined threshold. In this work, the threshold was set to 0.8. The benchmark returns the total number of expected, generated, and matched coupons across the entire benchmarking dataset.

\section{Logging}

The benchmark logs both the results for individual entries and any unexpected events encountered during processing. For instance, during the parsing of the pipeline’s JSON output, any cases of incorrectly formatted data or coupons containing unexpected fields are recorded in the logs.

\section{Benchmarking Results} \label{benchmarkResults}

By interpreting the number of expected coupons as the sum of true positives and false negatives, the number of generated coupons as the sum of true positives and false positives, and the number of matched coupons as the number of true positives, we can compute both recall and precision. Following discussions with Murmuras, it was determined that recall is the more important metric in our context, as correctly extracting as many coupons as possible is the primary objective, while false positives are considered to be of lesser concern.

In Figures~\ref{fig:berts_recall} to~\ref{fig:llama_vs_bert_precision}, the models are evaluated across six datasets. The Penny and Edeka datasets were used in their entirety, while for DM, Lidl, Rewe, and Rossmann, only the test splits were used. Figures~\ref{fig:llama_appwise_recall} to~\ref{fig:bert_appwise_precision}, however, follow a different evaluation protocol. In these figures, each app-wise pipeline was trained and evaluated exclusively on a single dataset. For example, the Llama-with-appwise pipeline evaluated on DM was trained solely on DM, and is therefore distinct from the same-named pipeline evaluated on Lidl. In this case as well, only the test splits of DM, Lidl, Rewe, and Rossmann were used for evaluation.

For a detailed description of the datasets, see Chapter~\ref{chap:datasets}. Information on the BERT-based pipelines is provided in Chapter~\ref{chap:bert}, while the Llama-based pipelines are discussed in Chapter~\ref{chap:llama}.

\subsection{Figures~\ref{fig:berts_recall} to~\ref{fig:llama_vs_bert_precision}}

Based on Figures~\ref{fig:berts_recall} and~\ref{fig:berts_precision}, BERT-top\_score emerged as the best-performing variant within the BERT-based pipelines. It consistently outperformed BERT-first in both recall and precision. While the comparison with BERT-concat is less clear-cut, BERT-top\_score achieved more than twice the recall on the DM dataset, which was a decisive factor in its overall evaluation. 

Similarly, Figures~\ref{fig:llama_wth_vs_w_recall} and~\ref{fig:llama_wth_vs_w_precision} show that Llama-wth outperforms Llama-w. While Llama-w demonstrated slightly better precision, recall was prioritized as the more important metric, as previously discussed. 

When comparing BERT-top\_score to Llama-wth (Figures \ref{fig:llama_vs_bert_recall} and \ref{fig:llama_vs_bert_precision}), the latter clearly emerges as the superior approach in terms of output quality.

Overall, the pipelines performed significantly better on the datasets they were trained on—namely DM, Lidl, Rewe, and Rossmann—compared to the previously unseen datasets, Penny and Edeka. However, Figures~\ref{fig:llama_wth_vs_w_recall} and~\ref{fig:llama_wth_vs_w_precision} suggest that the Llama-based pipelines exhibit a higher potential for generalization than their BERT-based counterparts.

\subsection{Figures~\ref{fig:llama_appwise_recall} to~\ref{fig:bert_appwise_precision}}

In Figure~\ref{fig:llama_appwise_recall}, Llama-wth outperformed each app-wise pipeline on its respective dataset, with the most pronounced improvement observed on the DM dataset. In contrast, Figure~\ref{fig:llama_appwise_precision} shows the opposite trend, where app-wise models achieved higher precision. This indicates that training on a bigger and more diverse dataset improved recall but resulted in a trade-off with precision.

A similar experiment, presented in Figures~\ref{fig:bert_appwise_recall} and~\ref{fig:bert_appwise_precision}, yielded less conclusive results for the BERT-based pipelines. For DM, Lidl, and Rewe, the recall and precision scores were only marginally affected by the increased dataset size and heterogeneity. However, for Rossmann, both recall and precision were notably higher for the app-wise model.

\section{Llama Quantizations}

After identifying Llama-wth as the best-performing pipeline variant, we evaluated several quantized versions of the underlying model—specifically \texttt{Q8\_0}, \texttt{Q4\_K\_M}, and \texttt{Q4\_0}~\cite{llama-cpp-quantization}—in terms of recall, precision, inference speed on a mobile device, and GGUF file size. 

The model speed was evaluated by running the Llama-wth model with various quantization schemes using the \texttt{llama.cpp} framework (see Chapter~\ref{chap:technologies}) with the default CPU backend on a Samsung A25 device equipped with 6~GB of RAM. An initial warm-up run was performed using a short prompt of approximately 10 tokens, during which 16 tokens were generated. For the actual measurement, a prompt of around 300 tokens—sampled from one of the datasets—was used, and the model generated 64 tokens.

Based on the results shown in Figures~\ref{fig:quantization_effect_on_recall},~\ref{fig:quantization_effect_on_precision},~\ref{fig:quantization_effect_on_model_speed}, and~\ref{fig:quantization_effect_on_gguf_file_size}, we found that quantization introduced no substantial degradation in output quality while offering clear improvements in inference speed and reduced model size. The latter also contributes to lower memory (RAM) usage during execution.

\chapter{Conclusion} \label{chap:conclusion}
\subsection{TODO}

\let\cleardoublepage\clearpage
\begin{appendices}
\chapter{Curriculum Learning Approach} \label{AppA}

To address class imbalance in Murmuras-provided data and the JSON-based selection pass, we implemented a curriculum learning algorithm inspired by curriculum learning \cite{CurrLearn}. This approach mimics human learning progression by gradually increasing task difficulty:

\begin{itemize}
    \item \textbf{Implementation}:
    \begin{itemize}
        \item Initial training on balanced datasets (equal coupon/non-coupon examples)
        \item Progressive introduction of more non-coupon samples
        \item Gradual complexity increase mirroring human learning patterns
    \end{itemize}
    
    \item \textbf{Observed Advantages}:
    \begin{itemize}
        \item Smoother, more stable loss descent during training
        \item Reduced overfitting compared to baseline approaches
        \item More predictable learning trajectory
    \end{itemize}
    
    \item \textbf{Performance Outcomes} \ref{fig:s_elg}, \ref{fig:s_epg}, \ref{fig:s_erg}:
    \begin{itemize}
        \item Consistently inferior metrics (accuracy/recall) versus baseline
        \item 30-epoch evaluation showed no competitive improvement
        \item Final decision to exclude from production pipeline
    \end{itemize}
\end{itemize}

Despite its theoretical benefits and training stability, the curriculum approach was ultimately abandoned as baseline methods demonstrated superior practical performance across all key metrics. This suggests that for our specific task and data characteristics, direct exposure to the true data distribution may be preferable to gradual introduction of complexity.

\begin{algorithm}
\caption{Curriculum Data Preparation Algorithm}
\begin{algorithmic}[1]
\Require Dataset $D$ with fields \texttt{texts}, \texttt{labels}
\Require Number of splits $S > 0$
\Ensure Sequence of training datasets

\State Initialize $\mathcal{R}_c \gets \{\}$, $\mathcal{R}_{\neg c} \gets \{\}$
\For{each row $r$ in $D$}
    \If{$r$ contains coupon labels}
        \State Extract spans of contiguous coupon tokens
        \State Add span info to $\mathcal{R}_c$
    \Else
        \State Add $r$ to $\mathcal{R}_{\neg c}$
    \EndIf
\EndFor

\State Store $\text{init\_len} \gets |\mathcal{R}_c|$
\For{each row $r$ in $\mathcal{R}_c$}
    \State Extend spans of $r$ proportionally using \textsc{ExtendSpans}
\EndFor
\State Yield initial dataset from $\mathcal{R}_c$

\For{$i = 1$ to $S-1$}
    \If{$i = S-1$}
        \State Append all of $\mathcal{R}_{\neg c}$ to $\mathcal{R}_c$
    \ElsIf{$i \bmod 2 = 0$}
        \State Append a subset of $\mathcal{R}_{\neg c}$ to $\mathcal{R}_c$
    \Else
        \For{each row $r$ in $\mathcal{R}_c$}
            \State Extend spans of $r$ proportionally
        \EndFor
    \EndIf
    \State Yield dataset from $\mathcal{R}_c$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{ExtendSpans Procedure}
\begin{algorithmic}[1]
\Procedure{ExtendSpans}{$spans, amount, max\_len$}
\If{$spans = \emptyset$}
    \State Create single span $[0, amount]$
\Else
    \State Distribute $amount$ evenly across spans
    \State Shift span boundaries without overlap
    \State Greedily expand remaining budget
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\chapter{BERT Selection pass JSON format} \label{AppB}

For the selection pass in our two-stage architecture, we evaluated two alternative data formats:

\begin{itemize}
    \item \textbf{Plain format}: A direct representation of the textual screen content for each timestamp (Listing~\ref{list:sel-input})
    \item \textbf{JSON format}: An enriched representation that additionally encodes the XML tree structure of the Android screen content (Listing~\ref{list:sel-json})
\end{itemize}

Following comprehensive fine-tuning experiments, we discontinued the JSON format approach due to several factors:

\begin{itemize}
    \item Performance metrics during training were comparable to or worse than the curriculum learning approach
    \item The JSON structure increased token count, resulting in:
    \begin{itemize}
        \item Longer training times
        \item Higher computational costs
    \end{itemize}
    \item The approach failed to deliver satisfactory improvements in model accuracy
\end{itemize}

The comparative results are presented in Figures~\ref{fig:s_elg}, \ref{fig:s_epg}, and~\ref{fig:s_erg}.
\begin{center}
   \begin{listing}
        \begin{minted}[frame=single,
                       framesep=3mm,
                       linenos=true,
                       xleftmargin=21pt,
                       tabsize=4]{js}
        {
            "text": null,
            "children": {
                "nan": {
                    "text": "Sensodyne",
                    "children": {}
                },
                "nan_0": {
                    "text": "G\\u00fcltig', 'bis', '15.09.2024",
                    "children": {}
                },
                "nan_1": {
                    "text": "Yippy",
                    "children": {}
                }
            }
        }
        \end{minted}
        \caption{Pipeline JSON format input example} 
        \label{list:sel-json}
    \end{listing}
\end{center}

\chapter{BERT selection pass fine-tuning results} \label{AppC}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/s_elg.png}
    \caption{Eval/loss statistics of selection pass during general training}
    \label{fig:s_elg}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/s_epg.png}
    \caption{Eval/precision statistics of selection pass during general training}
    \label{fig:s_epg}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/s_erg.png}
    \caption{Eval/recall statistics of selection pass during general training}
    \label{fig:s_erg}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/s_ers.png}
    \caption{Eval/recall statistics of selection pass during training on separate apps}
    \label{fig:s_ers}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/s_eras.png}
    \caption{Eval/recall statistics of selection pass during incremental separate training}
    \label{fig:s_eras}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/s_erag.png}
    \caption{Eval/recall statistics of selection pass during incremental combined training}
    \label{fig:s_erag}
\end{figure}

\chapter{BERT extraction pass fine-tuning results} \label{AppD}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/e_elg.png}
    \caption{Eval/loss statistics of extraction pass during general training}
    \label{fig:e_elg}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/e_epg.png}
    \caption{Eval/precision statistics of extraction pass during general training}
    \label{fig:e_epg}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/e_erg.png}
    \caption{Eval/recall statistics of extraction pass during general training}
    \label{fig:e_erg}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/e_ers.png}
    \caption{Eval/recall statistics of extraction pass during separate training}
    \label{fig:e_ers}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/e_eras.png}
    \caption{Eval/recall statistics of extraction pass during incremental separate training}
    \label{fig:e_eras}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/e_erag.png}
    \caption{Eval/recall statistics of extraction pass during incremental combined training}
    \label{fig:e_erag}
\end{figure}

\chapter{Coupon Similarity Metric} \label{AppE}
The similarity between coupons is computed by comparing their textual fields using the \texttt{ratio()} method of Python's \texttt{difflib.SequenceMatcher} class~\cite{python-difflib}. The similarity computation algorithm is presented in two stages: first, the baseline approach is described, followed by an improved version. 

The input to the algorithm consists of an expected coupon and a generated coupon, both containing the text fields \texttt{PRODUCT-NAME}, \texttt{DISCOUNT-TEXT}, \texttt{VALIDITY-TEXT}, and \texttt{ACTIVATION-TEXT}.

\section{Baseline Approach}
The similarity of each text field is computed individually. For instance, if \texttt{text\_1} denotes the value of the \texttt{PRODUCT-NAME} field in the expected coupon and \texttt{text\_2} denotes the corresponding field in the generated coupon, their similarity is computed as follows:

\begin{lstlisting}[language=Python]
similarity = difflib.SequenceMatcher(a=text_1, b=text_2).ratio()
\end{lstlisting}

Each field is assigned a weight reflecting its relative importance: \texttt{PRODUCT-NAME} is weighted 0.4, \texttt{DISCOUNT-TEXT} is weighted 0.3, \texttt{VALIDITY-TEXT} is weighted 0.2, and \texttt{ACTIVATION-TEXT} is weighted 0.1. The final similarity score is calculated as the weighted sum of the individual field similarities.

However, this approach has a notable flaw. Consider the case where the expected coupon has only the \texttt{PRODUCT-NAME} field filled with the string \texttt{"a"}, while the generated coupon has the \texttt{PRODUCT-NAME} field filled with \texttt{"b"}; all other fields are empty. Despite the minimal information provided, the resulting similarity score would be 0.6, which overestimates the similarity.

\section{Improved Algorithm}
To mitigate this issue, the improved algorithm modifies how empty fields are handled. Specifically, if a given field is empty in both the expected and generated coupons, its contribution to the final similarity score is ignored. The final similarity score is then rescaled accordingly to reflect only the non-empty fields. For example, if both the \texttt{VALIDITY-TEXT} and \texttt{ACTIVATION-TEXT} fields are empty in both coupons, their similarities are considered to be 0, and the final similarity score is divided by 0.7, that is, the sum of the remaining fields' weights. In case the sum of remaining fields' weights is 0, the similarity of the coupons is set to 1.

\chapter{Benchmarking Results} \label{AppF}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/benchmark/berts_recall.png}
    \caption{BERT-first vs BERT-concat vs BERT-top\_score - recall}
    \label{fig:berts_recall}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/benchmark/berts_precision.png}
    \caption{BERT-first vs BERT-concat vs BERT-top\_score - precision}
    \label{fig:berts_precision}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/benchmark/llama_wth_vs_w_recall.png}
    \caption{Llama-wth vs Llama-w - recall}
    \label{fig:llama_wth_vs_w_recall}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/benchmark/llama_wth_vs_w_precision.png}
    \caption{Llama-wth vs Llama-w - precision}
    \label{fig:llama_wth_vs_w_precision}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/benchmark/llama_vs_bert_recall.png}
    \caption{Llama-wth vs BERT-concat - recall}
    \label{fig:llama_vs_bert_recall}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/benchmark/llama_vs_bert_precision.png}
    \caption{Llama-wth vs BERT-concat - precision}
    \label{fig:llama_vs_bert_precision}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/benchmark/llama_wth_appwise_recall.png}
    \caption{Llama-wth-appwise family vs Llama-wth - recall}
    \label{fig:llama_appwise_recall}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/benchmark/llama_wth_appwise_precision.png}
    \caption{Llama-wth-appwise family vs Llama-wth - precision}
    \label{fig:llama_appwise_precision}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/benchmark/bert_top_score_appwise_recall.png}
    \caption{BERT-top\_score-appwise family vs BERT-top\_score - recall}
    \label{fig:bert_appwise_recall}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/benchmark/bert_top_score_appwise_precision.png}
    \caption{BERT-top\_score-appwise family vs BERT-top\_score - precision}
    \label{fig:bert_appwise_precision}
\end{figure}

\chapter{Comparison of Llama Quantizations} \label{AppG}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/quant_comp/quantization_effect_on_recall.png}
    \caption{Llama-wth quantization effect on recall}
    \label{fig:quantization_effect_on_recall}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/quant_comp/quantization_effect_on_precision.png}
    \caption{Llama-wth quantization effect on precision}
    \label{fig:quantization_effect_on_precision}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/quant_comp/quantization_effect_on_model_speed.png}
    \caption{Llama-wth quantization effect on model speed}
    \label{fig:quantization_effect_on_model_speed}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/quant_comp/quantization_effect_on_gguf_file_size.png}
    \caption{Llama-wth quantization effect on GGUF file size}
    \label{fig:quantization_effect_on_gguf_file_size}
\end{figure}

\chapter{Llama finetuning plots} \label{AppH}
Note that the epoch numbering starts from 0. That means the leftmost value on the plots is the value after the first epoch.
In case of incremental fine-tunings, the series name indicates the source of the data in increment that was added recently.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-wth-loss.png}
    \caption{Llama fine-tuning losses for baseline experiment.}
    \label{fig:llama-wth-loss}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-w-loss.png}
    \caption{Llama fine-tuning losses for baseline experiment (with task description).}
    \label{fig:llama-w-loss}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-appwise-wth-test.png}
    \caption{Llama fine-tuning test losses for application-wise runs.}
    \label{fig:llama-wth-appwise}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-appwise-w-test.png}
    \caption{Llama fine-tuning test losses for application-wise runs (with task description).}
    \label{fig:llama-w-appwise}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-15-tot-eval.png}
    \caption{Llama evaluation loss for fine-tuning with concatenated increments of size 15.}
    \label{fig:llama-inc-tot-15-eval}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-15-tot-test.png}
    \caption{Llama test loss for fine-tuning with concatenated increments of size 15.}
    \label{fig:llama-inc-tot-15-test}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-50-tot-eval.png}
    \caption{Llama evaluation loss for fine-tuning with concatenated increments of size 50.}
    \label{fig:llama-inc-tot-50-eval}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-50-tot-test.png}
    \caption{Llama test loss for fine-tuning with concatenated increments of size 50.}
    \label{fig:llama-inc-tot-50-test}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-100-tot-eval.png}
    \caption{Llama evaluation loss for fine-tuning with concatenated increments of size 100.}
    \label{fig:llama-inc-tot-100-eval}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-100-tot-test.png}
    \caption{Llama test loss for fine-tuning with concatenated increments of size 100.}
    \label{fig:llama-inc-tot-100-test}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-15-eval-prev.png}
    \caption{Llama evaluation loss for previous increments for fine-tuning with increments of size 15.}
    \label{fig:llama-inc-15-eval}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-50-eval-prev.png}
    \caption{Llama evaluation loss for previous increments for fine-tuning with increments of size 50.}
    \label{fig:llama-inc-50-eval}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-100-eval-prev.png}
    \caption{Llama evaluation loss for previous increments for fine-tuning with increments of size 100.}
    \label{fig:llama-inc-100-eval}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-15-test.png}
    \caption{Llama test loss for fine-tuning with increments of size 15.}
    \label{fig:llama-inc-15-test}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-50-test.png}
    \caption{Llama test loss for fine-tuning with increments of size 50.}
    \label{fig:llama-inc-50-test}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-100-test.png}
    \caption{Llama test loss for fine-tuning with increments of size 100.}
    \label{fig:llama-inc-100-test}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-15-tot-test-w.png}
    \caption{Llama test loss for fine-tuning with concatenated increments of size 15.}
    \label{fig:llama-inc-tot-15-test-w}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-50-tot-test-w.png}
    \caption{Llama test loss for fine-tuning with concatenated increments of size 50.}
    \label{fig:llama-inc-tot-50-test-w}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-100-tot-test-w.png}
    \caption{Llama test loss for fine-tuning with concatenated increments of size 100.}
    \label{fig:llama-inc-tot-100-test-w}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-10-test-w.png}
    \caption{Llama test loss for fine-tuning with increments of size 10.}
    \label{fig:llama-inc-10-test-w}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-50-test-w.png}
    \caption{Llama test loss for fine-tuning with increments of size 50.}
    \label{fig:llama-inc-50-test-w}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bachelor_images/llama_ft/llama-inc-100-test-w.png}
    \caption{Llama test loss for fine-tuning with increments of size 100.}
    \label{fig:llama-inc-100-test-w}
\end{figure}

\end{appendices}

% \chapter{Technologies}
% \chapter{Architecture design}
% \chapter{Pipelines}
% \chapter{Benchmark}
% \chapter{Performance}
% \chapter{Possible extensions}
% \chapter{Conclusion}
% \chapter{Charts}

\begin{thebibliography}{99}

\addcontentsline{toc}{chapter}{Bibliography}
\raggedright

\bibitem{murmuras} 
\textit{Murmuras website}.  
\url{https://murmuras.com/}.  
[Accessed 2025-02-11].

\bibitem{seo2023}
\textit{Seo, D., \& Yoo, Y. (2023). Improving Shopping Mall Revenue by Real-Time Customized Digital Coupon Issuance. IEEE Access, 11, 7924–7932.}
\url{https://doi.org/10.1109/ACCESS.2023.3239425}

\bibitem{coupon_definition}
\textit{Britannica Dictionary definition of COUPON}.
\url{https://www.britannica.com/dictionary/coupon}.
[Accessed 2025-02-03].

\bibitem{nayal2021}
\textit{Nayal, P., \& Pandey, N. (2021). What Makes a Consumer Redeem Digital Coupons? Behavioral Insights from Grounded Theory Approach. *Journal of Promotion Management*, 28(3), 205–238.}
\url{https://doi.org/10.1080/10496491.2021.1989541}

\bibitem{danaher2015}
\textit{Danaher, P. J., Smith, M. S., Ranasinghe, K., \& Danaher, T. S. (2015). *Where, when, and how long: Factors that influence the redemption of mobile phone coupons.* *Journal of Marketing Research*, *52*(5), 710--725.}
\url{https://journals.sagepub.com/doi/full/10.1509/jmr.13.0341}

\bibitem{jayadharshini2023}
\textit{Jayadharshini, P., Sharon Roji, Priya. C, Lalitha, K., Santhiya, S., Keerthika, S., \& Abinaya, N. (2023). *Enhancing Retailer Auctions and Analyzing the Impact of Coupon Offers on Customer Engagement and Sales Through Machine Learning.* *2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS)*, 1–6. }
\url{https://doi.org/10.1109/ICCEBS58601.2023.10448900}

\bibitem{design_of_coupons}
Xiong Keyi, Yang Wensheng
\textit{Research on the Design of E-coupons for Directional Marketing of Two Businesses in Competitive Environment}.
\url{https://www.sciencepublishinggroup.com/article/10.11648/j.ijefm.20200801.16}.
[Accessed 2025-02-04].

\bibitem{li2024}
\textit{Li, J. (2024). The evolution, applications, and future prospects of large language models: An in-depth overview. Applied and Computational Engineering 35, 234–244.}\url{https://doi.org/10.54254/2755-2721/35/20230399}

\bibitem{sui2024}
\textit{Sui, Y., Zhou, M., Zhou, M., Han, S., \& Zhang, D. (2024). Table meets LLM: Can large language models understand structured table data? A benchmark and empirical study. Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 645--654.}

\bibitem{targeted_reminders}
Li Li, et. al.
\textit{Targeted reminders of electronic coupons: using predictive analytics to facilitate coupon marketing}.
\url{https://link.springer.com/article/10.1007/s10660-020-09405-4}.
[Accessed 2025-02-04].

\bibitem{competitor_tariffs}
Bernhard König, et. al.
\textit{Analysing competitor tariffs with machine learning}.
\url{https://www.milliman.com/en/insight/analysing-competitor-tariffs-with-machine-learning}.
[Accessed 2025-02-04].

\bibitem{ml_general}
Iqbal H. Sarker
\textit{Machine Learning: Algorithms, Real-World Applications and Research Directions}.
\url{https://link.springer.com/article/10.1007/s42979-021-00592-x}.
[Accessed 2025-02-05].

\bibitem{emarketer_coupon_stats}
Sara Lebow
\textit{How consumers access digital coupons}.
\url{https://www.emarketer.com/content/how-consumers-access-digital-coupons}.
[Accessed 2025-02-05].

\bibitem{coupon_stats_2}
\textit{Unveiling IT Coupons Trends and Statistics}
\url{https://www.go-globe.com/unveiling-it-coupons-trends-statistics/}.
[Accessed 2025-02-05].

\bibitem{android_view}
\textit{Android API Reference - View}
\url{https://developer.android.com/reference/android/view/View}
[Accessed 2025-03-11]

\bibitem{brinkmann2023}
\textit{Brinkmann, A., Shraga, R., Der, R. C., \& Bizer, C. (2023). Product Information Extraction using ChatGPT. arXiv preprint, arXiv:2306.14921.}
\url{https://arxiv.org/abs/2306.14921}

\bibitem{chatgpt_params}
\textit{Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and
Dario Amodei. Language models are few-shot learners, 2020}

\bibitem{scapegraph_repo}
Marco Perini, Lorenzo Padoan, Marco Vinciguerra
\textit{Scrapegraph-ai}.
\url{https://github.com/VinciGit00/Scrapegraph-ai}.
[Accessed 2025-02-24].

\bibitem{ollama_repo}
\textit{Ollama}.
\url{https://github.com/ollama/ollama}.
[Accessed 2025-02-24].

\bibitem{scapegraph_usage}
Marco Perini, Lorenzo Padoan, Marco Vinciguerra
\textit{Scrapegraph-ai usage}.
\url{https://github.com/ScrapeGraphAI/Scrapegraph-ai?tab=readme-ov-file#-usage}.
[Accessed 2025-02-24].

\bibitem{scapegraph_sdks}
Marco Perini, Lorenzo Padoan, Marco Vinciguerra
\textit{Scrapegraph-ai API and SDKs}.
\url{https://github.com/ScrapeGraphAI/Scrapegraph-ai?tab=readme-ov-file#-scrapegraph-api--sdks}.
[Accessed 2025-02-24].

\bibitem{android_dev_site}
\textit{Android developer fundamentals website}.
\url{https://developer.android.com/guide/components/fundamentals}.
[Accessed 2025-02-24].

\bibitem{ios_dev_site}
\textit{Apple developer website}.
\url{https://developer.apple.com/develop/}.
[Accessed 2025-02-24].

\bibitem{omniparser_intro}
\textit{Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omni-
parser for pure vision based gui agent, 2024.}

\bibitem{cheng2024}
\textit{Cheng, K., Sun, Q., Chu, Y., Xu, F., Li, Y., Zhang, J., \& Wu, Z. (2024). SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents. arXiv preprint, arXiv:2401.10935.} \url{https://arxiv.org/abs/2401.10935}

\bibitem{mobile_resources}
Xiang Li, et. al.
\textit{Large Language Models on Mobile Devices: Measurements, Analysis, and Insights}
\url{https://dl.acm.org/doi/10.1145/3662006.366205}

\bibitem{LinguaLinked}
Junchen Zhao, et. al.
\textit{LinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices}
\url{https://arxiv.org/pdf/2312.00388}

\bibitem{sequence_matcher}
\textit{difflib — Helpers for computing deltas}
\url{https://docs.python.org/3/library/difflib.html}

\bibitem{francuz_1}
page 1
\textit{Deep Learning with Python}
\url{https://sourestdeeds.github.io/pdf/Deep%20Learning%20with%20Python.pdf}

\bibitem{francuz_2}
pages 2 and 3
\textit{Deep Learning with Python}
\url{https://sourestdeeds.github.io/pdf/Deep%20Learning%20with%20Python.pdf}

\bibitem{francuz_3}
pages 3 and 4
\textit{Deep Learning with Python}
\url{https://sourestdeeds.github.io/pdf/Deep%20Learning%20with%20Python.pdf}

\bibitem{francuz_8}
pages 7 and 8
\textit{Deep Learning with Python}
\url{https://sourestdeeds.github.io/pdf/Deep%20Learning%20with%20Python.pdf}

\bibitem{francuz_9}
pages 8 - 10
\textit{Deep Learning with Python}
\url{https://sourestdeeds.github.io/pdf/Deep%20Learning%20with%20Python.pdf}

\bibitem{nvidiaimage}
\textit{What’s the Difference Between Artificial Intelligence, Machine Learning and Deep Learning?}
\url{https://blogs.nvidia.com/blog/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/}

\bibitem{ibm_ai}
\textit{What is artificial intelligence (AI)?}
\url{https://www.ibm.com/think/topics/artificial-intelligence}

\bibitem{ibm_privacy}
\textit{Exploring privacy issues in the age of AI}
\url{https://www.ibm.com/think/insights/ai-privacy}

\bibitem{mycin}
\textit{MYCIN}
\url{https://www.britannica.com/technology/MYCIN}

\bibitem{supervised_ibm}
\textit{Supervised versus unsupervised learning: What's the difference?}
\url{https://www.ibm.com/think/topics/supervised-vs-unsupervised-learning}

\bibitem{benchmark} 
\textit{Computer Benchmark}.
\url{https://bhatabhishek-ylp.medium.com/benchmarking-in-computer-c6d364681512}.
[Accessed 2025-02-03].

\bibitem{not_sroka_vid} 
\textit{The privacy paradox with AI}.
\url{https://www.reuters.com/legal/legalindustry/privacy-paradox-with-ai-2023-10-31/}.
[Accessed 2025-03-24].

\bibitem{ai_scare2}
\textit{Beware the Privacy Violations in Artificial Intelligence Applications}
\url{https://www.isaca.org/resources/news-and-trends/isaca-now-blog/2021/beware-the-privacy-violations-in-artificial-intelligence-applications}
[Accessed 2025-04-01]

\bibitem{ai_scare3}
\textit{AI – the threats it poses to reputation, privacy and cyber security, and some practical solutions to combating those threats}
\url{https://www.taylorwessing.com/en/global-data-hub/2024/cyber-security---weathering-the-cyber-storms/ai---the-threats-it-poses-to-reputation}

\bibitem{ai_env_concerns}
\textit{AI has an environmental problem. Here’s what the world can do about that.}
\url{https://www.unep.org/news-and-stories/story/ai-has-environmental-problem-heres-what-world-can-do-about}
[Accessed 2025-03-24]

\bibitem{it_convergence} 
\textit{Top Use Cases of AI-Based Recommendation Systems}.
\url{https://www.itconvergence.com/blog/top-use-cases-of-ai-based-recommendation-systems/}.
[Accessed 2025-03-24].

\bibitem{builtin} 
\textit{ 14 Risks and Dangers of Artificial Intelligence (AI)}.
\url{https://builtin.com/artificial-intelligence/risks-of-artificial-intelligence}.
[Accessed 2025-03-24].

\bibitem{data_guard} 
\textit{The growing data privacy concerns with AI: What you need to know}.
\url{https://www.dataguard.com/blog/growing-data-privacy-concerns-ai/}.
[Accessed 2025-03-24].

\bibitem{transcend} 
\textit{Examining Privacy Risks in AI Systems}.
\url{https://transcend.io/blog/ai-and-privacy}.
[Accessed 2025-03-24].

\bibitem{ibm_vast_data} 
\textit{Exploring privacy issues in the age of AI}.
\url{https://www.ibm.com/think/insights/ai-privacy}.
[Accessed 2025-03-24].


\bibitem{forbes_dl_env}
\textit{Deep Learning’s Carbon Emissions Problem}.
\url{https://www.forbes.com/sites/robtoews/2020/06/17/deep-learnings-climate-change-problem/}.
[Accessed 2025-03-24].

\bibitem{this_study}
\textit{The growing energy footprint of artificial intelligence}.
\url{https://www.researchgate.net/publication/374598219_The_growing_energy_footprint_of_artificial_intelligence}.
%https://www.cell.com/joule/pdf/S2542-4351(23)00365-3.pdf
[Accessed 2025-04-16].

\bibitem{nyt_el}
\textit{A.I. Could Soon Need as Much Electricity as an Entire Country}.
\url{https://www.nytimes.com/2023/10/10/climate/ai-could-soon-need-as-much-electricity-as-an-entire-country.html}.
[Accessed 2025-03-24].

\bibitem{sci_dir_comp}
\textit{Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning}.
\url{https://www.sciencedirect.com/science/article/pii/S2210537923000124#sec7}.
[Accessed 2025-03-24].

\bibitem{sci_am_co2}
\textit{A Computer Scientist Breaks Down Generative AI’s Hefty Carbon Footprint}.
\url{https://www.scientificamerican.com/article/a-computer-scientist-breaks-down-generative-ais-hefty-carbon-footprint/}.
[Accessed 2025-03-24].

\bibitem{water_scarcity}
\textit{AI Is Accelerating the Loss of Our Scarcest Natural Resource: Water}.
\url{https://www.forbes.com/sites/cindygordon/2024/02/25/ai-is-accelerating-the-loss-of-our-scarcest-natural-resource-water/}.
[Accessed 2025-03-24].

\bibitem{first}
\textit{AI has an environmental problem. Here’s what the world can do about that.}.
\url{https://www.unep.org/news-and-stories/story/ai-has-environmental-problem-heres-what-world-can-do-about}.
[Accessed 2025-03-24].

\bibitem{ibm_dl}
\textit{What is deep learning?}.
\url{https://www.ibm.com/think/topics/deep-learning}.
[Accessed 2025-03-24].

\bibitem{BERT_intro}
\textit{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}.
\url{https://arxiv.org/abs/1810.04805v2}.
[Accessed 2025-04-04]

\bibitem{BERT_hf}
\textit{BERT model page on hf}.
\url{https://huggingface.co/google-bert/bert-base-uncased}.
[Accessed 2025-04-04]

\bibitem{Region_proposal}
\textit{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}
\url{https://arxiv.org/abs/1506.01497}
[Accessed 2025-04-04]

\bibitem{RoBERTa}
\textit{RoBERTa: A Robustly Optimized BERT Pretraining Approach}.
\url{https://arxiv.org/abs/1907.11692}
[Accessed 2025-04-04]

\bibitem{ALBERT}
\textit{ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}.
\url{https://arxiv.org/abs/1909.11942}
[Accessed 2025-04-04]

\bibitem{ALBERT_hf}
\textit{ALBERT model page on hf}.
\url{https://huggingface.co/albert/albert-base-v2}.
[Accessed 2025-04-04]

\bibitem{DISTILBERT}
\textit{DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}
\url{https://arxiv.org/abs/1910.01108}
[Accessed 2025-04-04]

\bibitem{BERT_comp}
\textit{Comparative Analysis of BERT Variants for Text Detection Tasks}
\url{https://www.researchgate.net/publication/385142549_Comparative_Analysis_of_BERT_Variants_for_Text_Detection_Tasks}
[Accessed 2025-04-04]

\bibitem{FBAIHF}
\textit{Facebook AI on HuggingFace}
\url{https://huggingface.co/FacebookAI}
[Accessed 2025-04-04]

\bibitem{BERT_multiling}
\textit{Multilingual BERT on HuggingFace}
\url{https://huggingface.co/google-bert/bert-base-multilingual-cased}
[Accessed 2025-04-04]

\bibitem{ModernBERTPaper}
\textit{Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}
\url{https://arxiv.org/abs/2412.13663}
[Accessed 2025-04-05]

\bibitem{ModernBERThf}
\textit{ModernBERT on HiggingFace}
\url{https://huggingface.co/answerdotai/ModernBERT-base}
[Accessed 2025-04-05]

\bibitem{CurrLearn}
\textit{Curriculum learning}
\url{https://dl.acm.org/doi/abs/10.1145/1553374.1553380}
[Accessed 2025-04-07]

\bibitem{echo_chambers} 
The echo chamber effect on social media
\url{https://pubmed.ncbi.nlm.nih.gov/33622786/}
[Accessed 2025-04-09]

\bibitem{finetuning_env_good} 
Energy and Carbon Considerations of Fine-Tuning BERT
\url{https://arxiv.org/html/2311.10267}
[Accessed 2025-04-09]

\bibitem{ibm_finetuning} 
What is fine-tuning?
\url{https://www.ibm.com/think/topics/fine-tuning}
[Accessed 2025-04-09]

\bibitem{finetune_cool_image} 
14.2. Fine-Tuning
\url{https://d2l.ai/chapter_computer-vision/fine-tuning.html}
[Accessed 2025-04-09]

\bibitem{ibm_quantization} 
What is quantization?
\url{https://www.ibm.com/think/topics/quantization/}
[Accessed 2025-04-09]

\bibitem{quant_hf} 
Quantization
\url{https://huggingface.co/docs/optimum/en/concept_guides/quantization}
[Accessed 2025-04-09]

\bibitem{quant_explained} 
Float32 vs Float16 vs BFloat16?
\url{https://newsletter.theaiedge.io/p/float32-vs-float16-vs-bfloat16}
[Accessed 2025-04-09]

\bibitem{attention}
Attention Is All You Need
\url{https://arxiv.org/pdf/1706.03762} 
[Accessed 2025-04-08]

\bibitem{medium_medium_t}
Exploring Multi-Head Attention: Why More Heads Are Better Than One
\url{https://medium.com/%40hassaanidrees7/exploring-multi-head-attention-why-more-heads-are-better-than-one-006a5823372b}
[Accessed 2025-04-08]

\bibitem{medium_t}
Understanding the Transformer Model: A Report on “Attention Is All You Need” by Ashish Vaswani et al.
\url{https://medium.com/%40shivayapandey359/attention-is-all-you-need-26586e6ab8ca}
[Accessed 2025-04-08]

\bibitem{pp}
\textit{Pareto principle}
\url{https://en.wikipedia.org/wiki/Pareto_principle}
[Accessed 2025-04-10]

\bibitem{iob2}
\textit{Named Entity Recognition}
\url{https://cs229.stanford.edu/proj2005/KrishnanGanapathy-NamedEntityRecognition.pdf}
[Accessed 2025-04-10]

\bibitem{meta-llama}
\textit{Llama 3.2 published by Meta}
\url{https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/}
[Accessed 2025-04-17]

\bibitem{unsloth}
\textit{Unsloth}
\url{https://unsloth.ai/}
[Accessed 2025-04-17]

\bibitem{hugging-face}
\textit{The Hugging Face Platform}
\url{https://huggingface.co/}
[accessed 17.04.2025]

\bibitem{github}
\textit{The Github platform}
\url{https://github.com/}
[accessed 17.04.2025]

\bibitem{modal}
\textit{The Modal platform}
\url{https://modal.com/}
[accessed 17.04.2025]

\bibitem{wandb}
\textit{The Wandb platform}
\url{https://wandb.ai/}
[accessed 17.04.2025]

\bibitem{python}
\textit{The website of Python programming language}
\url{https://www.python.org/}
[accessed 17.04.2025]

\bibitem{service_demo_app_repo}
\textit{Our experiment with mobile app that runs in background}
\url{https://github.com/ZPP-MURMURAS/ZPP_Murmuras/tree/main/research/service_demo_app}

\bibitem{kotlin}
\textit{Webpage of the Kotlin programming Language}
\url{https://kotlinlang.org/}
[accessed 24.04.2025]

\bibitem{hu2021loralowrankadaptationlarge}
\textit{dward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen (2021): LoRA: Low-Rank Adaptation of Large Language Models}
\url{https://arxiv.org/abs/2106.09685}

\bibitem{triton}
\textit{Tillet, Philippe and Kung, H. T. and Cox, David (2019): Triton: an intermediate language and compiler for tiled neural network computations}
\url{https://doi.org/10.1145/3315508.3329973}

\bibitem{lhoest2021datasetscommunitylibrarynatural}
\textit{Lhoest et al (2021): Datasets: A Community Library for Natural Language Processing}
\url{https://arxiv.org/abs/2109.02846}

\bibitem{evaluate}
\textit{evaluate Python library}
\url{https://pypi.org/project/evaluate/}
[accessed 24.05.2025]

\bibitem{wolf-etal-2020-transformers}
\textit{Wolf et al (2020): Transformers: State-of-the-Art Natural Language Processing}
\url{https://aclanthology.org/2020.emnlp-demos.6/}

\bibitem{llama-cpp}
\textit{Llama.cpp repository}
\url{https://github.com/ggml-org/llama.cpp}
[Accessed 25.04.2025]

\bibitem{spacy}
\textit{SpaCy Python library}
\url{https://pypi.org/project/spacy/}
[Accessed 25.04.2025]

\bibitem{onnx}
\textit{ONNX framework}
\url{https://github.com/onnx/onnx}
[Accessed 25.04.2025]

\bibitem{lite-rt}
\textit{LiteRT environment}
\url{https://ai.google.dev/edge/litert}
[Accessed 25.04.2025]

\bibitem{executorch}
\textit{ExecuTorch framework}
\url{https://pytorch.org/executorch-overview}
[Accessed 25.04.2025]

\bibitem{android-studio}
\textit{Android Studio webpage}
\url{https://developer.android.com/studio}
[Accessed 25.04.2025]

\bibitem{jupyter}
\textit{Jupyter Notebook Webpage}
\url{https://jupyter-notebook.readthedocs.io/en/stable/}
[Accessed 25.04.2025]

\bibitem{ipython}
\textit{IPython Kernel Webpage}
\url{https://ipython.org/}
[Accessed 25.04.2025]

\bibitem{spacy-exp}
\textit{Overview od spacy}
\url{https://github.com/ZPP-MURMURAS/ZPP_Murmuras/tree/main/research/spacy_research}
[Accessed 25.04.2025]

\bibitem{scrapegraph-exp}
\textit{Experiments with Scrapegraph AI}
\url{https://github.com/ZPP-MURMURAS/ZPP_Murmuras/tree/main/research/scrapegraphai}
[Accessed 25.04.2025]

\bibitem{onnx-exp}
\textit{Demonstrative mobile deployment of a model in ONNX/ framework.}
\url{https://github.com/ZPP-MURMURAS/ZPP_Murmuras/tree/main/research/onnx_demo_app}
[Accessed 25.04.2025]

\bibitem{lite-rt-exp}
\textit{Demostrative mobile deployment of LiteRT model.}
\url{https://github.com/ZPP-MURMURAS/ZPP_Murmuras/tree/main/research/litert_deployment}
[Accessed 25.04.2025]

\bibitem{executorch-exp}
\textit{Overview of ExecuTorch suitability in our problem.}
\url{https://github.com/ZPP-MURMURAS/ZPP_Murmuras/blob/main/research/executorch/executorch_llama_report.md}
[Accessed 25.04.2025]

\bibitem{python-difflib}
\textit{difflib --- Helpers for computing deltas}
\url{https://docs.python.org/3/library/difflib.html}
[Accessed 28.04.2025]

\bibitem{llm-as-a-judge}
\textit{Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, Jian Guo (2025): A Survey on LLM-as-a-Judge}
\url{https://doi.org/10.48550/arXiv.2411.15594}

\bibitem{llama-cpp-quantization}
\textit{Georgi Gerganov and contributors: Quantization Methods in llama.cpp}
\url{https://github.com/ggml-org/llama.cpp/blob/master/examples/quantize/README.md}


\bibitem{llama-32-intro}
\textit{Llama 3.2 introduced by Meta; Comparison of llama-3.2-1b with Gemma 2 2B IT}
\url{https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/}
[Accessed 04.05.2025]

\bibitem{unsloth-model-support}
\textit{The list of LLMs supported by the Unsloth tool}
\url{https://docs.unsloth.ai/get-started/all-our-models}
[Accessed 04.05.2025]

\bibitem{hf-bnb}
\textit{Documentation of the bitsandbytes library provided by the Hugging Face}
\url{https://huggingface.co/docs/bitsandbytes/main/en/index}
[Accessed 04.05.2025]

\bibitem{mao2023crossentropylossfunctionstheoretical}
\textit{Anqi Mao and Mehryar Mohri and Yutao Zhong (2023): Cross-Entropy Loss Functions: Theoretical Analysis and Applications}
\url{https://arxiv.org/abs/2304.07288}

\bibitem{gemma2}
\textit{
Gemma Team and Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and Léonard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ramé and Johan Ferret and Peter Liu and Pouya Tafti and Abe Friesen and Michelle Casbon and Sabela Ramos and Ravin Kumar and Charline Le Lan and Sammy Jerome and Anton Tsitsulin and Nino Vieillard and Piotr Stanczyk and Sertan Girgin and Nikola Momchev and Matt Hoffman and Shantanu Thakoor and Jean-Bastien Grill and Behnam Neyshabur and Olivier Bachem and Alanna Walton and Aliaksei Severyn and Alicia Parrish and Aliya Ahmad and Allen Hutchison and Alvin Abdagic and Amanda Carl and Amy Shen and Andy Brock and Andy Coenen and Anthony Laforge and Antonia Paterson and Ben Bastian and Bilal Piot and Bo Wu and Brandon Royal and Charlie Chen and Chintu Kumar and Chris Perry and Chris Welty and Christopher A. Choquette-Choo and Danila Sinopalnikov and David Weinberger and Dimple Vijaykumar and Dominika Rogozińska and Dustin Herbison and Elisa Bandy and Emma Wang and Eric Noland and Erica Moreira and Evan Senter and Evgenii Eltyshev and Francesco Visin and Gabriel Rasskin and Gary Wei and Glenn Cameron and Gus Martins and Hadi Hashemi and Hanna Klimczak-Plucińska and Harleen Batra and Harsh Dhand and Ivan Nardini and Jacinda Mein and Jack Zhou and James Svensson and Jeff Stanway and Jetha Chan and Jin Peng Zhou and Joana Carrasqueira and Joana Iljazi and Jocelyn Becker and Joe Fernandez and Joost van Amersfoort and Josh Gordon and Josh Lipschultz and Josh Newlan and Ju-yeong Ji and Kareem Mohamed and Kartikeya Badola and Kat Black and Katie Millican and Keelin McDonell and Kelvin Nguyen and Kiranbir Sodhia and Kish Greene and Lars Lowe Sjoesund and Lauren Usui and Laurent Sifre and Lena Heuermann and Leticia Lago and Lilly McNealus and Livio Baldini Soares and Logan Kilpatrick and Lucas Dixon and Luciano Martins and Machel Reid and Manvinder Singh and Mark Iverson and Martin Görner and Mat Velloso and Mateo Wirth and Matt Davidow and Matt Miller and Matthew Rahtz and Matthew Watson and Meg Risdal and Mehran Kazemi and Michael Moynihan and Ming Zhang and Minsuk Kahng and Minwoo Park and Mofi Rahman and Mohit Khatwani and Natalie Dao and Nenshad Bardoliwalla and Nesh Devanathan and Neta Dumai and Nilay Chauhan and Oscar Wahltinez and Pankil Botarda and Parker Barnes and Paul Barham and Paul Michel and Pengchong Jin and Petko Georgiev and Phil Culliton and Pradeep Kuppala and Ramona Comanescu and Ramona Merhej and Reena Jana and Reza Ardeshir Rokni and Rishabh Agarwal and Ryan Mullins and Samaneh Saadat and Sara Mc Carthy and Sarah Cogan and Sarah Perrin and Sébastien M. R. Arnold and Sebastian Krause and Shengyang Dai and Shruti Garg and Shruti Sheth and Sue Ronstrom and Susan Chan and Timothy Jordan and Ting Yu and Tom Eccles and Tom Hennigan and Tomas Kocisky and Tulsee Doshi and Vihan Jain and Vikas Yadav and Vilobh Meshram and Vishal Dharmadhikari and Warren Barkley and Wei Wei and Wenming Ye and Woohyun Han and Woosuk Kwon and Xiang Xu and Zhe Shen and Zhitao Gong and Zichuan Wei and Victor Cotruta and Phoebe Kirk and Anand Rao and Minh Giang and Ludovic Peran and Tris Warkentin and Eli Collins and Joelle Barral and Zoubin Ghahramani and Raia Hadsell and D. Sculley and Jeanine Banks and Anca Dragan and Slav Petrov and Oriol Vinyals and Jeff Dean and Demis Hassabis and Koray Kavukcuoglu and Clement Farabet and Elena Buchatskaya and Sebastian Borgeaud and Noah Fiedel and Armand Joulin and Kathleen Kenealy and Robert Dadashi and Alek Andreev (2024): Gemma 2: Improving Open Language Models at a Practical Size
}
\url{https://arxiv.org/abs/2408.00118}

\bibitem{yang2024qwen2technicalreport}
\textit{An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan (2024): Qwen2 Technical Report}
\url{https://arxiv.org/abs/2407.10671}

\bibitem{smollm2}
\textit{SmolLM2 model on Hugging Face}
\url{https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct}

\bibitem{gpt-neo}
\textit{Black, Sid and Leo, Gao and Wang, Phil and Leahy, Connor and Biderman, Stella (2021): GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow}
\url{https://doi.org/10.5281/zenodo.5297715}



\end{thebibliography}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: latin-2
%%% End: