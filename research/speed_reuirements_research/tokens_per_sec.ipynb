{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "In this notebook I will estimate minimal number of tokens that our llama model will consume per second.<br/>\n",
    "The following calculations are made under optimistic assumption that we will parse only content from text fields. <br/>\n",
    "Note: you will need llama tokenizer for running this notebook, which may require access request on HF."
   ],
   "id": "b9054c1adc463a7c"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T12:02:15.706248Z",
     "start_time": "2025-02-13T12:02:12.786009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import LlamaTokenizerFast\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers.models.vits.modeling_vits import VitsTextEncoder"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/murmuras/ZPP_murmuras/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:02:16.854720Z",
     "start_time": "2025-02-13T12:02:16.702666Z"
    }
   },
   "cell_type": "code",
   "source": "!python ../../tools/data_load.py coupons_1",
   "id": "f6001170a243caa1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/home/szymon/murmuras/ZPP_murmuras/research/speed_reuirements_research/../../tools/data_load.py\", line 8, in <module>\r\n",
      "    from googleapiclient.discovery import build, Resource\r\n",
      "ModuleNotFoundError: No module named 'googleapiclient'\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:02:19.956835Z",
     "start_time": "2025-02-13T12:02:19.879135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frames = []\n",
    "DS_PATH = Path(\"../..\") / \"datasets\" / \"coupons_1\"\n",
    "PATHS = [\n",
    "    DS_PATH / \"lidl\" / \"Kopia test_data_2024_11_25_lidl_plus_content_generic_2024-12-05T07_39_49.726955559+01_00.csv\",\n",
    "    DS_PATH / \"dm\" / \"Kopia test_data_2024_03_07_dm_content_generic_2024-12-05T10_09_32.502568365+01_00.csv\",\n",
    "    DS_PATH / \"rewe\" / \"Kopia test_data_2024_03_07_rewe_content_generic_2024-12-05T10_30_59.948177782+01_00.csv\",\n",
    "    DS_PATH / \"rossmann\" / \"Kopia test_data_2024_03_07_rossmann_content_generic_2024-12-05T10_24_07.981399375+01_00.csv\"\n",
    "]\n",
    "\n",
    "for path in PATHS:\n",
    "    frames.append(pd.read_csv(path))"
   ],
   "id": "c40ea65da4e3cbf5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:02:24.032818Z",
     "start_time": "2025-02-13T12:02:23.006908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "TEXT_COL_NAME = \"text\"\n",
    "TIMESTAMP_COL_NAME = \"seen_timestamp\"\n",
    "DEPTH_COL_NAME = \"view_depth\"\n",
    "VIEW_ID_COL_NAME = \"view_id\"\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "tokens_cum = 0\n",
    "seconds_cum = 0\n",
    "timestamps_cum = 0\n",
    "\n",
    "for frame in frames:\n",
    "\n",
    "    texts = frame[TEXT_COL_NAME]\n",
    "    texts = texts[texts.notnull()]\n",
    "    total_tokens = texts.apply(count_tokens).sum()\n",
    "    times = frame[TIMESTAMP_COL_NAME]\n",
    "    times = times[times > 0]\n",
    "    time_start = datetime.fromtimestamp(times.min() // 1000)\n",
    "    time_end = datetime.fromtimestamp(times.max() // 1000)\n",
    "    total_seconds = (time_end - time_start).total_seconds()\n",
    "    timestamps_cum += len(frame[TIMESTAMP_COL_NAME].unique())\n",
    "\n",
    "    print(total_tokens, total_seconds)\n",
    "    tokens_cum += total_tokens\n",
    "    seconds_cum += total_seconds\n",
    "\n",
    "print(f\"required min processing speed: {float(tokens_cum / seconds_cum)} tokens per second\")\n",
    "print(f\"tokens per timestamp: {tokens_cum / timestamps_cum}\")"
   ],
   "id": "e370b8e16b233c50",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11739 169.0\n",
      "15026 56.0\n",
      "23708 80.0\n",
      "14678 79.0\n",
      "required min processing speed: 169.6640625 tokens per second\n",
      "tokens per timestamp: 180.975\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Estimation for JSON encoding\n",
    "In the following section I will estimate the number of tokens consumed by LLama if we decide to preserve tree structure in form of JSON. <br/>\n",
    "To encode XML tree I will use following syntax:\n",
    "```json\n",
    "{\n",
    "  \"text\": \"text field content\",\n",
    "  \"children\": {\n",
    "    \"child1_view_id\": ...,\n",
    "    \"child2_view_id\": ...,\n",
    "    ...\n",
    "  }\n",
    "}\n",
    "```\n",
    "Additionally, two tree simplification operations are performed:<br/>\n",
    "* if a node has no children and no text it is removed\n",
    "* if a node has single child and no text it is collapsed - its child is transferred to node's parent under name `node_view_id.child_name` and node does not exist on its own\n",
    "* if a node has no children \"children\" dict keyt is removed"
   ],
   "id": "860589084069102f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T11:26:31.558381Z",
     "start_time": "2025-02-13T11:26:31.550376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "def collapse_tree(tree: dict) -> Tuple[Optional[dict], str]:\n",
    "    \"\"\"removes nodes that have only one child and no text\"\"\"\n",
    "    if len(tree['children']) < 2 and tree['text'] is None:\n",
    "        if len(tree['children']) == 1:\n",
    "            child_name, child = list(tree['children'].items())[0]\n",
    "            collapsed, name = collapse_tree(child)\n",
    "            if collapsed is not None:\n",
    "                name = f\"{child_name}.{name}\"\n",
    "            return collapsed, name\n",
    "        return None, \"\"\n",
    "    new_children = {}\n",
    "    for child_name, child in tree['children'].items():\n",
    "        collapsed, suffix = collapse_tree(child)\n",
    "        if collapsed is not None:\n",
    "            if suffix is not None:\n",
    "                new_children[f\"{child_name}.{suffix}\"] = collapsed\n",
    "            else:\n",
    "                new_children[child_name] = collapsed\n",
    "    tree['children'] = new_children\n",
    "    if len(tree['children']) == 0:\n",
    "        del tree['children']\n",
    "    return tree, \"\"\n",
    "\n",
    "def timestamp_batch_to_json(batch: pd.DataFrame):\n",
    "    \"\"\"takes batch representing single screen content and converts it to JSON representing XML structure\"\"\"\n",
    "    tree_path = []\n",
    "    res = {\"text\": None, \"children\": {}}\n",
    "\n",
    "    def _insert_at_path(key, val):\n",
    "        t = res\n",
    "        for k, d in tree_path:\n",
    "            t = t[\"children\"][k]\n",
    "        t[\"children\"][key] = val\n",
    "\n",
    "    for row in batch.iterrows():\n",
    "        text_field = row[1][TEXT_COL_NAME]\n",
    "        name = row[1][VIEW_ID_COL_NAME]\n",
    "        if isinstance(name, str):\n",
    "            name = name.rsplit('/')[-1]\n",
    "        if not isinstance(text_field, str):\n",
    "            text_field = None\n",
    "        depth = row[1][DEPTH_COL_NAME]\n",
    "        while len(tree_path) > 0 and tree_path[-1][1] >= depth:\n",
    "            tree_path.pop(-1)\n",
    "        _insert_at_path(name, {\"text\": text_field, \"children\": {}})\n",
    "        tree_path.append((name, depth))\n",
    "\n",
    "    return res"
   ],
   "id": "f111e9d45c04ace9",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T11:26:33.133188Z",
     "start_time": "2025-02-13T11:26:32.242273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from json import dumps\n",
    "\n",
    "\n",
    "seconds_cum = 0\n",
    "tokens_cum = 0\n",
    "total_timestamps = 0\n",
    "for frame in frames:\n",
    "    times = frame[]\n",
    "    times = times[times > 0]\n",
    "    time_start = datetime.fromtimestamp(times.min() // 1000)\n",
    "    time_end = datetime.fromtimestamp(times.max() // 1000)\n",
    "    seconds_cum += (time_end - time_start).total_seconds()\n",
    "    for _, subframe in frame.groupby(TIMESTAMP_COL_NAME):\n",
    "        total_timestamps += 1\n",
    "        tree = timestamp_batch_to_json(subframe)\n",
    "        tree = collapse_tree(tree)[0]\n",
    "        tree_str = dumps(tree)\n",
    "        tokens_cum += len(tokenizer.tokenize(tree_str))\n",
    "print(f\"{tokens_cum=}\\n{seconds_cum=}\\n{total_timestamps=}\")\n",
    "print(f\"incoming tokens per second: {tokens_cum / seconds_cum}\")\n",
    "print(f\"tokens per timestamp_seen (screen): {tokens_cum / total_timestamps}\")"
   ],
   "id": "8ab1615ee590399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_cum=104188\n",
      "seconds_cum=384.0\n",
      "total_timestamps=360\n",
      "incoming tokens per second: 271.3229166666667\n",
      "tokens per timestamp_seen (screen): 289.4111111111111\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "| metric                   | plain text format | json-encoded content |\n",
    "|--------------------------|-------------------|----------------------|\n",
    "| incoming tokens/s        | 169.66            | 271.32               |\n",
    "| total tokens             | 104188            | 104188               |\n",
    "| measurement duration [s] | 384               | 384                  |\n",
    "| tokens per timestamp     | 180.98            | 289.41               |\n"
   ],
   "id": "515a4fe4f5ac51ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
