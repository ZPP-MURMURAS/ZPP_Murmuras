{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "In this notebook I will estimate minimal number of tokens that our llama model will consume per second.<br/>\n",
    "The following calculations are made under optimistic assumption that we will parse only content from text fields. <br/>\n",
    "Note: you will need llama tokenizer for running this notebook, which may require access request on HF."
   ],
   "id": "b9054c1adc463a7c"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T08:28:40.190233Z",
     "start_time": "2025-01-09T08:28:39.350629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import LlamaTokenizerFast\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers.models.vits.modeling_vits import VitsTextEncoder"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/murmuras/ZPP_Murmuras/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T17:43:53.614490Z",
     "start_time": "2025-01-06T17:43:52.174538Z"
    }
   },
   "cell_type": "code",
   "source": "!python ../../tools/data_load.py coupons_1",
   "id": "f6001170a243caa1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/speed_reuirements_research/../../tools/data_load.py\", line 101, in <module>\r\n",
      "    children, parent, folder_names = create_fs_tree(service)\r\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/speed_reuirements_research/../../tools/data_load.py\", line 44, in create_fs_tree\r\n",
      "    .execute()\r\n",
      "     ^^^^^^^^^\r\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/.venv/lib/python3.12/site-packages/googleapiclient/_helpers.py\", line 130, in positional_wrapper\r\n",
      "    return wrapped(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/.venv/lib/python3.12/site-packages/googleapiclient/http.py\", line 923, in execute\r\n",
      "    resp, content = _retry_request(\r\n",
      "                    ^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/.venv/lib/python3.12/site-packages/googleapiclient/http.py\", line 191, in _retry_request\r\n",
      "    resp, content = http.request(uri, method, *args, **kwargs)\r\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/.venv/lib/python3.12/site-packages/google_auth_httplib2.py\", line 218, in request\r\n",
      "    response, content = self.http.request(\r\n",
      "                        ^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/.venv/lib/python3.12/site-packages/httplib2/__init__.py\", line 1581, in request\r\n",
      "    conn = self.connections[conn_key] = connection_type(\r\n",
      "                                        ^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/.venv/lib/python3.12/site-packages/httplib2/__init__.py\", line 1100, in __init__\r\n",
      "    context = _build_ssl_context(\r\n",
      "              ^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/.venv/lib/python3.12/site-packages/httplib2/__init__.py\", line 183, in _build_ssl_context\r\n",
      "    context.load_verify_locations(ca_certs)\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:28:53.225439Z",
     "start_time": "2025-01-09T08:28:53.128275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frames = []\n",
    "DS_PATH = Path(\"../..\") / \"datasets\" / \"coupons_1\"\n",
    "PATHS = [\n",
    "    DS_PATH / \"lidl\" / \"Kopia test_data_2024_11_25_lidl_plus_content_generic_2024-12-05T07_39_49.726955559+01_00.csv\",\n",
    "    DS_PATH / \"dm\" / \"Kopia test_data_2024_03_07_dm_content_generic_2024-12-05T10_09_32.502568365+01_00.csv\",\n",
    "    DS_PATH / \"rewe\" / \"Kopia test_data_2024_03_07_rewe_content_generic_2024-12-05T10_30_59.948177782+01_00.csv\",\n",
    "    DS_PATH / \"rossmann\" / \"Kopia test_data_2024_03_07_rossmann_content_generic_2024-12-05T10_24_07.981399375+01_00.csv\"\n",
    "]\n",
    "\n",
    "for path in PATHS:\n",
    "    frames.append(pd.read_csv(path))"
   ],
   "id": "c40ea65da4e3cbf5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T09:09:57.409021Z",
     "start_time": "2025-01-09T09:09:56.653438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "TEXT_COL_NAME = \"text\"\n",
    "TIMESTAMP_COL_NAME = \"seen_timestamp\"\n",
    "DEPTH_COL_NAME = \"view_depth\"\n",
    "VIEW_ID_COL_NAME = \"view_id\"\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "tokens_cum = 0\n",
    "seconds_cum = 0\n",
    "\n",
    "for frame in frames:\n",
    "\n",
    "    texts = frame[TEXT_COL_NAME]\n",
    "    texts = texts[texts.notnull()]\n",
    "    total_tokens = texts.apply(count_tokens).sum()\n",
    "    times = frame[TIMESTAMP_COL_NAME]\n",
    "    times = times[times > 0]\n",
    "    time_start = datetime.fromtimestamp(times.min() // 1000)\n",
    "    time_end = datetime.fromtimestamp(times.max() // 1000)\n",
    "    total_seconds = (time_end - time_start).total_seconds()\n",
    "\n",
    "    print(total_tokens, total_seconds)\n",
    "    tokens_cum += total_tokens\n",
    "    seconds_cum += total_seconds\n",
    "\n",
    "print(f\"required min processing speed: {float(tokens_cum / seconds_cum)} tokens per second\")"
   ],
   "id": "e370b8e16b233c50",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11739 169.0\n",
      "15026 56.0\n",
      "23708 80.0\n",
      "14678 79.0\n",
      "required min processing speed: 169.6640625 tokens per second\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Estimation for JSON encoding\n",
    "In the following section I will estimate the number of tokens consumed by LLama if we decide to preserve tree structure in form of JSON. <br/>\n",
    "To encode XML tree I will use following syntax:\n",
    "```json\n",
    "{\n",
    "  \"text\": \"text field content\",\n",
    "  \"children\": {\n",
    "    \"child1_view_id\": ...,\n",
    "    \"child2_view_id\": ...,\n",
    "    ...\n",
    "  }\n",
    "}\n",
    "```\n",
    "Additionally, two tree simplification operations are performed:<br/>\n",
    "* if a node has no children and no text it is removed\n",
    "* if a node has single child and no text it is collapsed - its child is transferred to node's parent under name `node_view_id.child_name` and node does not exist on its own\n",
    "* if a node has no children \"children\" dict keyt is removed"
   ],
   "id": "860589084069102f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T09:27:01.925223Z",
     "start_time": "2025-01-09T09:27:01.915639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "def collapse_tree(tree: dict) -> Tuple[Optional[dict], str]:\n",
    "    \"\"\"removes nodes that have only one child and no text\"\"\"\n",
    "    if len(tree['children']) < 2 and tree['text'] is None:\n",
    "        if len(tree['children']) == 1:\n",
    "            child_name, child = list(tree['children'].items())[0]\n",
    "            collapsed, name = collapse_tree(child)\n",
    "            if collapsed is not None:\n",
    "                name = f\"{child_name}.{name}\"\n",
    "            return collapsed, name\n",
    "        return None, \"\"\n",
    "    new_children = {}\n",
    "    for child_name, child in tree['children'].items():\n",
    "        collapsed, suffix = collapse_tree(child)\n",
    "        if collapsed is not None:\n",
    "            if suffix is not None:\n",
    "                new_children[f\"{child_name}.{suffix}\"] = collapsed\n",
    "            else:\n",
    "                new_children[child_name] = collapsed\n",
    "    tree['children'] = new_children\n",
    "    if len(tree['children']) == 0:\n",
    "        del tree['children']\n",
    "    return tree, \"\"\n",
    "\n",
    "def timestamp_batch_to_json(batch: pd.DataFrame):\n",
    "    \"\"\"takes batch representing single screen content and converts it to JSON representing XML structure\"\"\"\n",
    "    tree_path = []\n",
    "    res = {\"text\": None, \"children\": {}}\n",
    "\n",
    "    def _insert_at_path(key, val):\n",
    "        t = res\n",
    "        for k, d in tree_path:\n",
    "            t = t[\"children\"][k]\n",
    "        t[\"children\"][key] = val\n",
    "\n",
    "    for row in batch.iterrows():\n",
    "        text_field = row[1][TEXT_COL_NAME]\n",
    "        name = row[1][VIEW_ID_COL_NAME]\n",
    "        if isinstance(name, str):\n",
    "            name = name.rsplit('/')[-1]\n",
    "        if not isinstance(text_field, str):\n",
    "            text_field = None\n",
    "        depth = row[1][DEPTH_COL_NAME]\n",
    "        while len(tree_path) > 0 and tree_path[-1][1] >= depth:\n",
    "            tree_path.pop(-1)\n",
    "        _insert_at_path(name, {\"text\": text_field, \"children\": {}})\n",
    "        tree_path.append((name, depth))\n",
    "\n",
    "    return res"
   ],
   "id": "f111e9d45c04ace9",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T09:30:50.052430Z",
     "start_time": "2025-01-09T09:30:49.173148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from json import dumps\n",
    "\n",
    "\n",
    "seconds_cum = 0\n",
    "tokens_cum = 0\n",
    "total_timestamps = 0\n",
    "for frame in frames:\n",
    "    times = frame[TIMESTAMP_COL_NAME]\n",
    "    times = times[times > 0]\n",
    "    time_start = datetime.fromtimestamp(times.min() // 1000)\n",
    "    time_end = datetime.fromtimestamp(times.max() // 1000)\n",
    "    seconds_cum += (time_end - time_start).total_seconds()\n",
    "    for _, subframe in frame.groupby(TIMESTAMP_COL_NAME):\n",
    "        total_timestamps += 1\n",
    "        tree = timestamp_batch_to_json(subframe)\n",
    "        tree = collapse_tree(tree)[0]\n",
    "        tree_str = dumps(tree)\n",
    "        tokens_cum += len(tokenizer.tokenize(tree_str))\n",
    "print(f\"{tokens_cum=}\\n{seconds_cum=}\\n{total_timestamps=}\")\n",
    "print(f\"incoming tokens per second: {tokens_cum / seconds_cum}\")\n",
    "print(f\"tokens per timestamp_seen (screen): {tokens_cum / total_timestamps}\")"
   ],
   "id": "8ab1615ee590399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_cum=104188\n",
      "seconds_cum=384.0\n",
      "total_timestamps=360\n",
      "incoming tokens per second: 271.3229166666667\n",
      "tokens per timestamp_seen (screen): 289.4111111111111\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusions\n",
    "As we see, the required number of tokens processed each second exceeds values achieved by us on llama models. We should either reject real-time solution or explore cheaper models."
   ],
   "id": "515a4fe4f5ac51ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d9a45a3140b32df9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
