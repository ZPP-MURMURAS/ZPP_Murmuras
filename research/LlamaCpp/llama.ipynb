{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLama.CPP\n",
    "LLama.CPP is a tool that enables us to run LLM inference on a local machine. This is true for both desktop and mobile (which is our main focus). \\\n",
    "In this notebook I present results of my research on this tool; how we can use, where we can use it, and what are its limitations."
   ],
   "id": "3791af64fce2f8c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Build Llama.cpp on Desktop\n",
    "For obvious reasons (such as physical keyboard, mouse etc), experimenting with Llama.cpp is much easier on desktop. \\\n",
    "So while we target mobile platforms, for learning and research purposes I worked with the desktop version, \\\n",
    "and so here are the steps to run Llama.cpp on desktop (here I present steps for Linux, but Windows version is also available in the repository docs):\n",
    "1. Clone the Llama.cpp repository, which can be found here: https://github.com/ggerganov/llama.cpp\n",
    "2. Install CMake (if you don't have it already) by running `sudo apt install cmake`\n",
    "3. Install Clang compiler by running `sudo apt install clang`\n",
    "4. (Optional) On mobile i install libllvm instead (it should contain clang and cmake). I didn't try this, but you can.\n",
    "5. Enter the llama.cpp directory and create a build directory by running `cmake -B build`\n",
    "6. Build the project by running `cmake --build build --config Release`\n"
   ],
   "id": "f00fe8ad9f06f0aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Run LLM on Desktop\n",
    "There are actually three ways to run LLMs (not true for BERTs, I will describe this later). For each of them,\n",
    "you will need a model saved in a .gguf format (which is a serialized model format used by Llama.cpp). \\\n",
    "TheBloke user on Hugging Face has shared a lot of models converted to this format; you can find him here: https://huggingface.co/TheBloke.\n",
    "For the purpose of examples here, I will use TheBloke/Llama-2-7B-GGUF (https://huggingface.co/TheBloke/Llama-2-7B-GGUF)."
   ],
   "id": "b9770d2811fbdd37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Run LLM directly from the command line\n",
    "First of all, you need to download the model. When you hed to the model page and its files, it will contain \\\n",
    "a lot of different versions with different quantizations (described on the model's page). \\\n",
    "I will use the Q5 version. Now, you can download and store it anywhere, but for performance reasons \\\n",
    "(at least on android) it is suggested to store it under ~/ path.\\\n",
    "Now, you can run the model by running the following command:\n",
    "```bash\n",
    "./build/bin/llama-simple -m ~/llama-2-7b.Q5_K_S.gguf -c 4096 -p \"What is the capital of France?\"\n",
    "```\n",
    "Now, it is a big model, and running it on CPU may take a while. But, if everything goes smoothly, you should \\\n",
    "get something similar to this: \\\n",
    "![llama-7B](./ResearchImages/llama7bdesktop.png)"
   ],
   "id": "db10c0c538518406"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Run LLM as a server\n",
    "Llama.cpp has a basic server for running with LLMs (and it actually has a nice API, so we can write scripts to communicate with it). \\\n",
    "To run the server, you need to run the following command:\n",
    "```bash\n",
    "./build/bin/llama-server -m ~/llama-2-7b.Q5_K_S.gguf -c 4096\n",
    "```\n",
    "After that you can go under http://127.0.0.1:8080/ and you should see the server's page: \\\n",
    "![llama-server](./ResearchImages/LLamacppserver.png)\n",
    "In this particular case, results are very bad. But it is dependent on the model, I played with smaller ones \\\n",
    "and while responses were not perfect, they were making sense and were actually related to the topic.\n"
   ],
   "id": "2c3887618d5d6834"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Build Llama.cpp on Mobile\n",
    "\n",
    "This is actually a really fun part. To run Llama.cpp on mobile, you need a terminal, and Termux is all you need.  \n",
    "It is a terminal emulator for Android, and you can find it here: [https://termux.dev/en/](https://termux.dev/en/).  \n",
    "You can download it directly from the Play Store, but for the newest version, I suggest getting it from F-Droid (I did it both ways).  \n",
    "To get it from F-Droid, follow the steps described on the project's page.  \n",
    "\n",
    "After you install Termux, you have to basically repeat the steps from the desktop version.  \n",
    "First of all, you need to install:\n",
    "- **Git** by running `pkg install git`\n",
    "- **libllvm** by running `pkg install libllvm` (this should contain both clang and cmake)\n",
    "\n",
    "Now, steps are almost identical to the desktop version:\n",
    "1. Clone the Llama.cpp repository by running `git clone https://github.com/ggerganov/llama.cpp'\n",
    "2. Enter the llama.cpp directory by running `cd llama.cpp`\n",
    "3. Create a build directory by running `cmake -B build`\n",
    "4. Build the project by running `cmake --build build --config Release`\n",
    "\n",
    "    "
   ],
   "id": "d846168c416d33ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Run LLM on Mobile\n",
    "There are two ways to run LLMs on mobile, and they are actually the same as on desktop:"
   ],
   "id": "1bf8f3303baaffbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Run LLM directly from the command line\n",
    "This time, we will use a smaller model (but you can also play with the 7b llama, it works). \\\n",
    "To get the model, we will uses curl for the ease of use: \\\n",
    "```bash\n",
    "curl -L -o tinyllama-1.1b-chat-v0.3.Q5_K_S.gguf https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q5_K_S.gguf\n",
    "```\n",
    "This should download the file and save it under your current location so before doing this, I suggest you enter ~/ dir ('cd ..' from llama.cpp folder). \\\n",
    "\n",
    "Now, to run the model, you can run the following command:\n",
    "```bash\n",
    "./build/bin/llama-simple -m ~/tinyllama-1.1b-chat-v0.3.Q5_K_S.gguf -c 4096 -p \"What is the capital of France?\"\n",
    "```\n",
    "\n",
    "And you should get a response similar to this: \\\n",
    "![tinyllama-1.1B](./ResearchImages/tiny_termux.png)\n",
    "\n",
    "One thing you can observe, is that while the response is far from perfect, it is related to the question.\n",
    "And it was really fast (almost instant)."
   ],
   "id": "493a76cda22e089e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Run LLM as a server\n",
    "To run the server, you need to run the following command:\n",
    "```bash\n",
    "./build/bin/llama-server -m ~/tinyllama-1.1b-chat-v0.3.Q5_K_S.gguf -c 4096\n",
    "```\n",
    "After that you can go under http://127.0.0.1:8080/ and you should see the server's page: \\\n",
    "![llama-server](./ResearchImages/termux_server.png) \\\n",
    "As you can see, the app is not fitted for mobile, but it works. Now, main problem with this (for both mobile and desktop) \\\n",
    "is that it will generate response of the length specified by context size. So, if you set it to 4096, it will generate 4096 tokens. \\\n",
    "As a result, it will either cut its answers, or generate random garbage to fill the space. \\"
   ],
   "id": "9439bb5ad32d2937"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4 BERT and custom models with Llama.cpp\n",
    "This is where things are starting to get worse and worse. \\\n",
    "First of all, BERTs are TECHNICALLY supported by Llama.cpp, but I will describe how to run one in a second. \\\n",
    "Do you remember the .gguf format? It is not a commot format, it was created by the author of Llama.cpp (to replace .ggml format but that's another story).  \\\n",
    "Every model you want to run with Llama.cpp has to be converted to this format. TheBloke guy has a few of them, but if we want to \\\n",
    "fine-tune our onw model and run it , we need to somehow convert it to this format. \\\n",
    "Now, there is a function that you can use for this: 'convert_hf_to_gguf.py', but it wasn't working for me when used with google/bert-base-uncased. \\\n",
    "I was getting an error that this type of bert (specifically 'BertForMaskedLM') is not supported. \\\n",
    "There is another function called 'convert_hf_to_gguf_update.py'. After some digging it turned out that it has a hardcoded list of models that \\\n",
    "llama.cpp supports. If you want to use a new one, you have to add it to this list, run this function, and as a result you will get \\\n",
    "a python function called 'get_vocab_base_pre()' that you should paste in the place of the old one in the 'convert_hf_to_gguf.py'. \\\n",
    "I did that, and except the fact that I had to remove all other models because half of them are restricted now so they cannot be downloaded and script crashes, \\\n",
    "it seemed that it worked. But, when I tried to convert the model, I was still getting an error that the model is not supported. \\\n",
    "After even more digging I found out why: For every type of model, there is a DEDICATED implementation of the conversion function. \\\n",
    "There is one for BertModel, but BertModel != BertForMaskedLM (I tried to force the code to treat it as BertModel, but it didn't work). \\\n",
    "So, it seems that if we want to use it with our own models, we have to write our own conversion function. \\\n",
    "\n",
    " "
   ],
   "id": "e1953d1952c6d092"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Still, I was curious how does the pipeline work, so I converted and run on of the two BERT models supported by Llama.cpp: \\\n",
    "https://huggingface.co/BAAI/bge-small-en-v1.5\n",
    "https://github.com/FlagOpen/FlagEmbedding\n",
    "The main problem with this model is that it only returns numerical embeddings, so we would probably manually have to convert them to words. \\\n",
    "Anyway, here are the steps to convert and run this model:\n",
    "1. Download the snapshot of the model: to do this, you can use the python script below:\n",
    "```python\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\"BAAI/bge-small-en-v1.5\")\n",
    "```\n",
    "2. Get the snapshot's path: this snapshot will be saved in the hugging face subfolder of the ~/.cache folder. For me it was\n",
    "```bash\n",
    "/home/gustaw/.cache/huggingface/hub/models--BAAI--bge-small-en-v1.5/snapshots/5c38ec7c405ec4b44b94cc5a9bb96e735b38267a'\n",
    "```\n",
    "3. Setup python env: in llama.cpp, run the following commands:\n",
    "```bash\n",
    "python -m venv .\n",
    "source .venv/bin/activate\n",
    "```\n",
    "This will create and start venv for you. Now, enter 'requirements' dir, and run:\n",
    "```bash\n",
    "pip install -r requirements-all.txt\n",
    "```\n",
    "Now get back to the root of llama.cpp.\n",
    "4. (Optional): Run 'convert_hf_to_gguf_update.py' to see how it behaves. To run it, call:\n",
    "```bash\n",
    "python convert_hf_to_gguf_update.py <your_hugging_face_token>\n",
    "```\n",
    "For the Bert-only version, I got the following output:\n",
    "![update](./ResearchImages/convert_update.png)\n",
    "5. Run the conversion: now, run the following command:\n",
    "```bash\n",
    "python convert_hf_to_gguf.py -m <path_to_snapshot>\n",
    "```\n",
    "For me, it was:\n",
    "```bash\n",
    "python convert_hf_to_gguf.py -m /home/gustaw/.cache/huggingface/hub/models--BAAI--bge-small-en-v1.5/snapshots/5c38ec7c405ec4b44b94cc5a9bb96e735b38267a\n",
    "```\n",
    "Output:\n",
    "![convert](./ResearchImages/convert.png)\n",
    "As you can see, it said where it stored the model.\n",
    "6. Quanitze the model: now, run the following command:\n",
    "```bash\n",
    "./build/bin/llama-quantize -m <path_to_converted_model.gguf> <quantization_type>\n",
    "```\n",
    "For me, it was:\n",
    "```bash\n",
    "./build/bin/llama-quantize /home/gustaw/.cache/huggingface/hub/models--BAAI--bge-small-en-v1.5/snapshots/5c38ec7c405ec4b44b94cc5a9bb96e735b38267a/5c38ec7c405ec4b44b94cc5a9bb96e735b38267a-33M-5c38ec7c405ec4b44b94cc5a9bb96e735b38267a-F16.gguf Q8_0\n",
    "```\n",
    "Output: \\\n",
    "![quantize](./ResearchImages/quant.png) \\\n",
    "As you can probably deduce from my command, I used Q8_0 quantization. \\\n",
    "7. Run the model: now, run the following command:\n",
    "```bash\n",
    "sudo build/bin/llama-embedding --batch-size 4096 --ctx-size 2048 -m <path-to_quantized_model> -p <prompt>\n",
    "```\n",
    "Again, for me it was:\n",
    "```bash\n",
    "sudo build/bin/llama-embedding --batch-size 4096 --ctx-size 2048 -m /home/gustaw/.cache/huggingface/hub/models--BAAI--bge-small-en-v1.5/snapshots/5c38ec7c405ec4b44b94cc5a9bb96e735b38267a/ggml-model-Q8_0.gguf -p \"What is the capital of France?\"\n",
    "```\n",
    "Path of the quantized model can be found in the output of the quantization script. \\\n",
    "Two things: first, without sudo this script won't have provoleges to create threads needed for calculations.\\\n",
    "Second, batch size has to be bigger than context size, otherwise you will get a cpp assertion error. \\\n",
    "Output: \\\n",
    "![bert](./ResearchImages/converted_run.png)"
   ],
   "id": "12da9a858ce0737c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Conclusion\n",
    "Llama.cpp is a great tool for local inference of LLMs (and other supported types of models). But only of the supported ones.\n",
    "Idea of usage of this tool comes from this paper: https://arxiv.org/html/2410.03613v1. What quickly became obvious for me is that \\\n",
    "in ths research the model used for testing was llama-2.7B, which is basically a model from the tutorial. \\\n",
    "Adapting it for our own needs may not necessary require a lot of code (I'm talking about adding a code for conversion of the model), \\\n",
    "but it may be a lot of work to actually learn what needs to be done. \\\n",
    "To summarize, I think that it is technically possible to use Llama.cpp for our needs, but it will be challenging, \\\n",
    "and so I would leave it as a last resort resulution. \\"
   ],
   "id": "920b03bef428e789"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Appendix: Resources used/related to this research\n",
    "https://github.com/ggerganov/llama.cpp \\\n",
    "https://www.reddit.com/r/LocalLLaMA/comments/14rncnb/local_llama_on_android_phone/ \\\n",
    "https://www.1a-insec.net/w/21-adding-semantic-search/ \\\n",
    "https://github.com/skeskinen/bert.cpp \\\n",
    "https://www.youtube.com/watch?v=jOEu0PE4ozM"
   ],
   "id": "c50255136f24d2cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Take Termux from f-droid\n",
    "\n",
    "# pkg install cmake\n",
    "# pkg install libllvm\n",
    "# git clone llamacpp\n",
    "# cd llamacpp\n",
    "# cmake -B build\n",
    "# cmake --build build --config Release\n",
    "\n",
    "# cd ..\n",
    "# curl -L -o llama-2-7b-chat.Q2_K.gguf https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf\n",
    "# cd llamacpp\n",
    "# ./build/bin/llama-server -m ~/tinyllama-1.1b-chat-v0.3.Q8_0.gguf -c 4096\n",
    "\n",
    "\n",
    "# Works for both mobile and desktop\n",
    "# https://github.com/ggerganov/llama.cpp/tree/master/examples/server has a server with API,\n",
    "# so technically we can make app, deploy server somewhere on mobile and app would call the local server\n",
    "\n",
    "# has some android demo to checkout, do it"
   ],
   "id": "b2c6f87069ffeb86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://www.reddit.com/r/LocalLLaMA/comments/14rncnb/local_llama_on_android_phone/\n",
   "id": "ff77c791cda3bfe6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "403e51ca4e4d7767"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
