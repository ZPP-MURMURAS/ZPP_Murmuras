{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLama.CPP\n",
    "LLama.CPP is a tool that enables us to run LLM inference on a local machine. This is true for both desktop and mobile (which is our main focus). \\\n",
    "In this notebook I present results of my research on this tool; how we can use, where we can use it, and what are its limitations."
   ],
   "id": "3791af64fce2f8c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Build Llama.cpp on Desktop\n",
    "For obvious reasons (such as physical keyboard, mouse etc), experimenting with Llama.cpp is much easier on desktop. \\\n",
    "So while we target mobile platforms, for learning and research purposes I worked with the desktop version, \\\n",
    "and so here are the steps to run Llama.cpp on desktop (here I present steps for Linux, but Windows version is also available in the repository docs):\n",
    "1. Clone the Llama.cpp repository, which can be found here: https://github.com/ggerganov/llama.cpp\n",
    "2. Install CMake (if you don't have it already) by running `sudo apt install cmake`\n",
    "3. Install Clang compiler by running `sudo apt install clang`\n",
    "4. (Optional) On mobile i install libllvm instead (it should contain clang and cmake). I didn't try this, but you can.\n",
    "5. Enter the llama.cpp directory and create a build directory by running `cmake -B build`\n",
    "6. Build the project by running `cmake --build build --config Release`\n"
   ],
   "id": "f00fe8ad9f06f0aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Run LLM on Desktop\n",
    "There are actually three ways to run LLMs (not true for BERTs, I will describe this later). For each of them,\n",
    "you will need a model saved in a .gguf format (which is a serialized model format used by Llama.cpp). \\\n",
    "TheBloke user on Hugging Face has shared a lot of models converted to this format; you can find him here: https://huggingface.co/TheBloke.\n",
    "For the purpose of examples here, I will use TheBloke/Llama-2-7B-GGUF (https://huggingface.co/TheBloke/Llama-2-7B-GGUF)."
   ],
   "id": "b9770d2811fbdd37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Run LLM directly from the command line\n",
    "First of all, you need to download the model. When you hed to the model page and its files, it will contain \\\n",
    "a lot of different versions with different quantizations (described on the model's page). \\\n",
    "I will use the Q5 version. Now, you can download and store it anywhere, but for performance reasons \\\n",
    "(at least on android) it is suggested to store it under ~/ path.\\\n",
    "Now, you can run the model by running the following command:\n",
    "```bash\n",
    "./build/bin/llama-simple -m ~/llama-2-7b.Q5_K_S.gguf -c 4096 -p \"What is the capital of France?\"\n",
    "```\n",
    "Now, it is a big model, and running it on CPU may take a while. But, if everything goes smoothly, you should \\\n",
    "get something similar to this: \\\n",
    "![llama-7B](./ResearchImages/llama7bdesktop.png)"
   ],
   "id": "db10c0c538518406"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Run LLM as a server\n",
    "Llama.cpp has a basic server for running with LLMs (and it actually has a nice API, so we can write scripts to communicate with it). \\\n",
    "To run the server, you need to run the following command:\n",
    "```bash\n",
    "./build/bin/llama-server -m ~/llama-2-7b.Q5_K_S.gguf -c 4096\n",
    "```\n",
    "After that you can go under http://127.0.0.1:8080/ and you should see the server's page: \\\n",
    "![llama-server](./ResearchImages/LLamacppserver.png)\n",
    "In this particular case, results are very bad. But it is dependent on the model, I played with smaller ones \\\n",
    "and while responses were not perfect, they were making sense and were actually related to the topic.\n"
   ],
   "id": "2c3887618d5d6834"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Build Llama.cpp on Mobile\n",
    "\n",
    "This is actually a really fun part. To run Llama.cpp on mobile, you need a terminal, and Termux is all you need.  \n",
    "It is a terminal emulator for Android, and you can find it here: [https://termux.dev/en/](https://termux.dev/en/).  \n",
    "You can download it directly from the Play Store, but for the newest version, I suggest getting it from F-Droid (I did it both ways).  \n",
    "To get it from F-Droid, follow the steps described on the project's page.  \n",
    "\n",
    "After you install Termux, you have to basically repeat the steps from the desktop version.  \n",
    "First of all, you need to install:\n",
    "- **Git** by running `pkg install git`\n",
    "- **libllvm** by running `pkg install libllvm` (this should contain both clang and cmake)\n",
    "\n",
    "Now, steps are almost identical to the desktop version:\n",
    "1. Clone the Llama.cpp repository by running `git clone https://github.com/ggerganov/llama.cpp'\n",
    "2. Enter the llama.cpp directory by running `cd llama.cpp`\n",
    "3. Create a build directory by running `cmake -B build`\n",
    "4. Build the project by running `cmake --build build --config Release`\n",
    "\n",
    "    "
   ],
   "id": "d846168c416d33ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Run LLM on Mobile\n",
    "There are two ways to run LLMs on mobile, and they are actually the same as on desktop:"
   ],
   "id": "1bf8f3303baaffbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Run LLM directly from the command line\n",
    "This time, we will use a smaller model (but you can also play with the 7b llama, it works). \\\n",
    "To get the model, we will uses curl for the ease of use: \\\n",
    "```bash\n",
    "curl -L -o tinyllama-1.1b-chat-v0.3.Q5_K_S.gguf https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q5_K_S.gguf\n",
    "```\n",
    "This should download the file and save it under your current location so before doing this, I suggest you enter ~/ dir ('cd ..' from llama.cpp folder). \\\n",
    "\n",
    "Now, to run the model, you can run the following command:\n",
    "```bash\n",
    "./build/bin/llama-simple -m ~/tinyllama-1.1b-chat-v0.3.Q5_K_S.gguf -c 4096 -p \"What is the capital of France?\"\n",
    "```\n",
    "\n",
    "And you should get a response similar to this: \\\n",
    "![tinyllama-1.1B](./ResearchImages/tiny_termux.png)\n",
    "\n",
    "One thing you can observe, is that while the response is far from perfect, it is related to the question.\n",
    "And it was really fast (almost instant)."
   ],
   "id": "493a76cda22e089e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Run LLM as a server\n",
    "To run the server, you need to run the following command:\n",
    "```bash\n",
    "./build/bin/llama-server -m ~/tinyllama-1.1b-chat-v0.3.Q5_K_S.gguf -c 4096\n",
    "```\n",
    "After that you can go under http://127.0.0.1:8080/ and you should see the server's page: \\\n",
    "![llama-server](./ResearchImages/termux_server.png) \\\n",
    "As you can see, the app is not fitted for mobile, but it works. Now, main problem with this (for both mobile and desktop) \\\n",
    "is that it will generate response of the length specified by context size. So, if you set it to 4096, it will generate 4096 tokens. \\\n",
    "As a result, it will either cut its answers, or generate random garbage to fill the space. \\"
   ],
   "id": "9439bb5ad32d2937"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# 4 BERT and custom models with Llama.cpp",
   "id": "73406096034dc367"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Take Termux from f-droid\n",
    "\n",
    "# pkg install cmake\n",
    "# pkg install libllvm\n",
    "# git clone llamacpp\n",
    "# cd llamacpp\n",
    "# cmake -B build\n",
    "# cmake --build build --config Release\n",
    "\n",
    "# cd ..\n",
    "# curl -L -o llama-2-7b-chat.Q2_K.gguf https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf\n",
    "# cd llamacpp\n",
    "# ./build/bin/llama-server -m ~/tinyllama-1.1b-chat-v0.3.Q8_0.gguf -c 4096\n",
    "\n",
    "\n",
    "# Works for both mobile and desktop\n",
    "# https://github.com/ggerganov/llama.cpp/tree/master/examples/server has a server with API,\n",
    "# so technically we can make app, deploy server somewhere on mobile and app would call the local server\n",
    "\n",
    "# has some android demo to checkout, do it"
   ],
   "id": "b2c6f87069ffeb86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://www.reddit.com/r/LocalLLaMA/comments/14rncnb/local_llama_on_android_phone/\n",
   "id": "ff77c791cda3bfe6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "403e51ca4e4d7767"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
