{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLama.CPP\n",
    "LLama.CPP is a tool that enables us to run LLM inference on a local machine. This is true for both desktop and mobile (which is our main focus). \\\n",
    "In this notebook I present results of my research on this tool; how we can use, where we can use it, and what are its limitations."
   ],
   "id": "3791af64fce2f8c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Build Llama.cpp on Desktop\n",
    "For obvious reasons (such as physical keyboard, mouse etc), experimenting with Llama.cpp is much easier on desktop. \\\n",
    "So while we target mobile platforms, for learning and research purposes I worked with the desktop version, \\\n",
    "and so here are the steps to run Llama.cpp on desktop (here I present steps for Linux, but Windows version is also available in the repository docs):\n",
    "1. Clone the Llama.cpp repository, which can be found here: https://github.com/ggerganov/llama.cpp\n",
    "2. Install CMake (if you don't have it already) by running `sudo apt install cmake`\n",
    "3. Install Clang compiler by running `sudo apt install clang`\n",
    "4. (Optional) On mobile i install libllvm instead (it should contain clang and cmake). I didn't try this, but you can.\n",
    "5. Enter the llama.cpp directory and create a build directory by running `cmake -B build`\n",
    "6. Build the project by running `cmake --build build --config Release`\n"
   ],
   "id": "f00fe8ad9f06f0aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Run LLM on Desktop\n",
    "There are actually three ways to run LLMs (not true for BERTs, I will describe this later). For each of them,\n",
    "you will need a model saved in a .gguf format (which is a serialized model format used by Llama.cpp). \\\n",
    "TheBloke user on Hugging Face has shared a lot of models converted to this format; you can find him here: https://huggingface.co/TheBloke.\n",
    "For the purpose of examples here, I will use TheBloke/Llama-2-7B-GGUF (https://huggingface.co/TheBloke/Llama-2-7B-GGUF)."
   ],
   "id": "b9770d2811fbdd37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Run LLM directly from the command line\n",
    "First of all, you need to download the model. When you hed to the model page and its files, it will contain \\\n",
    "a lot of different versions with different quantizations (described on the model's page). \\\n",
    "I will use the Q5 version. Now, you can download and store it anywhere, but for performance reasons \\\n",
    "(at least on android) it is suggested to store it under ~/ path.\\\n",
    "Now, you can run the model by running the following command:\n",
    "```bash\n",
    "./build/bin/llama-simple -m ~/llama-2-7b.Q5_K_S.gguf -c 4096 -p \"What is the capital of France?\"\n",
    "```\n",
    "Now, it is a big model, and running it on CPU may take a while. But, if everything goes smoothly, you should \\\n",
    "get something similar to this: \\\n",
    "![llama-7B](./ResearchImages/llama7bdesktop.png)"
   ],
   "id": "db10c0c538518406"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Run LLM as a server\n",
    "Llama.cpp has a basic server for running with LLMs (and it actually has a nice API, so we can write scripts to communicate with it). \\\n",
    "To run the server, you need to run the following command:\n",
    "```bash\n",
    "./build/bin/llama-server -m ~/llama-2-7b.Q5_K_S.gguf -c 4096\n",
    "```\n",
    "After that you can go under http://127.0.0.1:8080/ and you should see the server's page: \\\n",
    "![llama-server](./ResearchImages/LLamacppserver.png)\n",
    "In this particular case, results are very bad. But it is dependent on the model, I played with smaller ones \\\n",
    "and while responses were not perfect, they were making sense and were actually related to the topic.\n"
   ],
   "id": "2c3887618d5d6834"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Take Termux from f-droid\n",
    "\n",
    "# pkg install cmake\n",
    "# pkg install libllvm\n",
    "# git clone llamacpp\n",
    "# cd llamacpp\n",
    "# cmake -B build\n",
    "# cmake --build build --config Release\n",
    "\n",
    "# cd ..\n",
    "# curl -L -o llama-2-7b-chat.Q2_K.gguf https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf\n",
    "# cd llamacpp\n",
    "# ./build/bin/llama-server -m ~/tinyllama-1.1b-chat-v0.3.Q8_0.gguf -c 4096\n",
    "\n",
    "\n",
    "# Works for both mobile and desktop\n",
    "# https://github.com/ggerganov/llama.cpp/tree/master/examples/server has a server with API,\n",
    "# so technically we can make app, deploy server somewhere on mobile and app would call the local server\n",
    "\n",
    "# has some android demo to checkout, do it"
   ],
   "id": "b2c6f87069ffeb86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://www.reddit.com/r/LocalLLaMA/comments/14rncnb/local_llama_on_android_phone/\n",
   "id": "ff77c791cda3bfe6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "403e51ca4e4d7767"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
