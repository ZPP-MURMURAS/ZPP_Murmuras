{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 0. Introduction\n",
    "In this notebook, I will go through inference examples with models both listed as supported by Llama.cpp, and one that is technically not listed, but I managed to run it. Additionally, I will present a way to enforce a specific json format of the model output.\n",
    "\n",
    "Throughout this notebook, I will assume that the reader has a basic understanding of the contents of the 'introduction' folder that contains example workflows of Llama.cpp."
   ],
   "id": "ecf10ba59e0ba761"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Enforcing a specific json format\n",
    "Llama.cpp \"llama-cli\" binary supports enforcing different json formats for the output using [GNBF](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md) grammars or [JSON schemas](https://json-schema.org/). When using JSON schemas, even the one provided as examples, I ran into issues. Luckily, Llama.cpp provides a [script](https://github.com/ggerganov/llama.cpp/blob/master/examples/json_schema_to_grammar.py) which can be used to convert JSON schemas to GNBF grammars. This is the approach that I used.\n",
    "\n",
    "For the example purposes, I will use the [Llama 3.2 1B  model](https://huggingface.co/meta-llama/Llama-3.2-1B) with K_5 quantization (more on models and quantizations later in the notebook). Also, I will use \"prompt.txt\" file with a very small example prompt and \"schema.json\" file with the schema that I want to enforce on the model output. Both of these files can be found under the \"miscellaneous\" directory.\n",
    "\n",
    "To run the inference with the enforced schema, I ran the following command\"\n",
    "```bash\n",
    "./build/bin/llama-cli -m ~/llama3215.gguf --file prompt_large.txt --grammar \"$( python examples/json_schema_to_grammar.py schema.json )\"\n",
    "```\n",
    "\n",
    "The result of this command is the following: \\\n",
    "![Example result](images/prompt_example.png)\n",
    "\n",
    "Few things to note here:\n",
    "1. The output matches the provided schema, but its formatting may differ between different runs.\n",
    "2. It is very accurate; but, for bigger prompts (example in the \"prompt_large.txt\" file), the output omits some of the data.\n",
    "3. This schema does not enforce the model to end. Older models/ models with stronger quantizations may not generate the \"[end of text]\" token and will generate data infinitely (unless we prevent them from doing so).\n",
    "4. Prompt has a specific format; Llama models are, by default, trained for sentence completion, so simply asking the model what are the products in the file may cause it to talk about something related, but not produce desired results. Because of this, prompts need to be formatted in a way that the model will \"complete\" them, not answer them.\n"
   ],
   "id": "b338c50a8658a58e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Running different LLama models\n",
    "Llama.cpp has a list of models that are explicitly [supported](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf_update.py). This proved to be an issue with BERT models, because every model listed here has its own architecture and for every architecture, there is code written in order to run the model. Supported BERT models have different underlying type than the ones we use, but my hypothesis was that if they had the same underlying type, it would be possible to run them.\n",
    "\n",
    "And I am happy to announce that I proved my hypothesis. I run both explicitly supported models: [Llama 2-7B](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama 3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B), BUT, I also managed to run the [Llama 3.2 1B](https://huggingface.co/meta-llama/Llama-3.2-1B) model with different quantizations. To archeive that, I followed simliar steps I described for the BERT model in the introduction folder, with one notable change: I added the following line to the \"convert_hf_to_gguf_update.py\" file:\n",
    "```python\n",
    "{\"name\": \"llama-bpe\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Llama-3.2-1B\", },\n",
    "```\n",
    "\n",
    "This way, by running this script, I made Llama.cpp \"aware\" of this model, and because it has the same underlying type as the older models, there were no issues with running it.\n",
    "\n",
    "Also, as part of my experiments, I managed to run the [Hermes-Llama-3.2-3B](https://huggingface.co/NousResearch/Hermes-3-Llama-3.2-3B-GGUF/tree/main) model (another \"unsupported\" model). I was looking for some alternatives to the base Meta products, and I found this model. It has 3B parameters. You can find its comparison with other models later in this notebook."
   ],
   "id": "1e53b3855b100a84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Issues and limitations\n",
    "As mentioned before, older models and/or models with stronger quantizations may not generate the \"[end of text]\" token and will generate data infinitely/will repeat it a few times before finishing. To reproduce this I tried both Llama-2 and Llama-3.2 with stringest quantizations (Llama-2 is the oldest one, Llama-3.2 is the smallest one), but it was very difficult to reproduce this issue. Instead, I observed that those variants tends to produce a lot of non-existing data, e.g. Llama-2: \\\n",
    "![Llama-2](images/llama2_ghost.png)\n",
    "\n",
    "This is problematic; If models would only produce repetitive data, we could play with the \"--repeat-penalty\" and \"--repeat-last-n\" flags to penalize the model for generating similar/same tokens/sequences and force it to generate \"[end of text]\" token. In this case, it seems that the only way would be to set the limit on generated tokens, but we would need to know the size of the result.\n",
    "\n",
    "Still, this issue is present mostly for the models with stronger quantizations, and using something like Q_5/Q_6 quantization should be enough to prevent this issue from happening."
   ],
   "id": "57feb96765ba8c51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Performance\n",
    "I did not prepare detailed performance tests, but I used Llama.cpp's built-in benchmarks to showcase \\\n",
    " how different models behave with the Q5_K_M quantization (pp stands for prompt processing test, \\\n",
    " while tg stands for token generation test; 512 and 128 represent the test size in tokens):\n",
    "\n",
    "## 4.1 Laptop performance (16gb ram)\n",
    "\n",
    "| model                              |       size |     params | backend    | threads |          test |                  t/s |\n",
    "|------------------------------------| ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n",
    "| (Llama 3.2) llama 1B Q5_K - Medium | 861.81 MiB |     1.24 B | CPU        |       6 |         pp512 |        112.95 ± 8.73 |\n",
    "| (Llama 3.2) llama 1B Q5_K - Medium | 861.81 MiB |     1.24 B | CPU        |       6 |         tg128 |         34.98 ± 2.50 |\n",
    "| (Llama 3) llama 8B Q5_K - Medium   |   5.33 GiB |     8.03 B | CPU        |       6 |         pp512 |         14.69 ± 0.22 |\n",
    "| (Llama 3) llama 8B Q5_K - Medium   |   5.33 GiB |     8.03 B | CPU        |       6 |         tg128 |          6.23 ± 0.23 |\n",
    "| (Llama 2) llama 7B Q5_K - Medium   |   4.45 GiB |     6.74 B | CPU        |       6 |         pp512 |         15.42 ± 0.22 |\n",
    "| (Llama 2) llama 7B Q5_K - Medium   |   4.45 GiB |     6.74 B | CPU        |       6 |         tg128 |          7.10 ± 0.11 |\n",
    "| (Hermes) llama 3B Q5_K - Medium    |   2.16 GiB |     3.21 B | CPU        |       6 |         pp512 |         35.35 ± 0.47 |\n",
    "| (Hermes) llama 3B Q5_K - Medium    |   2.16 GiB |     3.21 B | CPU        |       6 |         tg128 |         13.88 ± 0.44 |\n",
    "\n",
    "## 4.2 Android performance (Redmi Note 11, 6gb ram)\n",
    "| model                              |       size |     params | backend    | threads |          test |                  t/s |\n",
    "|------------------------------------| ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n",
    "| (Llama 3.2) llama 1B Q5_K - Medium | 861.81 MiB |     1.24 B | CPU        |       8 |         pp512 |         20.25 ± 0.14 |\n",
    "| (Llama 3.2) llama 1B Q5_K - Medium | 861.81 MiB |     1.24 B | CPU        |       8 |         tg128 |          6.66 ± 0.29 |\n",
    "| (Llama 3) llama 8B Q5_K - Medium   |   5.33 GiB |     8.03 B | CPU        |       8 |         pp512 |          1.99 ± 0.06 |\n",
    "| (Llama 3) llama 8B Q5_K - Medium   |   5.33 GiB |     8.03 B | CPU        |       8 |         tg128 |          0.09 ± 0.00 |\n",
    "| (Llama 2) llama 7B Q5_K - Medium   |   4.45 GiB |     6.74 B | CPU        |       8 |         pp512 |          2.08 ± 0.10 |\n",
    "| (Llama 2) llama 7B Q5_K - Medium   |   4.45 GiB |     6.74 B | CPU        |       8 |         tg128 |          0.10 ± 0.00 |\n",
    "| (Hermes) llama 3B Q5_K - Medium    |   2.16 GiB |     3.21 B | CPU        |       8 |         pp512 |          5.47 ± 0.41 |\n",
    "| (Hermes) llama 3B Q5_K - Medium    |   2.16 GiB |     3.21 B | CPU        |       8 |         tg128 |          2.26 ± 0.10 |\n",
    "\n",
    "## 4.3 Summary\n",
    "First of all, very important note: my phone has problems with the battery (it's always at 1%, even when it's full), and it might have an impact \\\n",
    "on the results. I don't have any other phone available to me, and I wanted to have some results from the phone, so I decided to include them. \\\n",
    "Keeping that in mind, it's not difficult to notice a drastic difference in performance between the laptop and the phone. \\\n",
    "So drastic that I would advise against using models larger than 3B parameters (and Hermes is still slow). And, to put differences between models \\\n",
    "tested on the phone into perspective, it took me around 2-3 minutes to run benchmarks for the 1B Llama. and 3-4 hours to run the Llama 3 model (8B). \\\n",
    "Of course, we can play with different quantizations, but large models are orders of magnitude slower than e.g. Llama 1B.\n",
    "\n",
    "About the Hermes model; I was looking for some alternatives to Meta-\"native\" Llamas, and I found it. It had already prepared .gguf files, \\\n",
    "so I decided to give it a try. This model is a \"full parameter fine-tune\" of the base Llama, so I should probably test the base model, \\\n",
    "especially if I don't care about the accuracy for now, but I managed to run it, so I wanted to include it in the benchmarks."
   ],
   "id": "a58b4adf43d397e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Helpful tools/links\n",
    "## 5.1 CURLs\n",
    "I found it tedious to download models on android, so to make your life easier, I include here CURLs to download models that I used in this notebook (as you can see, two of those .gguf files were made by me):\n",
    "```bash\n",
    "curl -L -o llama-2-7b.Q5_K_M.gguf https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q5_K_M.gguf\n",
    "\n",
    "curl -L -o Hermes-3-Llama-3.2-3B.Q5_K_M.gguf https://huggingface.co/NousResearch/Hermes-3-Llama-.2-3B-GGUF/resolve/main/Hermes-3-Llama-3.2-3B.Q5_K_M.gguf\n",
    "\n",
    "curl -L -o llama-3-8B.Q5_K_M.gguf https://huggingface.co/GustawB/Llama-3-8B-ggufs/resolve/main/llama-3-8B.Q5_K_M.gguf\n",
    "\n",
    "curl -L -o llama-3.2-1B.Q5_K_M.gguf https://huggingface.co/GustawB/Llama-3.2-1B-ggufs/resolve/main/llama-3.2-1B.Q5_K_M.gguf\n",
    "```\n",
    "\n",
    "## 5.2 scrcpy\n",
    "I'm lazy, and I don't like typing long commands on my phone. So I decided to find a tool to mirror my phone screen on my PC. I found [scrcpy](https://github.com/Genymobile/scrcpy) from Genymobile. It's very easy in installation and usage, it mirrors your phone's screen to your desktop, and it lets you use both PC controls and phone controls simultaneously.Below is a screen from the sample session using this tool:\n",
    "![scrcpy](images/scrcpy_usage.png)"
   ],
   "id": "2079446fadb018d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:12:35.656714Z",
     "start_time": "2024-12-22T21:12:35.654928Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "710e1c6872e1cc14",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
