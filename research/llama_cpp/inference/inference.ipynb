{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 0. Introduction\n",
    "In this notebook, I will go through inference examples with models both listed as supported by Llama.cpp, and one that is technically not listed, but I managed to run it. Additionally, I will present a way to enforce a specific json format of the model output.\n",
    "\n",
    "Throughout this notebook, I will assume that the reader has a basic understanding of the contents of the 'introduction' folder that contains example workflows of Llama.cpp."
   ],
   "id": "ecf10ba59e0ba761"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Enforcing a specific json format\n",
    "Llama.cpp \"llama-cli\" binary supports enforcing different json formats for the output using [GNBF](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md) grammars or [JSON schemas](https://json-schema.org/). When using JSON schemas, even the one provided as examples, I ran into issues. Luckily, Llama.cpp provides a [script](https://github.com/ggerganov/llama.cpp/blob/master/examples/json_schema_to_grammar.py) which can be used to convert JSON schemas to GNBF grammars. This is the approach that I used.\n",
    "\n",
    "For the example purposes, I will use the [Llama 3.2 1B  model](https://huggingface.co/meta-llama/Llama-3.2-1B) with K_5 quantization (more on models and quantizations later in the notebook). Also, I will use \"prompt.txt\" file with a very small example prompt and \"schema.json\" file with the schema that I want to enforce on the model output. Both of these files can be found under the \"miscellaneous\" directory.\n",
    "\n",
    "To run the inference with the enforced schema, I ran the following command\"\n",
    "```bash\n",
    "./build/bin/llama-cli -m ~/llama3215.gguf --file prompt_large.txt --grammar \"$( python examples/json_schema_to_grammar.py schema.json )\"\n",
    "```\n",
    "\n",
    "The result of this command is the following: \\\n",
    "![Example result](images/prompt_example.png)\n",
    "\n",
    "Few things to note here:\n",
    "1. The output matches the provided schema, but its formatting may differ between different runs.\n",
    "2. It is very accurate; but, for bigger prompts (example in the \"prompt_large.txt\" file), the output omits some of the data.\n",
    "3. This schema does not enforce the model to end. Older models/ models with stronger quantizations may not generate the \"[end of text]\" token and will generate data infinitely (unless we prevent them from doing so).\n",
    "4. Prompt has a specific format; Llama models are, by default, trained for sentence completion, so simply asking the model what are the products in the file may cause it to talk about something related, but not produce desired results. Because of this, prompts need to be formatted in a way that the model will \"complete\" them, not answer them.\n"
   ],
   "id": "b338c50a8658a58e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c4d29aaf5bdec84d"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
