{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Possibilities for ~~TensorFlow Lite~~ LiteRT usage",
   "id": "61c3e3f13ceee45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Dependencies install (may require several GB of storage, Im so sorry for that :((( ):\n",
    "I strongly recommend creating separate venv for this branch. Otherwise `optimum` might installation go crazy and download like 30 versions of tensorflow each 600MB big (idk why)\n",
    "```bash\n",
    "pip install optimum[exporters-tf]\n",
    "```\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ],
   "id": "4d2d4ad1d86fb495"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Getting .tflite model\n",
    "In this section I will present different methods for getting .tflite model file and note results of my experiments"
   ],
   "id": "eb430445ff9a37d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### optimum tool\n",
    "Optimum is tool that allows converting selected hugging face models to .tflite format with simple bash command. Here is [list](https://huggingface.co/docs/optimum/exporters/tflite/overview) of supported architectures. Note that this list contains architectures, not checkpoints so we can theoretically download fine-tuned or quantitized versions of models. In practise I have tested several bert derivatives and none of them worked. \\\n",
    "Command for converting model directly from HF:\n",
    "```shell\n",
    "optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n",
    "```\n",
    "Above command generates directory containing several files. The most important ones are `model.tflite` containing model itself and `tokenizer.json` with tokens assignment. "
   ],
   "id": "cfc952136c35d819"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### tflite.TFLiteConverter\n",
    "Solution available in `tensorflow` library that converts models from following formats - [docs](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter):\n",
    "* Jax model\n",
    "* Keras model\n",
    "* SavedModel format\n",
    "* tf.ConcreteFunction\n",
    "Theoretically converting Keras and ConcreteFunction models makes large amount of checkpoints available to us.\\\n",
    "* At this moment I did not tested JAX models\n",
    "* Probably the easiest way of getting transformers in Keras is [KerasNLP](https://keras.io/api/keras_nlp/models/). They even have nice [tutorial](https://github.com/tensorflow/codelabs/blob/main/KerasNLP/io2023_workshop.ipynb) on fine tuning and exporting LLMs to .tflite. Sadly it is no longer working. Regardless of that below I have places some demo on exporting GPT2\n",
    "* SavedModel give us opportunity to use TensorFlow models\n",
    "* tf.ConcreteFunction can be used to export functions decorated with `@tf.function`. Sounds useful but I wasn't able to get it working.\n",
    "#### Demo of exporting Keras model"
   ],
   "id": "c51e2c9287a44d36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model saved above is generally hard to utilize due to strange input/output format. I had no time for making it work (requires further investigation if we want to use tflite)",
   "id": "ab04f59fbb8543f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ai_edge_torch\n",
    "another [tool](https://ai.google.dev/edge/litert/models/convert_pytorch) provided by google that enables us converting pytorch models to tflite. It supports quantization.\\\n",
    "* for further research, I did not manage to have some functional demo on LLMs"
   ],
   "id": "23f3a75cd9d00b24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Deploying model on mobile\n",
    "### Local Inference\n",
    "#### Tf Lite wrappers\n",
    "TensorFlow Lite provides wrappers for some classes of models, an example is bert for question answering task. [Here](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android) is demo showing it.\\\n",
    "Note: to get example running I had to downgrade JDK to 17 but I do not guarantee that this will work for you\n",
    "#### Interpreter API\n",
    "Interpreter API is more flexible way of running `.tflite` models. In `app/` directory there is a demonstrational application that uses it.\n",
    "* Interpreter [docs](https://ai.google.dev/edge/api/tflite/java/org/tensorflow/lite/Interpreter) Java - to be used on the edge\n",
    "* Interpreter [docs](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) Python - to be used on dev machine\\\n",
    "Interpreter is object that runs model encoded into .tflite file. Single inference can be done with `run(...)` or `runForMultipleInputsOutputs(...)` if our model takes multiple inputs and/or returns multiple outputs.\n",
    "#### Working demo\n",
    "I have prepared an app that utilizes Interpreter API and runs BERT inference locally to perform mask filling task.\n",
    "This demo uses files generated by optimum tool. You should generate tflite model before deploying app.\n",
    ".tflite file and tokenizer.json files should be placed in `/main/assets` directory in android studio project. For proper creation of `assets` dir rightclick folder->new->Folder->Assets Folder. Files there can be accessed via `Context.assets.openFd(path)`. \n",
    "##### On specyfing proper input for Interpreter\n",
    "To get info about required input for .tflite model:\n",
    "1. Load model into notebook\n"
   ],
   "id": "54a3c833f31c1786"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T01:08:15.001558Z",
     "start_time": "2024-11-21T01:08:11.318227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "interpreter = tf.lite.Interpreter(model_path=\"bert_tflite/model.tflite\")"
   ],
   "id": "da0140bc5dada8f3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 02:08:11.689640: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-21 02:08:11.858237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732151291.917072   12309 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732151291.934772   12309 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-21 02:08:12.084422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/szymon/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_12309/933251911.py\", line 1, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/tensorflow/__init__.py\", line 49, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import autograph\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/core/ag_ctx.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.utils import ag_logging\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/utils/__init__.py\", line 17, in <module>\n",
      "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/utils/context_managers.py\", line 19, in <module>\n",
      "    from tensorflow.python.framework import ops\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 62, in <module>\n",
      "    from tensorflow.python.framework import tensor as tensor_lib\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/tensorflow/python/framework/tensor.py\", line 35, in <module>\n",
      "    from tensorflow.python.framework import tensor_util\n",
      "  File \"/home/szymon/murmuras/ZPP_Murmuras/research/LiteRT_deploymnet/.venv/lib/python3.9/site-packages/tensorflow/python/framework/tensor_util.py\", line 39, in <module>\n",
      "    from tensorflow.python.framework import fast_tensor_util\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Run following code",
   "id": "188c55c0fe83a0f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T01:09:12.100898Z",
     "start_time": "2024-11-21T01:09:12.095811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "\n",
    "# Print input details\n",
    "for i, input_tensor in enumerate(input_details):\n",
    "    print(f\"Input {i}:\")\n",
    "    print(f\"  Name: {input_tensor['name']}\")\n",
    "    print(f\"  Shape: {input_tensor['shape']}\")\n",
    "    print(f\"  Data type: {input_tensor['dtype']}\")\n",
    "    print(f\"  Quantization parameters: {input_tensor['quantization']}\")\n",
    "    \n",
    "output_details = interpreter.get_output_details()\n",
    "for i, output_tensor in enumerate(output_details):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(f\"  Name: {output_tensor['name']}\")\n",
    "    print(f\"  Shape: {output_tensor['shape']}\")\n",
    "    print(f\"  Data type: {output_tensor['dtype']}\")"
   ],
   "id": "19be84229623528b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 0:\n",
      "  Name: model_attention_mask:0\n",
      "  Shape: [  1 128]\n",
      "  Data type: <class 'numpy.int64'>\n",
      "  Quantization parameters: (0.0, 0)\n",
      "Input 1:\n",
      "  Name: model_input_ids:0\n",
      "  Shape: [  1 128]\n",
      "  Data type: <class 'numpy.int64'>\n",
      "  Quantization parameters: (0.0, 0)\n",
      "Input 2:\n",
      "  Name: model_token_type_ids:0\n",
      "  Shape: [  1 128]\n",
      "  Data type: <class 'numpy.int64'>\n",
      "  Quantization parameters: (0.0, 0)\n",
      "Output 0:\n",
      "  Name: StatefulPartitionedCall:0\n",
      "  Shape: [    1   128 30522]\n",
      "  Data type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Working example is provided in `/app` directory mentioned before. App created there performs single mask substitution using bert model.",
   "id": "a2e28ad754524b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Local example of inference with bert and interpreter API",
   "id": "ab363dcdb2efc318"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T01:14:39.357345Z",
     "start_time": "2024-11-21T01:14:38.826069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "with open(\"bert_tflite/tokenizer.json\", \"r\") as f:\n",
    "    tokens = json.load(f)\n",
    "rmap = {v: k for k, v in tokens[\"model\"][\"vocab\"].items()}\n",
    "def detokenize(tensor):\n",
    "    res = []\n",
    "    for e in tensor:\n",
    "        i = np.argmax(e)\n",
    "        res.append(rmap[int(i)])\n",
    "    return res\n",
    "\n",
    "def tokenize(text):\n",
    "    return [tokens[\"model\"][\"vocab\"][w] for w in text.split()]\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.reset_all_variables()\n",
    "inp = tokenize(\"[SEP] paris is [MASK] of france\")\n",
    "interpreter.set_tensor(input_details[1]['index'], tf.constant([([0] * (128 - len(inp))) + inp], dtype=tf.int64))\n",
    "interpreter.set_tensor(input_details[0]['index'], tf.constant([[0] * 128], dtype=tf.int64))\n",
    "interpreter.set_tensor(input_details[2]['index'], tf.constant([[0] * 128], dtype=tf.int64))\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "detokenize(output_data[0])[-5:]"
   ],
   "id": "b52a1175f0d47fa3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paris', 'is', 'capital', 'of', 'france']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "345ae078009218ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
