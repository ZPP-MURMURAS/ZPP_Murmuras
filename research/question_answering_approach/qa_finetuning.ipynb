{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# QA finetuning\n",
    "Notebook demonstrating fine-tuning bert model for qa task"
   ],
   "id": "c2bf7203b78a6b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning on coupon data\n",
    "### 1. dataset preparation"
   ],
   "id": "8bd22e3a1d58f9ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:50:09.048038Z",
     "start_time": "2024-11-27T22:50:08.036006Z"
    }
   },
   "cell_type": "code",
   "source": "import json, pandas",
   "id": "864865a3765996c8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:04:03.534625Z",
     "start_time": "2024-11-27T23:04:03.526154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "USED_COLUMNS = [\"Text\", \"View Class Name\"]\n",
    "\n",
    "with open(\"ds/18929485529_expected.json\", \"r\", encoding='utf-8') as f:\n",
    "    resps = json.load(f)\n",
    "    \n",
    "for x in resps[\"coupons\"]:\n",
    "    x.pop(\"discount\")\n",
    "    x.pop(\"validity\")\n",
    "    \n",
    "frame = pandas.read_csv(\"ds/18929485529.csv\", encoding='utf-8')\n",
    "# currently hardcoded\n",
    "sample_indices = [slice(2, 7), slice(7, 12), slice(49, 54), slice(78, 83), slice(92, 97), slice(97, 102)]\n",
    "\n",
    "frame = frame[USED_COLUMNS]\n",
    "contexts = [frame[ind].to_csv() for ind in sample_indices]\n",
    "\n",
    "QUESTIONS = {\n",
    "    \"old_price\": \"What was the old, higher price of product?\",\n",
    "    \"new_price\": \"What is the current price of product?\",\n",
    "    \"product_name\": \"How is the product named?\"\n",
    "}"
   ],
   "id": "d5d85a76ed13a712",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Converting answers to locations in contexts\n",
    "Note on dataset:\n",
    "I have cleared dataset provided in coupon-extraction-demo repo:\n",
    "* I have removed `FELIX Knabber Mix 12 x 85 g` and `FELIX So gut wie es aussieht in Gelee` products as I believe they are not correctly labeled\n",
    "* i have changed old price of `FELIX Knabber Mix 200 g` to 2.99 and name to \"FELIX Knabber Mix\""
   ],
   "id": "726c8751920d2be0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:04:05.598546Z",
     "start_time": "2024-11-27T23:04:05.337488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.vocab[\"[Q1]\"] = tokenizer.vocab[\"[unused128]\"]\n",
    "tokenizer.vocab[\"[Q2]\"] = tokenizer.vocab[\"[unused129]\"]\n",
    "tokenizer.vocab[\"[Q3]\"] = tokenizer.vocab[\"[unused130]\"]\n",
    "\n",
    "answers_converted = {k: [] for k in QUESTIONS}\n",
    "answers = {k: [e[k] for e in resps['coupons']] for k in QUESTIONS}\n",
    "\n",
    "tokenized = tokenizer(contexts, return_offsets_mapping=True, add_special_tokens=False)\n",
    "\n",
    "for i, (ctx, tokenized_ctx, ctx_offsets) in enumerate(zip(contexts, tokenized[\"input_ids\"], tokenized[\"offset_mapping\"])):\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_ctx)\n",
    "    token_offsets = []\n",
    "    for token, (start, end) in zip(decoded_tokens, ctx_offsets):\n",
    "        token_offsets.append({\"token\": token, \"start\": start, \"end\": end, \"text\": ctx[start:end]})\n",
    "\n",
    "    # Print tokens alongside their positions and text\n",
    "    \"\"\"for t in token_offsets:\n",
    "        print(f\"Token: {t['token']}, Start: {t['start']}, End: {t['end']}, Text: '{t['text']}'\")\"\"\"\n",
    "        \n",
    "    for q in answers:\n",
    "        answer = answers[q][i]\n",
    "        start_char = ctx.find(answer)\n",
    "        end_char = start_char + len(answer)\n",
    "        \n",
    "        # Locate the corresponding tokens\n",
    "        start_token_idx = None\n",
    "        end_token_idx = None\n",
    "        \n",
    "        for idx, (start, end) in enumerate(ctx_offsets):\n",
    "            if start <= start_char < end:\n",
    "                start_token_idx = idx\n",
    "            if start < end_char <= end:\n",
    "                end_token_idx = idx\n",
    "                break\n",
    "        \n",
    "        print(f\"Answer: '{answer}'\")\n",
    "        print(f\"Character-level Start: {start_char}, End: {end_char}\")\n",
    "        print(f\"Token-level Start: {start_token_idx}, End: {end_token_idx}\")\n",
    "        \n",
    "        answers_converted[q].append([start_token_idx, end_token_idx])"
   ],
   "id": "63a92a7318ea2c23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '14.99'\n",
      "Character-level Start: 29, End: 34\n",
      "Token-level Start: 10, End: 12\n",
      "Answer: '9.99'\n",
      "Character-level Start: 62, End: 66\n",
      "Token-level Start: 23, End: 25\n",
      "Answer: 'JOHNNIE WALKER Red Label Blended Scotch'\n",
      "Character-level Start: 125, End: 164\n",
      "Token-level Start: 48, End: 53\n",
      "Answer: '0.99'\n",
      "Character-level Start: 29, End: 33\n",
      "Token-level Start: 10, End: 12\n",
      "Answer: '0.75'\n",
      "Character-level Start: 61, End: 65\n",
      "Token-level Start: 23, End: 25\n",
      "Answer: 'SAN MIGUEL Especial'\n",
      "Character-level Start: 125, End: 144\n",
      "Token-level Start: 48, End: 52\n",
      "Answer: '2.99'\n",
      "Character-level Start: 30, End: 34\n",
      "Token-level Start: 10, End: 12\n",
      "Answer: '2.79'\n",
      "Character-level Start: 63, End: 67\n",
      "Token-level Start: 23, End: 25\n",
      "Answer: 'FELIX Knabber Mix'\n",
      "Character-level Start: 128, End: 145\n",
      "Token-level Start: 48, End: 52\n",
      "Answer: '8.99'\n",
      "Character-level Start: 30, End: 34\n",
      "Token-level Start: 10, End: 12\n",
      "Answer: '5.85'\n",
      "Character-level Start: 63, End: 67\n",
      "Token-level Start: 23, End: 25\n",
      "Answer: 'CHANTRÃ‰ Weinbrand'\n",
      "Character-level Start: 128, End: 145\n",
      "Token-level Start: 48, End: 52\n",
      "Answer: '2.79'\n",
      "Character-level Start: 26, End: 30\n",
      "Token-level Start: 8, End: 10\n",
      "Answer: '2.49'\n",
      "Character-level Start: 59, End: 63\n",
      "Token-level Start: 21, End: 23\n",
      "Answer: 'PENNY Gouda-Scheiben'\n",
      "Character-level Start: 132, End: 152\n",
      "Token-level Start: 49, End: 55\n",
      "Answer: '1.09'\n",
      "Character-level Start: 26, End: 30\n",
      "Token-level Start: 8, End: 10\n",
      "Answer: '0.69'\n",
      "Character-level Start: 59, End: 63\n",
      "Token-level Start: 21, End: 23\n",
      "Answer: 'KINDER Ãœberraschungs-Ei'\n",
      "Character-level Start: 133, End: 156\n",
      "Token-level Start: 49, End: 58\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create JSON dataset and convert it to datasets library object",
   "id": "d3b29560324da58b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:04:07.342391Z",
     "start_time": "2024-11-27T23:04:06.946856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "as_json = [{\n",
    "   \"id\": ci * len(QUESTIONS) + qi,\n",
    "   \"title\":\"example_title\",\n",
    "   \"context\": ctx,\n",
    "   \"question\": QUESTIONS[q_key],\n",
    "   \"answers\":{\n",
    "      \"text\":[\n",
    "         answers[q_key][ci]\n",
    "      ],\n",
    "      \"answer_start\":[\n",
    "         answers_converted[q_key][ci][0]\n",
    "      ]\n",
    "   }\n",
    "} for ci, ctx in enumerate(contexts) for qi, q_key in enumerate(QUESTIONS)]\n",
    "with open(\"ds.json\", \"w\", encoding='utf-8') as f:\n",
    "    for entry in as_json:\n",
    "        json.dump(entry, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"ds.json\")\n",
    "dataset"
   ],
   "id": "340be9edd4481846",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49bde32838b1430f827fca260d424528"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 18\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:04:07.386457Z",
     "start_time": "2024-11-27T23:04:07.343388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answers\"]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, answer in enumerate(answers):\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = start_char + len(answer['text'][0])\n",
    "        \n",
    "        # Map start and end character positions to token indices\n",
    "        start_positions.append(inputs.char_to_token(i, start_char))\n",
    "        end_positions.append(inputs.char_to_token(i, end_char - 1))\n",
    "        \n",
    "        # Handling edge cases where the tokenizer may not capture the exact indices\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length - 1\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset['train'].map(preprocess_function, batched=True)\n"
   ],
   "id": "4949bb6c6727f523",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b98151cc6904bde9f6c6c8a2e868561"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets test basic bert on our problem",
   "id": "155fdc3739fb8222"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:04:07.869790Z",
     "start_time": "2024-11-27T23:04:07.489946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\")\n",
    "result = qa_pipeline({\"context\": contexts[5], \"question\": QUESTIONS[\"old_price\"]})\n",
    "print(result)"
   ],
   "id": "265693e8637c8a75",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\szymon\\Desktop\\pycharm\\ZPP_Murmuras\\venv\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.00034702528500929475, 'start': 96, 'end': 118, 'answer': 'gespart,android.widget'}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we see it is performing poorly\n",
    "### Fine-Tuning\n",
    "You may skip this cell and just download fine-tuned model from my HuggingFace profile below"
   ],
   "id": "abe1d49fe3fd72c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:12:25.919597Z",
     "start_time": "2024-11-27T23:12:25.911256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "id": "f871a122fd612561",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8946cdcbd8e405aa9654b62cdd71847"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:38:28.622443Z",
     "start_time": "2024-11-27T23:15:16.695825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"bert-uncased-finetuned-csv-qa\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.02,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "id": "cc5c5be1e0a1f507",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\szymon\\Desktop\\pycharm\\ZPP_Murmuras\\venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\szymon\\AppData\\Local\\Temp\\ipykernel_6356\\3248611141.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 22:49, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.441133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.568306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.901905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.364568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.809478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.315282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.945634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.690385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.463345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.216905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.996276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.837834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.706813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.600760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.523185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.465860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.414006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.373982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.314173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.291183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.271062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.272958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.264544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.250704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.243232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.237253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.240492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.239674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.235222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.229756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.206757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.203890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.203601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.200404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.202953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.200281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.195787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.191459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.186824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.181187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.176851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.172868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.169764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.168779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.0835047149658203, metrics={'train_runtime': 1391.4307, 'train_samples_per_second': 0.647, 'train_steps_per_second': 0.072, 'total_flos': 235167081062400.0, 'train_loss': 1.0835047149658203, 'epoch': 50.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:43:59.053046Z",
     "start_time": "2024-11-27T23:43:08.617417Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.push_to_hub()",
   "id": "8e48a68f9891873",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70333007d14643c597d29e58c5274c8c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0acc4306644248d3a31eed9079aa9159"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24b8cb25c2024989bb8a935ea49b0535"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SzymonKozl/bert-uncased-finetuned-csv-qa/commit/c76e76790667977ab28482f9ee31c21877e7c188', commit_message='End of training', commit_description='', oid='c76e76790667977ab28482f9ee31c21877e7c188', pr_url=None, repo_url=RepoUrl('https://huggingface.co/SzymonKozl/bert-uncased-finetuned-csv-qa', endpoint='https://huggingface.co', repo_type='model', repo_id='SzymonKozl/bert-uncased-finetuned-csv-qa'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5b0d4383cf316259"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:46:34.072381Z",
     "start_time": "2024-11-27T23:46:33.663815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"SzymonKozl/bert-uncased-finetuned-csv-qa\", tokenizer=\"SzymonKozl/bert-uncased-finetuned-csv-qa\")\n",
    "result = qa_pipeline(**{\"context\": contexts[0], \"question\": QUESTIONS[\"product_name\"]})\n",
    "print(result)"
   ],
   "id": "8bf8c81683841f2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0004328570794314146, 'start': 125, 'end': 143, 'answer': 'JOHNNIE WALKER Red'}\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we see the results are not perfect but there is some improvement",
   "id": "d044efd4162c52ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusions\n",
    "It is hard to draw a conclusions from fine tuning on such small dataset. However here are several observations that might help in further work:\n",
    "* almost identical prompts do not work. Following example resulted in model answering the same answer to each question:\n",
    "```py\n",
    "QUESTIONS = {\n",
    "    \"old_price\": \"What is the old price of product?\",\n",
    "    \"new_price\": \"What is the new price of product?\",\n",
    "    \"product_name\": \"What is the name of the discounted product?\"\n",
    "}\n",
    "```\n",
    "* fine tuning QA for CSV blocks with no dropped columns results in nan loss and prevents training at all\n",
    "* At this moment evaluation results are not looking promising - but we will need to check on real datasets - purpose of this notebook was to show possibility to treat our problem as QA task"
   ],
   "id": "2eef1e16f802c59b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
