{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# QA finetuning\n",
    "Notebook demonstrating fine-tuning bert model for qa task"
   ],
   "id": "c2bf7203b78a6b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Background\n",
    "### What is QA task\n",
    "QA is machine learning task focused on retrieving answers for questions from given text context. In this notebook I will experiment with extractive QA, architecture in which model predicts fragment of input context in which response is. It is worth noticing that there are different approaches to QA task, like generative QA which allows model to generate response that might not be explicit part of context.\n",
    "### Why we would want to use this?\n",
    "Our task seems similar to extractive QA. There are some limitations to be addressed, for product name being split with some xml tag like\n",
    "```xml\n",
    "Å›miej<bold>Å¼elki</bold>\n",
    "```\n",
    "that can make it impossible to select single fragment containing answer to question \"What is name of product?\" but in general this approach seems suitable.\n",
    "### Is it possible?\n",
    "tldr: idk </br>\n",
    "I did not find and fine-tunes of BERT on xml or even html. Although there are llama models finetuned for RAG, task similar to QA working with HTML data (like zstanjj/HTML-Pruner-Llama-1B).\n"
   ],
   "id": "5d498c9a0be960a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Before starting\n",
    "put 18929485529.csv and 18929485529_expected.json into /ds folder"
   ],
   "id": "798623b2301a9a46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning on coupon data\n",
    "### 1. dataset preparation\n",
    "We remove most of the columns and create contexts from hardcoded slices of dataset"
   ],
   "id": "8bd22e3a1d58f9ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T20:04:05.518666Z",
     "start_time": "2024-12-03T20:04:05.124132Z"
    }
   },
   "cell_type": "code",
   "source": "import json, pandas",
   "id": "864865a3765996c8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T20:04:45.221905Z",
     "start_time": "2024-12-03T20:04:45.210803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "USED_COLUMNS = [\"Text\", \"View Class Name\"]\n",
    "\n",
    "with open(\"ds/18929485529_expected.json\", \"r\", encoding='utf-8') as f:\n",
    "    resps = json.load(f)\n",
    "    \n",
    "for x in resps[\"coupons\"]:\n",
    "    x.pop(\"discount\")\n",
    "    x.pop(\"validity\")\n",
    "    \n",
    "frame = pandas.read_csv(\"ds/18929485529.csv\", encoding='utf-8')\n",
    "# currently hardcoded\n",
    "sample_indices = [slice(2, 7), slice(7, 12), slice(49, 54), slice(78, 83), slice(92, 97), slice(97, 102)]\n",
    "\n",
    "frame = frame[USED_COLUMNS]\n",
    "contexts = [frame[ind].to_csv() for ind in sample_indices]\n",
    "\n",
    "QUESTIONS = {\n",
    "    \"old_price\": \"What was the old, higher price of product?\",\n",
    "    \"new_price\": \"What is the current price of product?\",\n",
    "    \"product_name\": \"How is the product named?\"\n",
    "}"
   ],
   "id": "d5d85a76ed13a712",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Converting answers to locations in contexts\n",
    "Now we need to convert our dataset to format used in QA training. That means connecting contexts and questions with relevant answers.\n",
    "#### Note on dataset used\n",
    "I have cleared dataset provided in coupon-extraction-demo repo:\n",
    "* I have removed `FELIX Knabber Mix 12 x 85 g` and `FELIX So gut wie es aussieht in Gelee` products as I believe they are not correctly labeled\n",
    "* i have changed old price of `FELIX Knabber Mix 200 g` to 2.99 and name to \"FELIX Knabber Mix\""
   ],
   "id": "726c8751920d2be0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T20:04:52.947394Z",
     "start_time": "2024-12-03T20:04:51.079073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "answers_converted = {k: [] for k in QUESTIONS}\n",
    "answers = {k: [e[k] for e in resps['coupons']] for k in QUESTIONS}\n",
    "\n",
    "tokenized = tokenizer(contexts, return_offsets_mapping=True, add_special_tokens=False)\n",
    "\n",
    "for i, (ctx, tokenized_ctx, ctx_offsets) in enumerate(zip(contexts, tokenized[\"input_ids\"], tokenized[\"offset_mapping\"])):\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_ctx)\n",
    "    token_offsets = []\n",
    "    for token, (start, end) in zip(decoded_tokens, ctx_offsets):\n",
    "        token_offsets.append({\"token\": token, \"start\": start, \"end\": end, \"text\": ctx[start:end]})\n",
    "        \n",
    "    for q in answers:\n",
    "        answer = answers[q][i]\n",
    "        start_char = ctx.find(answer)\n",
    "        end_char = start_char + len(answer)\n",
    "        \n",
    "        # Locate the corresponding tokens\n",
    "        start_token_idx = None\n",
    "        end_token_idx = None\n",
    "        \n",
    "        for idx, (start, end) in enumerate(ctx_offsets):\n",
    "            if start <= start_char < end:\n",
    "                start_token_idx = idx\n",
    "            if start < end_char <= end:\n",
    "                end_token_idx = idx\n",
    "                break\n",
    "        \n",
    "        print(f\"Answer: '{answer}'\")\n",
    "        print(f\"Character-level Start: {start_char}, End: {end_char}\")\n",
    "        print(f\"Token-level Start: {start_token_idx}, End: {end_token_idx}\")\n",
    "        \n",
    "        answers_converted[q].append([start_token_idx, end_token_idx])"
   ],
   "id": "63a92a7318ea2c23",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/murmuras/ZPP_Murmuras/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '14.99'\n",
      "Character-level Start: 28, End: 33\n",
      "Token-level Start: 10, End: 12\n",
      "Answer: '9.99'\n",
      "Character-level Start: 60, End: 64\n",
      "Token-level Start: 23, End: 25\n",
      "Answer: 'JOHNNIE WALKER Red Label Blended Scotch'\n",
      "Character-level Start: 121, End: 160\n",
      "Token-level Start: 48, End: 53\n",
      "Answer: '0.99'\n",
      "Character-level Start: 28, End: 32\n",
      "Token-level Start: 10, End: 12\n",
      "Answer: '0.75'\n",
      "Character-level Start: 59, End: 63\n",
      "Token-level Start: 23, End: 25\n",
      "Answer: 'SAN MIGUEL Especial'\n",
      "Character-level Start: 121, End: 140\n",
      "Token-level Start: 48, End: 52\n",
      "Answer: '2.99'\n",
      "Character-level Start: 29, End: 33\n",
      "Token-level Start: 10, End: 12\n",
      "Answer: '2.79'\n",
      "Character-level Start: 61, End: 65\n",
      "Token-level Start: 23, End: 25\n",
      "Answer: 'FELIX Knabber Mix'\n",
      "Character-level Start: 124, End: 141\n",
      "Token-level Start: 48, End: 52\n",
      "Answer: '8.99'\n",
      "Character-level Start: 29, End: 33\n",
      "Token-level Start: 10, End: 12\n",
      "Answer: '5.85'\n",
      "Character-level Start: 61, End: 65\n",
      "Token-level Start: 23, End: 25\n",
      "Answer: 'CHANTRÃ‰ Weinbrand'\n",
      "Character-level Start: 124, End: 141\n",
      "Token-level Start: 48, End: 52\n",
      "Answer: '2.79'\n",
      "Character-level Start: 25, End: 29\n",
      "Token-level Start: 8, End: 10\n",
      "Answer: '2.49'\n",
      "Character-level Start: 57, End: 61\n",
      "Token-level Start: 21, End: 23\n",
      "Answer: 'PENNY Gouda-Scheiben'\n",
      "Character-level Start: 128, End: 148\n",
      "Token-level Start: 49, End: 55\n",
      "Answer: '1.09'\n",
      "Character-level Start: 25, End: 29\n",
      "Token-level Start: 8, End: 10\n",
      "Answer: '0.69'\n",
      "Character-level Start: 57, End: 61\n",
      "Token-level Start: 21, End: 23\n",
      "Answer: 'KINDER Ãœberraschungs-Ei'\n",
      "Character-level Start: 129, End: 152\n",
      "Token-level Start: 49, End: 58\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create JSON dataset and convert it to datasets library object",
   "id": "d3b29560324da58b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T20:05:36.721245Z",
     "start_time": "2024-12-03T20:05:36.205615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "as_json = [{\n",
    "   \"id\": ci * len(QUESTIONS) + qi,\n",
    "   \"title\":\"example_title\",\n",
    "   \"context\": ctx,\n",
    "   \"question\": QUESTIONS[q_key],\n",
    "   \"answers\":{\n",
    "      \"text\":[\n",
    "         answers[q_key][ci]\n",
    "      ],\n",
    "      \"answer_start\":[\n",
    "         answers_converted[q_key][ci][0]\n",
    "      ]\n",
    "   }\n",
    "} for ci, ctx in enumerate(contexts) for qi, q_key in enumerate(QUESTIONS)]\n",
    "with open(\"ds.json\", \"w\", encoding='utf-8') as f:\n",
    "    for entry in as_json:\n",
    "        json.dump(entry, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"ds.json\")\n",
    "dataset"
   ],
   "id": "340be9edd4481846",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 18 examples [00:00, 2229.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 18\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### define preprocessing",
   "id": "5c748890553af6d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:04:07.386457Z",
     "start_time": "2024-11-27T23:04:07.343388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answers\"]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, answer in enumerate(answers):\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = start_char + len(answer['text'][0])\n",
    "        \n",
    "        # Map start and end character positions to token indices\n",
    "        start_positions.append(inputs.char_to_token(i, start_char))\n",
    "        end_positions.append(inputs.char_to_token(i, end_char - 1))\n",
    "        \n",
    "        # Handling edge cases where the tokenizer may not capture the exact indices\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length - 1\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset['train'].map(preprocess_function, batched=True)\n"
   ],
   "id": "4949bb6c6727f523",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b98151cc6904bde9f6c6c8a2e868561"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Reference results\n",
    "we will try to get some results for basic bert model"
   ],
   "id": "155fdc3739fb8222"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T20:07:39.355394Z",
     "start_time": "2024-12-03T20:07:38.896098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\")\n",
    "result = qa_pipeline({\"context\": contexts[5], \"question\": QUESTIONS[\"product_name\"]})\n",
    "print(QUESTIONS[\"product_name\"], result)"
   ],
   "id": "265693e8637c8a75",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is the product named? {'score': 0.00044473286834545434, 'start': 109, 'end': 149, 'answer': 'widget.TextView\\n100,KINDER Ãœberraschungs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/murmuras/ZPP_Murmuras/.venv/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we see it responded to the question about product name with fragment that indeed contains part of product name, but has also a lot of garbage. There is room for improvement.\n",
    "### Fine-Tuning\n",
    "In the following part I perform simple fine tuning on small dataset I've created above. </br>\n",
    "As this process is long, you may skip this cell and just download fine-tuned model from my HuggingFace profile below"
   ],
   "id": "abe1d49fe3fd72c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:12:25.919597Z",
     "start_time": "2024-11-27T23:12:25.911256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "id": "f871a122fd612561",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8946cdcbd8e405aa9654b62cdd71847"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:38:28.622443Z",
     "start_time": "2024-11-27T23:15:16.695825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"bert-uncased-finetuned-csv-qa\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.02,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "id": "cc5c5be1e0a1f507",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\szymon\\Desktop\\pycharm\\ZPP_Murmuras\\venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\szymon\\AppData\\Local\\Temp\\ipykernel_6356\\3248611141.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 22:49, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.441133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.568306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.901905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.364568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.809478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.315282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.945634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.690385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.463345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.216905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.996276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.837834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.706813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.600760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.523185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.465860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.414006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.373982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.314173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.291183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.271062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.272958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.264544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.250704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.243232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.237253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.240492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.239674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.235222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.229756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.206757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.203890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.203601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.200404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.202953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.200281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.195787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.191459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.186824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.181187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.176851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.172868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.169764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.168779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.0835047149658203, metrics={'train_runtime': 1391.4307, 'train_samples_per_second': 0.647, 'train_steps_per_second': 0.072, 'total_flos': 235167081062400.0, 'train_loss': 1.0835047149658203, 'epoch': 50.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:43:59.053046Z",
     "start_time": "2024-11-27T23:43:08.617417Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.push_to_hub()",
   "id": "8e48a68f9891873",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70333007d14643c597d29e58c5274c8c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0acc4306644248d3a31eed9079aa9159"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24b8cb25c2024989bb8a935ea49b0535"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SzymonKozl/bert-uncased-finetuned-csv-qa/commit/c76e76790667977ab28482f9ee31c21877e7c188', commit_message='End of training', commit_description='', oid='c76e76790667977ab28482f9ee31c21877e7c188', pr_url=None, repo_url=RepoUrl('https://huggingface.co/SzymonKozl/bert-uncased-finetuned-csv-qa', endpoint='https://huggingface.co', repo_type='model', repo_id='SzymonKozl/bert-uncased-finetuned-csv-qa'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing fine-tuned model",
   "id": "bffc2ee53e0995a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:46:34.072381Z",
     "start_time": "2024-11-27T23:46:33.663815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"SzymonKozl/bert-uncased-finetuned-csv-qa\", tokenizer=\"SzymonKozl/bert-uncased-finetuned-csv-qa\")\n",
    "result = qa_pipeline(**{\"context\": contexts[0], \"question\": QUESTIONS[\"product_name\"]})\n",
    "print(result)"
   ],
   "id": "8bf8c81683841f2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0004328570794314146, 'start': 125, 'end': 143, 'answer': 'JOHNNIE WALKER Red'}\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we see the results are not perfect (the true name is \"JOHNNIE WALKER Red Label\") but it does not look bad",
   "id": "d044efd4162c52ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusions\n",
    "It is hard to draw a conclusions from fine tuning on such small dataset. However here are several observations that might help in further work:\n",
    "* almost identical prompts do not work. Following example resulted in model answering the same answer to each question:\n",
    "```py\n",
    "QUESTIONS = {\n",
    "    \"old_price\": \"What is the old price of product?\",\n",
    "    \"new_price\": \"What is the new price of product?\",\n",
    "    \"product_name\": \"What is the name of the discounted product?\"\n",
    "}\n",
    "```\n",
    "* fine tuning QA for CSV blocks with no dropped columns results in nan loss and prevents training at all\n",
    "* At this moment evaluation results are far from perfect - but we will need to check on real datasets - purpose of this notebook was to show possibility to treat our problem as QA task"
   ],
   "id": "2eef1e16f802c59b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f55df5b95c40300c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
