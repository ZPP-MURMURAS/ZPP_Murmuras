### Pipeline benchmark
#### Overview
`benchmark.py` is a script that benchmarks the accuracy of different pipelines for our project. It aims to provide a measurable and fair comparison between different pipelines by assessing their accuracy on a common dataset. \


#### Running the benchmark
To run the benchmark, the user must run the following command: 
```bash
python3 benchmark.py -d [name of the HuggingFace dataset to download] -p [a command to run the pipeline]
```
for example: 
```bash
python3 benchmark.py -d zpp-murmuras/llama-ds-wth -p "python3 proto_pipeline.py"
```

The pipeline should receive the input through stdin and output the results through stdout.
The dataset is expected to have a field 'Context' with the string input to the pipeline and a field 'Response' with the expected output of the pipeline.

#### Benchmarking process
For each entry in the dataset the script reads the coupons that the pipeline generated and compares them to the expected coupons. The expected and generated coupons are matched by computing a similarity matrix and sequentially finding maximum values with distinct rows and columns until the maximum is below a predefined threshold. The final score is computed by dividing the sum of coupon similarities from all entries by the total number of expected coupons. The number of hallucinated coupons, i.e. coupons that were generated by the pipeline and not matched, is also counted, but it does not affect the score.
#### Coupon formats 
There are two coupon formats: `Coupon` and `CouponSimple`. `CouponSimple` is used by default, but the user can switch to `Coupon` by using the `-e` flag.
```python
class Coupon:
    product_name: str
    new_price: Optional[str] = None
    old_price: Optional[str] = None
    percents: List[str] = field(default_factory=list)
    other_discounts: List[str] = field(default_factory=list)
    dates: Optional[str] = None

class CouponSimple:
    product_name: str
    discount_text: str
    validity_text: str
```

#### Arguments/options
`-h, --help`: show this help message and exit

`-d, --dataset`: Name of the HuggingFace dataset to download (e.g., zpp-murmuras/llama-ds-wth)

`-p, --pipeline`: Command to run the pipeline (e.g., ./run_pipeline)

`-e, --extended`: Use the extended format (ie. Coupon)

`-c, --cache_dir`: Directory to cache the dataset (default: ./datasets)

`-s, --split`: The split from the dataset to use. If you want to use split A and B, write A+B (default: Edeka+Penny)

`-t, --threshold`: The minimum similarity between coupons to be matched (default: 0.5)

`-l, --log_file`: The name of the log file (default: benchmark.log)
  
#### Sources 
- [Benchmarking AI](https://mlsysbook.ai/contents/core/benchmarking/benchmarking.html)
- [What is a benchmark and why do you need it?](https://www.mim.ai/what-is-a-benchmark-and-why-do-you-need-it/)
- Murmuras coupon_llm benchmark 
