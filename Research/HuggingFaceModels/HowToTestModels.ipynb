{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load dataset",
   "id": "2274d8258010851"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:47:09.058607Z",
     "start_time": "2024-11-05T13:47:07.486149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load only the test split of the CoNLL-2003 dataset\n",
    "raw_dataset = load_dataset(\"conll2003\", split=\"test\")\n",
    "raw_dataset"
   ],
   "id": "f544e451b0ea3567",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "    num_rows: 3453\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Get labels",
   "id": "82f6ddd79249780d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:47:09.064614Z",
     "start_time": "2024-11-05T13:47:09.059617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ner_feature = raw_dataset.features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ],
   "id": "2314cccc893ecd32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Load fine-tuned models from the HuggingFace",
   "id": "b60f9a54e99d84e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:47:10.090944Z",
     "start_time": "2024-11-05T13:47:09.064614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model_names = [\"GustawB/albert-finetuned-ner\", \"GustawB/distilbert-finetuned-ner\", \"GustawB/bert-finetuned-ner\"]\n",
    "model_tokenizers = {}\n",
    "models = {}\n",
    "# Load tokenizers and models in an automatic fashion\n",
    "for name in model_names:\n",
    "    model_tokenizers[name] = AutoTokenizer.from_pretrained(name)\n",
    "    models[name] = AutoModelForTokenClassification.from_pretrained(\n",
    "        name,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )"
   ],
   "id": "3195c172cf87bf98",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 Tokenize and align labels",
   "id": "5c7a3aaa1b1747a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:47:10.096182Z",
     "start_time": "2024-11-05T13:47:10.091952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ],
   "id": "c90b6dfe0307bbb5",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:47:10.105706Z",
     "start_time": "2024-11-05T13:47:10.097191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ],
   "id": "69478e195e23e460",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:47:10.158662Z",
     "start_time": "2024-11-05T13:47:10.105706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = {}\n",
    "for name in model_names:\n",
    "    tokenized_datasets[name] = raw_dataset.map(\n",
    "        lambda examples: tokenize_and_align_labels(examples, model_tokenizers[name]),\n",
    "        batched=True,\n",
    "        remove_columns=raw_dataset.column_names,\n",
    "    )\n",
    "tokenized_datasets"
   ],
   "id": "c8a990e04d922d76",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GustawB/albert-finetuned-ner': Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 3453\n",
       " }),\n",
       " 'GustawB/distilbert-finetuned-ner': Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 3453\n",
       " }),\n",
       " 'GustawB/bert-finetuned-ner': Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 3453\n",
       " })}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Collate the datasets",
   "id": "dc982d6980a6d7bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:47:10.171033Z",
     "start_time": "2024-11-05T13:47:10.158662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "collators = {}\n",
    "for name in model_names:\n",
    "    collators[name] = DataCollatorForTokenClassification(tokenizer=model_tokenizers[name])"
   ],
   "id": "cec497099e21204f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Define metrics",
   "id": "36a2e74e88d3a2c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:47:11.305860Z",
     "start_time": "2024-11-05T13:47:10.172040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ],
   "id": "a3d527d0c85667b1",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Evaluate models",
   "id": "2f6ee649cfdc2596"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T14:46:41.968140Z",
     "start_time": "2024-11-05T14:43:01.200582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from pprint import pprint\n",
    "\n",
    "from accelerate import Accelerator, ProfileKwargs\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Setup for resource profiling with Accelerate\n",
    "profile_kwargs = ProfileKwargs(\n",
    "    activities=[\"cpu\"],  # Track CPU utilization, can also use \"gpu\" if using a GPU\n",
    "    record_shapes=True\n",
    ")\n",
    "\n",
    "results = {}\n",
    "resource_logs = {}\n",
    "for name in model_names:\n",
    "    # Initialize Accelerator with profiling\n",
    "    accelerator = Accelerator(cpu=True, kwargs_handlers=[profile_kwargs])\n",
    "    # Prepare model with Accelerator\n",
    "    model = accelerator.prepare(models[name])\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        per_device_eval_batch_size=8,\n",
    "        output_dir=\"./results\",\n",
    "        logging_dir=\"./logs\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        data_collator=collators[name],\n",
    "        args=args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=model_tokenizers[name],\n",
    "    )\n",
    "    \n",
    "    # Profiling with Accelerator\n",
    "    with accelerator.profile() as prof:  # Begin profiling\n",
    "        eval_results = trainer.evaluate(tokenized_datasets[name])  # Run evaluation\n",
    "        results[name] = eval_results\n",
    "        resource_logs[name] = prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10)\n",
    "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    break\n",
    "    #results[name] = trainer.evaluate(tokenized_datasets[name])\n",
    "    \n",
    "pprint(results)\n",
    "pprint(resource_logs)"
   ],
   "id": "5ef962d09d1d570b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_22388\\2209588928.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 01:32]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T14:47:13.340743Z",
     "start_time": "2024-11-05T14:47:13.335811Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5dc0c92c77a778b0",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a409331e7f870ba6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
