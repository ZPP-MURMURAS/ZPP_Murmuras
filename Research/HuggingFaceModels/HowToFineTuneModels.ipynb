{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load the dataset",
   "id": "d1f4aba08760c44"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:47.228822Z",
     "start_time": "2024-11-05T08:49:41.311475Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "raw_datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "d0f5359f5ef06ea6",
   "metadata": {},
   "source": [
    "# 2. List Labels for words available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "b45d04d542713f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:47.233855Z",
     "start_time": "2024-11-05T08:49:47.229829Z"
    }
   },
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "8408829648066afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:47.243395Z",
     "start_time": "2024-11-05T08:49:47.234861Z"
    }
   },
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "b5bf38635aaa2a39",
   "metadata": {},
   "source": [
    "# 3. Showcase mapping between words and tokens"
   ]
  },
  {
   "cell_type": "code",
   "id": "576c2b38079f4251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:47.261647Z",
     "start_time": "2024-11-05T08:49:47.244405Z"
    }
   },
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU    rejects German call to boycott British lamb . \n",
      "B-ORG O       B-MISC O    O  O       B-MISC  O    O \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "4e82ccf3e16afe9f",
   "metadata": {},
   "source": [
    "# 4. Load BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b588d0ffeb9cb04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:47.924949Z",
     "start_time": "2024-11-05T08:49:47.261647Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.is_fast"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "9bcabf3c5290ed02",
   "metadata": {},
   "source": [
    "# 5. Example of how to tokenize a sentence"
   ]
  },
  {
   "cell_type": "code",
   "id": "bbadd470e7df1e47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:47.932263Z",
     "start_time": "2024-11-05T08:49:47.925956Z"
    }
   },
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "f58353e702acefd7",
   "metadata": {},
   "source": "Code above introduces an issue: word lamb was split into two, and additional special tokens were added. Now, we have an input of len 12, while having 9 labels. We need to align them."
  },
  {
   "cell_type": "code",
   "id": "f1ed2cebe7269c80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:47.938902Z",
     "start_time": "2024-11-05T08:49:47.933274Z"
    }
   },
   "source": [
    "inputs.word_ids() # Mapping between tokens and words"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "98909a5b9d695244",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:47.946354Z",
     "start_time": "2024-11-05T08:49:47.939908Z"
    }
   },
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "214080aad136ce40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:47.954625Z",
     "start_time": "2024-11-05T08:49:47.946354Z"
    }
   },
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "d5f46c717450b4be",
   "metadata": {},
   "source": [
    "# 6. Function for both tokenization and alignment"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d118e15e4593ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:47.963200Z",
     "start_time": "2024-11-05T08:49:47.955633Z"
    }
   },
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "e137d3a563315a40",
   "metadata": {},
   "source": [
    "Use the function to tokenize and align labels for all datasets. map() function applies a function to all elements of a list,\n",
    "and with batched = True we apply it to all elements of a batch to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "id": "6b954d1cedd9d251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:49.767632Z",
     "start_time": "2024-11-05T08:49:47.964206Z"
    }
   },
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "tokenized_datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b0d9e73f4c84783bcd6f02a2bc9a2d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c185aa24c2534c48bfb93b96c5510c29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c5d7b49bb9945d8a51dd1a0e1615855"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "f845879c33e54201",
   "metadata": {},
   "source": [
    "# 7. Data Collation\n",
    "Now we need to collate the data into batches, and pad the sequences to the same length. This way, sentences in every batch will be the same length,\n",
    "but the length will vary between batches."
   ]
  },
  {
   "cell_type": "code",
   "id": "9b56a0a8bc205a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:50.335112Z",
     "start_time": "2024-11-05T08:49:49.768640Z"
    }
   },
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])\n",
    "\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n",
      "[-100, 1, 2, -100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "2ed767af58c2ea0f",
   "metadata": {},
   "source": [
    "As you can see, the labels are padded with -100, which is a special token that will be ignored by this model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8cd308d9c26e86",
   "metadata": {},
   "source": [
    "# 8. Define metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "36814936bec64d38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:51.863826Z",
     "start_time": "2024-11-05T08:49:50.336119Z"
    }
   },
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Take labels for the first example\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "dbe6082127190a3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:51.885825Z",
     "start_time": "2024-11-05T08:49:51.864837Z"
    }
   },
   "source": [
    "# Make a prediction with a wrong label to showcase thew result of metrics computation\n",
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': np.float64(1.0),\n",
       "  'recall': np.float64(0.5),\n",
       "  'f1': np.float64(0.6666666666666666),\n",
       "  'number': np.int64(2)},\n",
       " 'ORG': {'precision': np.float64(1.0),\n",
       "  'recall': np.float64(1.0),\n",
       "  'f1': np.float64(1.0),\n",
       "  'number': np.int64(1)},\n",
       " 'overall_precision': np.float64(1.0),\n",
       " 'overall_recall': np.float64(0.6666666666666666),\n",
       " 'overall_f1': np.float64(0.8),\n",
       " 'overall_accuracy': 0.8888888888888888}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "cf7b2e721f35bb2a",
   "metadata": {},
   "source": [
    "# 9. Define proper metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a309bc2d5a7548fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:51.891022Z",
     "start_time": "2024-11-05T08:49:51.886769Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "9f1df65d9430c853",
   "metadata": {},
   "source": [
    "# 10. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe09c20b81d5321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:51.900737Z",
     "start_time": "2024-11-05T08:49:51.892033Z"
    }
   },
   "source": [
    "# Define mapping between labels and ids\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n",
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "febcfd992b826f34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:52.208207Z",
     "start_time": "2024-11-05T08:49:51.901743Z"
    }
   },
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model.config.num_labels"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "415bba4715f5a1b0",
   "metadata": {},
   "source": [
    "# 11 Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e354954e5d97df3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:52.222608Z",
     "start_time": "2024-11-05T08:49:52.210837Z"
    }
   },
   "source": [
    "# Log in to the Hugging Face Hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "222a09be4a814c1a8666c089db47280f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "a31c255deb87d83c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:53:15.658791Z",
     "start_time": "2024-11-05T08:53:15.646744Z"
    }
   },
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Arguments for training\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "76befe14a482c57d",
   "metadata": {},
   "source": [
    "# 12. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "95ad53e467979a02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T11:39:33.195487Z",
     "start_time": "2024-11-05T09:58:57.781953Z"
    }
   },
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train(); #On my PC this will take around 90 minutes, uncomment at your own risk\n",
    "\n",
    "# Save the model to the Hugging Face Hub\n",
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_18612\\2007957657.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 1:38:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.069629</td>\n",
       "      <td>0.899496</td>\n",
       "      <td>0.930831</td>\n",
       "      <td>0.914895</td>\n",
       "      <td>0.981662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.070156</td>\n",
       "      <td>0.936979</td>\n",
       "      <td>0.945809</td>\n",
       "      <td>0.941374</td>\n",
       "      <td>0.984959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.064164</td>\n",
       "      <td>0.936724</td>\n",
       "      <td>0.951700</td>\n",
       "      <td>0.944152</td>\n",
       "      <td>0.986519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d18fab3866f8430ab127e0eaf8b1168e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73533cd2934443ff8af3a300add725ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "000c954532274b9f830706e5ff9de627"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/GustawB/bert-finetuned-ner/commit/e10a0c20bc2c293205ee90a5c9ec49513266e693', commit_message='Training complete', commit_description='', oid='e10a0c20bc2c293205ee90a5c9ec49513266e693', pr_url=None, repo_url=RepoUrl('https://huggingface.co/GustawB/bert-finetuned-ner', endpoint='https://huggingface.co', repo_type='model', repo_id='GustawB/bert-finetuned-ner'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 13.5 Fine tune other models for comparison",
   "id": "c71cc47287ac4e7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T09:53:04.867336Z",
     "start_time": "2024-11-05T08:58:07.211264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fine-tune DistilBert\n",
    "\n",
    "distilBertModel = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-cased\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "distilBertAgrs = TrainingArguments(\n",
    "    \"distilbert-finetuned-ner-gbgb\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "distilBerttrainer = Trainer(\n",
    "    model=distilBertModel,\n",
    "    args=distilBertAgrs,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "distilBerttrainer.train()\n",
    "distilBerttrainer.push_to_hub(commit_message=\"Training complete\")"
   ],
   "id": "f38ebaddf98b2bb5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_18612\\585876913.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  distilBerttrainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 54:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.093700</td>\n",
       "      <td>0.089384</td>\n",
       "      <td>0.873695</td>\n",
       "      <td>0.901043</td>\n",
       "      <td>0.887158</td>\n",
       "      <td>0.973303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.083326</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>0.925783</td>\n",
       "      <td>0.918671</td>\n",
       "      <td>0.980176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.074842</td>\n",
       "      <td>0.913895</td>\n",
       "      <td>0.934197</td>\n",
       "      <td>0.923935</td>\n",
       "      <td>0.982016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: Root=1-6729eb02-7e12797b7e96f9ee4f0330e0;95de3382-7784-4d02-bf77-6b4f5ccf5933)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"GustawB\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\ZPP_Murmuras\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 406\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\PycharmProjects\\ZPP_Murmuras\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[1;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPError\u001B[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mHfHubHTTPError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 29\u001B[0m\n\u001B[0;32m     19\u001B[0m distilBerttrainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     20\u001B[0m     model\u001B[38;5;241m=\u001B[39mdistilBertModel,\n\u001B[0;32m     21\u001B[0m     args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     26\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[0;32m     27\u001B[0m )\n\u001B[0;32m     28\u001B[0m distilBerttrainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m---> 29\u001B[0m \u001B[43mdistilBerttrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpush_to_hub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommit_message\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTraining complete\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ZPP_Murmuras\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4622\u001B[0m, in \u001B[0;36mTrainer.push_to_hub\u001B[1;34m(self, commit_message, blocking, token, revision, **kwargs)\u001B[0m\n\u001B[0;32m   4620\u001B[0m \u001B[38;5;66;03m# In case the user calls this method with args.push_to_hub = False\u001B[39;00m\n\u001B[0;32m   4621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhub_model_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 4622\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_hf_repo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4624\u001B[0m \u001B[38;5;66;03m# Needs to be executed on all processes for TPU training, but will only save on the processed determined by\u001B[39;00m\n\u001B[0;32m   4625\u001B[0m \u001B[38;5;66;03m# self.args.should_save.\u001B[39;00m\n\u001B[0;32m   4626\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_model(_internal_call\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\ZPP_Murmuras\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4434\u001B[0m, in \u001B[0;36mTrainer.init_hf_repo\u001B[1;34m(self, token)\u001B[0m\n\u001B[0;32m   4431\u001B[0m     repo_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mhub_model_id\n\u001B[0;32m   4433\u001B[0m token \u001B[38;5;241m=\u001B[39m token \u001B[38;5;28;01mif\u001B[39;00m token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mhub_token\n\u001B[1;32m-> 4434\u001B[0m repo_url \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_repo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepo_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprivate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhub_private_repo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   4435\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhub_model_id \u001B[38;5;241m=\u001B[39m repo_url\u001B[38;5;241m.\u001B[39mrepo_id\n\u001B[0;32m   4436\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpush_in_progress \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ZPP_Murmuras\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[0;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ZPP_Murmuras\\.venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3544\u001B[0m, in \u001B[0;36mHfApi.create_repo\u001B[1;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001B[0m\n\u001B[0;32m   3542\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m RepoUrl(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mendpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   3543\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m HfHubHTTPError:\n\u001B[1;32m-> 3544\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[0;32m   3545\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3546\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ZPP_Murmuras\\.venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3531\u001B[0m, in \u001B[0;36mHfApi.create_repo\u001B[1;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001B[0m\n\u001B[0;32m   3528\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   3530\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3531\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3532\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m   3533\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m exist_ok \u001B[38;5;129;01mand\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m409\u001B[39m:\n\u001B[0;32m   3534\u001B[0m         \u001B[38;5;66;03m# Repo already exists and `exist_ok=True`\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ZPP_Murmuras\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:468\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n\u001B[0;32m    463\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    464\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Forbidden: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merror_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    465\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCannot access content at: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    466\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure your token has the correct permissions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    467\u001B[0m     )\n\u001B[1;32m--> 468\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m416\u001B[39m:\n\u001B[0;32m    471\u001B[0m     range_header \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mrequest\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRange\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mHfHubHTTPError\u001B[0m: (Request ID: Root=1-6729eb02-7e12797b7e96f9ee4f0330e0;95de3382-7784-4d02-bf77-6b4f5ccf5933)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"GustawB\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T12:59:20.447946Z",
     "start_time": "2024-11-05T11:46:24.804696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fine-tune AlBERT\n",
    "\n",
    "AlBertModel = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"albert-base-v2\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "AlBertAgrs = TrainingArguments(\n",
    "    \"albert-finetuned-ner-gbgb\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "AlBerttrainer = Trainer(\n",
    "    model=AlBertModel,\n",
    "    args=AlBertAgrs,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "AlBerttrainer.train()\n",
    "AlBerttrainer.push_to_hub(commit_message=\"Training complete\")"
   ],
   "id": "7f7755dbbdcd973f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c0240a7e14240899557b00f9351a145"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dd53ed7afab4a359571839f7c920f0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_18612\\2233204810.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  AlBerttrainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 1:12:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.484265</td>\n",
       "      <td>0.407918</td>\n",
       "      <td>0.273982</td>\n",
       "      <td>0.327796</td>\n",
       "      <td>0.850209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.372616</td>\n",
       "      <td>0.490323</td>\n",
       "      <td>0.383709</td>\n",
       "      <td>0.430514</td>\n",
       "      <td>0.877759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.337130</td>\n",
       "      <td>0.503215</td>\n",
       "      <td>0.460956</td>\n",
       "      <td>0.481159</td>\n",
       "      <td>0.889813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/44.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "420caaea5a7948ecb2ebe4e0a07f7b9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a932462ad3a6468ca2c8107ff56af1db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "418e10112bba4359abdf7f9583c822c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/GustawB/albert-finetuned-ner-gbgb/commit/80561fd086b38182d566b91377f5ef1fc8d20e58', commit_message='Training complete', commit_description='', oid='80561fd086b38182d566b91377f5ef1fc8d20e58', pr_url=None, repo_url=RepoUrl('https://huggingface.co/GustawB/albert-finetuned-ner-gbgb', endpoint='https://huggingface.co', repo_type='model', repo_id='GustawB/albert-finetuned-ner-gbgb'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "c9041ddd5755f0c0",
   "metadata": {},
   "source": [
    "# 13 Use the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f5e07744688a936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:54.227141Z",
     "start_time": "2024-11-05T08:49:54.227141Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint, I use this so that training won't kill my PC\n",
    "model_checkpoint = \"huggingface-course/bert-finetuned-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca0126b8292481d6",
   "metadata": {},
   "source": [
    "# 14 Playing with bert-base-cased model\n",
    "As you probably noticed, we are fine-tuning bert-base-cased model. You can find it here: https://huggingface.co/google-bert/bert-base-cased?text=The+goal+of+life+is+%5BMASK%5D.\n",
    "According to its description, its main purpose is to be fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions.\n",
    "Still, it supports token classification (showcased above), but also sentence prediction (example below)."
   ]
  },
  {
   "cell_type": "code",
   "id": "8b0f12ca96bb3c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:49:54.228137Z",
     "start_time": "2024-11-05T08:49:54.228137Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-cased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 15. Models to train and test",
   "id": "a53d8d964b2387e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "https://huggingface.co/FacebookAI/xlm-roberta-base \\\n",
    "https://huggingface.co/distilbert/distilroberta-base \\\n",
    "https://huggingface.co/google-bert/bert-base-cased \\\n",
    "https://huggingface.co/google-bert/bert-base-uncased \\\n",
    "https://huggingface.co/distilbert/distilbert-base-cased \\\n",
    "https://huggingface.co/distilbert/distilbert-base-uncased \\\n",
    "https://huggingface.co/microsoft/deberta-v3-base \\\n",
    "https://huggingface.co/albert/albert-base-v2 \\\n",
    "https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D (Very lacking docs) \\\n",
    "https://huggingface.co/prajjwal1/bert-tiny (some dude) \\"
   ],
   "id": "42a1756f19d13909"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a2d09d39a8b3d714",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
